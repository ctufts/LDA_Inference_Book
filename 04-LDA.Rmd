
# LDA as a Generative Model

This chapter is going to focus on LDA as a generative model. I'm going to build on the unigram generation example from the last chapter and with each new example a new variable will be added until we work our way up to LDA.  After getting a grasp of LDA as a generative model in this chapter, the following chapter will focus on working backwards to answer the following question: "If I have a bunch of documents, how do I infer topic information (word distributions, topic mixtures) from them?"    


## General Terminology

Let's get the ugly part out of the way, the symbols that are going to be used in the model. 

* alpha : In order to determine the value of $\theta$, the topic distirbution of the document, we sample from a dirichlet distribution using $\overrightarrow{\alpha}$ as the input parameter.  What does this mean? The $\overrightarrow{\alpha}$ values are our prior information about the topic mixtures for that document. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the total number of words from each topic across all documents as the $\overrightarrow{\beta}$ values. 

* beta : In order to determine the value of $\phi$, the word distirbution of a given topic, we sample from a dirichlet distribution using $\overrightarrow{\beta}$ as the input parameter.  What does this mean? The $\overrightarrow{\beta}$ values are our prior information about the word distribution in a topic. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the number of times each word was used for a given topic as the $\overrightarrow{\beta}$ values. 

* theta : Is the topic proportion of a given document. More importantly it will be used as the parameter for the multinomial distribution used to identify the topic of the next word. To clarify, the selected topic's word distribution will then be used to select a word _w_. 

* phi : Is the word distribution of each topic, i.e. the probability of each word in the vocabulary being generated if a given topic, _z_,  is selected.  
* k : Topic index
* z : Topic selected for the next word to be generated. 

Outside of the variables above all the distributions should be familiar from the previous chapter. 

## Generative Model

IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

(NOTE: You need to figure out a way to connect the code example of generating words/docs back to the math (or at least one of the other symbolic representations). This is necesary for explaining how to flip to inferenece. It is fine if the inference part leaves out the long process of derivation, but you need to be able to unstack the generative model so that the transfer to inference makes sense - this has been covered at the end of chapter 2 in the multinomial/dirichlet example, but should be expanded to show how plate diagrams work.....ugh I hate these)


LDA is know as a generative model. What is a generative model? Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space [@bishop2006pattern]. This means we can create documents with a mixture of topics and a mixture of words based on thosed topics. Let's start off with a simple example of generating unigrams. 


### Generating Documents 

#### Topic Word Mixtures, Document Topic Mixtures, and Document Length Known

Building on the document generating model in chapter two, let's try to create documents that have words drawn from more than one topic. To clarify the contraints of the model will be:

* set number of topics (2)
* constant topic distributions in each document
* constant word distribution in each topic 


Known values:

* 2 topics : word distributions of each topic below
    * $\phi_{1}$ = [ &#x1F4D5; = 0.8, &#x1F4D8; = 0.2, &#x1F4D7; = 0.0 ]
    * $\phi_{2}$ = [ &#x1F4D5; = 0.2, &#x1F4D8; = 0.1, &#x1F4D7; = 0.7 ]
* All Documents have same topic distribution:
    * $\theta = [ topic a = 0.5, topic b = 0.5 ]$
* All Documents contain 10 words
    

* For d = 1 to D where D is the number of documents
    + For w = 1 to W  where W is the number of words in document *d*
        + *Select the topic for word w *
        + $z_{i}$ ~ Multinomial($\theta_{d}$)
        + *Select word based on topic z's word distribution*
        + $w_{i}$ ~ Multinomial($\phi^{(z_{i})}$)


```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(MCMCpack)
library(tidyverse)
library(knitr)
library(kableExtra) 
```



```{r, echo=TRUE, warning=FALSE, message=FALSE} 

k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters

phi <- matrix(c(0.1, 0, 0.9,
                0.4, 0.4, 0.2), 
              nrow = k, 
              ncol = length(vocab), 
              byrow = TRUE)

theta <- c(0.5, 0.5)

N <- 10 #words in each document
ds <-tibble(doc_id = rep(0,N*M), 
            word = rep('', N*M),
            topic = rep(0, N*M)
            ) 
            
row_index <- 1
for(m in 1:M){
  for(n in 1:N){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic)
    row_index <- row_index + 1
  }
}

ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word, collapse = ' ')
) %>% kable()
```

#### Topic Word Mixtures, Document Topic Mixtures, and Document Length Unknown

```{r} 
k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters

phi <- matrix(c(0.1, 0, 0.9,
                0.4, 0.4, 0.2), 
              nrow = k, 
              ncol = length(vocab), 
              byrow = TRUE)

theta <- c(0.5, 0.5)
xi <- 10 # average document length 
N <- rpois(M, xi) #words in each document
ds <-tibble(doc_id = rep(0,sum(N)), 
            word   = rep('', sum(N)),
            topic  = rep(0, sum(N))
            ) 
            
row_index <- 1
for(m in 1:M){
  for(n in 1:N[m]){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic)
    row_index <- row_index + 1
  }
}

ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word, collapse = ' ')
) %>% kable()
```

#### Static Topic Word Mixtures, Varying Document Topic Distributions

So this time we will introduce documents with different topic distributions. The document length is still known, as is the topic word mixtures, but we will be randomly sampling the topic distribution of each document. 
Provide an example of document generation based on a set # of topics, with set topic word distributions and set document topic distributions

Known values:
* 2 topics : word distributions of each topic below
    * $\phi_{1} = [ , ]$
    * $\phi_{2} = [ , ]$
* All Documents have same topic distribution:
    * $\theta = [ , ]$
* All Documents contain 10 words
    

* For d = 1 to D where D is the number of documents
    + For w = 1 to W  where W is the number of words in document *d*
        + *Select the topic for word w *
        + $z_{i}$ ~ Multinomial($\theta_{d}$)
        + *Select word based on topic z's word distribution * 
        + $w_{i}$ ~ Multinomial($\phi^{(z_{i})}$)



The generative process for LDA is shown below. 

1. For k = 1 to K where K is the total number of topics
    + *Sample parameters for word distribution of each topic*
    + $\phi^{(k)}$ ~ Dirichlet($\beta$) 
2. For d = 1 to D where number of documents is D
    + *Sample parameters for document topic distribution*
    + $\theta_{d}$ ~ Dirichlet($\alpha$)
    + For w = 1 to W  where W is the number of words in document *d*
        + *Select the topic for word w *
        + $z_{i}$ ~ Multinomial($\theta_{d}$)
        + *Select word based on topic z's word distribution * 
        + $w_{i}$ ~ Multinomial($\phi^{(z_{i})}$)




Then change example to be set # of topics, set topic word distributions, but document topic distributions are drawn from dirichlet

Then go to full LDA (example below)


```{r  echo = TRUE, warning=FALSE, message=FALSE}

k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters



phi <- matrix(c(0.1, 0, 0.9,
                0.4, 0.4, 0.2), 
              nrow = k, 
              ncol = length(vocab), 
              byrow = TRUE)


xi <- 10 # average document length 
N <- rpois(M, xi) #words in each document
ds <-tibble(doc_id = rep(0,sum(N)), 
            word   = rep('', sum(N)),
            topic  = rep(0, sum(N)), 
            theta_a = rep(0, sum(N)),
            theta_b = rep(0, sum(N))
            ) 
            
row_index <- 1
for(m in 1:M){
  theta <-  rdirichlet(1, alphas)
  
  for(n in 1:N[m]){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic,theta)
    row_index <- row_index + 1
  }
}

ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word, collapse = ' '), 
  topic_a = round(as.numeric(unique(theta_a)), 2), 
  topic_b = round(as.numeric(unique(theta_b)), 2) 
) %>% kable()

```

### LDA Generative Model 

```{r}

k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters


betas <- rep(1,length(vocab)) # dirichlet parameters for topic word distributions
phi <- rdirichlet(k, betas)


xi <- 10 # average document length 
N <- rpois(M, xi) #words in each document
ds <-tibble(doc_id = rep(0,sum(N)), 
            word   = rep('', sum(N)),
            topic  = rep(0, sum(N)), 
            theta_a = rep(0, sum(N)),
            theta_b = rep(0, sum(N))
            # , 
            # word_prop = rep(0, sum(N))
            ) 
            
row_index <- 1
for(m in 1:M){
  theta <-  rdirichlet(1, alphas)
  
  for(n in 1:N[m]){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic,theta)
    row_index <- row_index + 1
  }
}

ds %>% group_by(doc_id, topic) %>% summarise(
  tokens = paste(word, collapse = ' '), 
  topic_a = round(as.numeric(unique(theta_a)), 2), 
  topic_b = round(as.numeric(unique(theta_b)), 2) 
) %>% kable() 

```
(NOTE: Spread the table so topics are side by side for each doc)

```{r r GenerativeTable, echo = FALSE, warning=FALSE, message=FALSE}
```


The LDA generative process for each document is shown below[@darling2011theoretical]: 

$$
p(w,z,\theta,\phi|\alpha, B) = p(\phi|B)p(\theta|\alpha)p(z|\theta)p(w|\phi_{z})
$$

You may be like me and have a hard time seeing how we get to the equation above and what it even means. If we look back at the pseudo code for the LDA model it is a bit easier to see how we got here. We start by giving a probability of a topic for each word in the vocabulary, $\phi$. This value is drawn randomly from a dirichlet distribution with the parameter $\beta$ giving us our  first term $p(\phi|\beta)$. The next step is generating documents which starts by calculating the topic mixture of the document, $\theta_{d}$ generated from a dirichlet distribution with the parameter $\alpha$. This is our second term $p(\theta|\alpha)$. You can see the following two terms also follow this trend. The topic, _z_, of the next word is drawn from a multinomial distribuiton with the parameter $\theta$. Once we know _z_, we use the distribution of words in topic _z_, $\phi_{z}$, to determine the word that is generated.  

