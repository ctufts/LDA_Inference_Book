
# Parameter Estimation

I'm going to build off of a simple example which can be extended through the 
remainder of this tutorial - a coin flip. A coin flip has 2 possible outcomes: 
heads or tails. 

__Conditioning for your brain__ : Try to start keeping in mind that we think of a work in a similar 
manner: word exists in the document or word doesn't exist in the document 

The equation below is a fundamental building block on the way we think about inference. 
The 4 components are:

* __Prior__: Defines our prior beliefs of the parameter. Do we believe to a good degree of 
certainty that the coin is fair?  Maybe take a step back and ask yourself 'do I 
trust the manufacturer of this coin?'. If this manufacturer has always had great quality
fair coins then you would probably estimate your prior to be 0.5 (50% heads, 50% tails). 
* __Posterior__: The probability of the parameter __given__ the evidence. The only way 
to know this value is to already have the evidence...but we can estimate it. Think of 
it this way give 100 coin flips with 47 heads and 53 tails what is the probability that
theta is 0.5 (coin is fair)?
* __Likelihood__: The probability of the evidence __given__ the parameter. Given that 
we know the coin is fair (theta = 0.5) what is the probability of having 47 heads out 
of 100 flips? 
* __Evidence__: The probability of all possible outcomes. Probability of 1/100 heads, 
2/100 heads, .....



$$
\underbrace{p(\Theta|D)}_{posterior} = {\overbrace{p(D|\Theta)}^{likelihood}
  \overbrace{p(\Theta)}^{prior} \over \underbrace{p(D)}_{evidence}}
$$


## Maximum Likelihood
The simplest method of parameter estimation is the maximum likelihood method. Effectively
we calculate the parameter that maximizes the likelihood. So what does this actually 
mean? 

$$
L(p;x)\approx f(x;p)=\prod\limits_{i=1}^n f(x_i;p)=\prod\limits_{i=1}^n p^x(1-p)^{1-x}
$$
```{r ML, echo = TRUE, fig.cap = 'Scatter Plot - Random Data'}

```

## Maximum a Posteriori (MAP)
MAP is similar to the method of maximum likelihood estimation, but it also let's us 
include information about our prior beliefs. 

## Bayesian Inference

Where does the evidence term go? 


Make an example of this and cite Krutchke - I assume in the earlier chapters there is something present. 

For explanation #1 you can use the metropolis hastings algorithm examples. You are 
sampling from a posterior without a normalizing factor, but you use the proportions
 of the samples to determine the probability (i.e. you infer the normalization 
 term through sampling - it is basically built in at that point.)
[Explanation 1: The normalizing constant is not interesting, this is very common in bayesian statistics, . With Gibbs sampling, Metropolis-Hasting or any other Monte Carlo method, what you are doing is drawing samples from this posterior. That is, the more density around a point, the more samples you'll get there.

Then, once you have enough samples from this posterior distribution, you know that the normalized density at some point xx is the proportion of samples that felt at that point.

You can even plot an histogram on the samples to see the (unnormalized) posterior.

In other words, if I give you the samples 1,3,4,5,1.....,3,4,16,11,3,4,5,1.....,3,4,16,1 and I tell you these are samples from a density function, you know to compute the probability of every value.


..... Explanation 3 If you use conjugate priors for the individual variables, the denominator of their conditional probability will be always nice and familiar, and you will know what the normalizing constant in the denominator is. This is why Gibbs sampling is so popular when the joint probability is ugly but the conditional probabilities are nice.](https://stats.stackexchange.com/questions/138644/confusion-in-gibbs-sampling#138648)

For explanation #3 use the coin flip example... 
p(theta1, theta2|D) show how denominator p(d) drops out. 



### Conjugate Priors

The general premise of conjugate priors is that the prior has the same form 
as the posterior distribution. Ok, but what does that mean for us? In a pragmatic 
sense it means we need to look for a pattern when solving for our posterior. 

Let's take the coin flip example. We want to estimate the posterior, the 
probability of $\theta$ given the observed data. Obviously we don't have observed 
data so we will need to use the likelihood and the prior to estimate the posterior. 
For this case we will use the beta distribution to estimate the prior with the 
form:

[BETA DISTRIBUTION - use overbraces to show the pattern ]

#### Bernoulli & Beta
The beta distribution serves as the conjugate prior for the bernoulli distribution.
The beta distribution can be thought of as a probability distribution of 
distributions[@Robinson2014beta]. 

Since we are building up to estimation of a coin's bias, I'll stick with the coin flip
to describe how the beta distribution works. In general the more information I have
the more narrow my distribution will be.  In a logical sense if you flip a coin 2 
times resulting in 1 heads and 1 tails how sure are you that the coin is fair? Probably not all that sure, right? But what if you flipped the coin 200 times and 
it resulted in 100 heads and 100 tails? You would be much more confident that the coin
is fair.  Below you will see the shape of the beta distribution dependent on it's shape parameters $\alpha$ and $\beta$. 

```{r betashape, echo = TRUE, fig.cap = 'Beta Distribution'}
library(ggplot2)
a <- c(1, 10, 100)
b <- c(1, 10, 100)
params <- cbind(a,b)
ds <- NULL
n <- seq(0,1,0.01)
for(i in 1:nrow(params)){
  ds <- rbind(data.frame(x = n, y = dbeta(n, params[i,1], params[i,2]),
                             parameters = paste0("\U03B1 = ",params[i,1],
                                                 ", \U03B2 = ", params[i,2])), ds)
}

ggplot(ds, aes(x = x, y = y, color=parameters)) + geom_line() + 
  labs(x = 'Density')

```

What about the cases where $\alpha$ and $\beta$ are not equal or close to equal? Well in those 
cases you would probably assume a bit of skew in the distribution, i.e. your coin may
be less than fair. 

```{r betaShapeSkewed, echo = TRUE, fig.cap = 'Beta Distribution - Skewed'}
library(ggplot2)
a <- c(6, 4)
b <- c(4, 6)
params <- cbind(a,b)
ds <- NULL
n <- seq(0,1,0.01)
for(i in 1:nrow(params)){
  ds <- rbind(data.frame(x = n, y = dbeta(n, params[i,1], params[i,2]),
                             parameters = paste0("\U03B1 = ",params[i,1],
                                                 ", \U03B2 = ", params[i,2])), ds)
}

ggplot(ds, aes(x = x, y = y, color=parameters)) + geom_line() + 
  labs(x = 'Density')

```

### Gibbs Sampling

* What happens to the evidence term & why can we ignore it? 


### Bias of Two Coins

