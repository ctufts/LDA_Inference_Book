# LDA Inference

Now it is time to flip the problem around. What happens when I have a bunch of documents and I want to know what topics are present in each document and what words are present (most probable) in each topic? In terms of math, the values we need to answer the questions above are:

$$
p(\theta, \phi, z|w, \alpha, \beta) = {p(\theta, \phi, z, w|\alpha, \beta) \over p(w|\alpha, \beta)}
\tag{26}
$$
Equation 26 says the following:   
The probability of the topic proportion, the word distribution of each topic, and the topic label given a word (in a document) and the hyperparameters $\alpha$ and $\beta$. In a more basic term this means the probability of a specific topic _z_ given the word _w_ (and our prior assumptions, i.e. hyperparameters), once the topic label of a word is known, then we can answer derive $\phi$ and $\theta$.


Let's take a step from the math and just map out things we know versus the things we don't know in our current scenario:

*Known Parameters*

* *Documents:* We have a set number of documents we want to identify the topic strucutres in. 
* *Words:* We have a collection of words and word counts for each document which we place in a document word matrix. 
* *Vocabulary:* The unique list of words across all documents. 
* *Hyperparemeters:*
    * $\overrightarrow{\alpha}$
    * $\overrightarrow{\beta}$

And on to the parts we don't know....

* Unknown (Latent) Parameters * 

* *Number of Topics:* We need to determine the number of topics we assume are present in the documents. For the purposes of this book I'm going to skip how to estimate this number properly. If you are interested in learning more about how to estimate the number of topics present in a document see REFERENCE LIST. 
* *Document Topic Mixture:* We need to determine the topic distribution in each document. 
* *Word Distribution of Each Topic:* We need to know the distribution of words in each topic. Obviously some words are going to occur very often in a topic while others may have zero probability of occurring in a topic. 
* *Word topic assignment:* This is actually the main thing we need to infer. To be clear, if we know the topic assignment of every word in every document, then we can derive the document topic mixture and the word distribution of each topic. 


Quick note:   
Equation 26 is based on the following statistical property
$$
p(A, B | C) = {p(A,B,C) \over p(C)}
$$

The derivation connecting equation 26 to the actual Gibbs sampling solution to determine _z_ for each word in eacch document, $\overrightarrow{\theta}$, and 
$\overrightarrow{\phi}$ is very complicated and I'm just going to gloss over a few steps. For complete derivations see (site heinrich and that other one from darling). 



Edwin Chen style overview, make a graphic of this.... 




----- Give general description of pseudo code 
- go through each doc, each word, get probability of topic, assign topic, repeat
  - this all gets lost in the math - it really is that simple, all the confusing components 
come from the derivation of the probability calculation ....


What if I don't want to generate docuements. What if my goal is to infer what topics are present in each document and what words belong to each topic? This is were LDA for inference comes into play.

Before going through any derivations of how we infer the document topic distributions and the word distributions of each topic, I want to go over the process of inference more generally. 

<b>The General Idea of the Inference Process</b>

1. <b>Initialization:</b> Randomly select a topic for each word in each document from a multinomial distribution. 
2. <b>Gibbs Sampling:</b>  
* For <i>i</i> iterations
* For document d in documents:
* For each word in document d:
* <i>assign a topic to the current word based on probability of the topic given the topic of all other words (except the current word)</i>   

If you recall from the previous chapters on Gibbs sampling to infer the value of each $\theta$ we calculate the value of of 
$p(\theta_{}|)

Use Darling as an outline for the derivation process .... cite carpenter and heinrich (same as Darling does). 


Below is a toy example that creates a set of documents based on the 3 word (emoji) vocabulary. Each of the 10 documents has a differnt topic mixture and is assigned a random lenght. Our aim with inferrence is to infer the topic mixture of each document and the word distributions of each topic. 

```{r}
get_topic <- function(k){ 
which(rmultinom(1,size = 1,rep(1/k,k))[,1] == 1)
} 


k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters
beta <- 1


phi <- matrix(c(0.1, 0, 0.9,
                0.4, 0.4, 0.2), 
              nrow = k, 
              ncol = length(vocab), 
              byrow = TRUE)


xi <- 100 # average document length 
N <- rpois(M, xi) #words in each document
ds <-tibble(doc_id = rep(0,sum(N)), 
            word   = rep('', sum(N)),
            topic  = rep(0, sum(N)), 
            theta_a = rep(0, sum(N)),
            theta_b = rep(0, sum(N))
) 

row_index <- 1
for(m in 1:M){
  theta <-  rdirichlet(1, alphas)
  
  for(n in 1:N[m]){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic,theta)
    row_index <- row_index + 1
  }
}

ds$doc_id <- as.numeric(ds$doc_id)



ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word, collapse = ' '), 
  topic_a = round(as.numeric(unique(theta_a)), 2), 
  topic_b = round(as.numeric(unique(theta_b)), 2) 
) %>% kable()


######### Inference ############### 



current_state <- ds %>% dplyr::select(doc_id, word, topic)
current_state$topic <- NA


t <- length(unique(current_state$word))

# n_doc_topic_count  
n_doc_topic_count <- matrix(0, nrow = m, ncol = k)
# document_topic_sum
n_doc_topic_sum  <- rep(0,m)
# topic_term_count
n_topic_term_count <- matrix(0, nrow = k, ncol = t)
colnames(n_topic_term_count) <- unique(current_state$word)
# topic_term_sum
n_topic_sum  <- rep(0,k)
p <- rep(0, k)
# initialize topics
for( i in 1:nrow(current_state)){
  current_state$topic[i] <- get_topic(k)
  n_doc_topic_count[current_state$doc_id[i],current_state$topic[i]] <- n_doc_topic_count[current_state$doc_id[i],current_state$topic[i]] + 1
  n_doc_topic_sum[current_state$doc_id[i]] <- n_doc_topic_sum[current_state$doc_id[i]] + 1
  n_topic_term_count[current_state$topic[i] , current_state$word[i]] <- n_topic_term_count[current_state$topic[i] ,
                                                                                           current_state$word[i]] + 1
  n_topic_sum[current_state$topic[i]] = n_topic_sum[current_state$topic[i]] + 1
  
}



# topics mixtures keep converging to 1 an 0, this is incorrect,
# or it is a result fo the topics (word distributions) being too similar? 
# gibbs
for (iter in 1:100){
  
  for(j in 1:nrow(current_state)){
    
    # decrement counts
    cs_topic <- current_state$topic[j]
    cs_doc   <- current_state$doc_id[j]
    cs_word  <- current_state$word[j]
    
    n_doc_topic_count[cs_doc,cs_topic] <- n_doc_topic_count[cs_doc,cs_topic] - 1
    n_doc_topic_sum[cs_doc] <- n_doc_topic_sum[cs_doc] - 1
    n_topic_term_count[cs_topic , cs_word] <- n_topic_term_count[cs_topic , cs_word] - 1
    n_topic_sum[cs_topic] = n_topic_sum[cs_topic] -1
    
    # get probability for each topic, select topic with highest prob
    for(topic in 1:k){
      p[topic] <- (n_topic_term_count[topic, cs_word] + beta) *
        (n_doc_topic_count[cs_doc,topic] + alphas[k])/
        sum(n_topic_term_count[topic,] + beta)
    }
    new_topic <- which.max(p)
    
    # update counts
    n_doc_topic_count[cs_doc,new_topic] <- n_doc_topic_count[cs_doc,new_topic] + 1
    n_doc_topic_sum[cs_doc] <- n_doc_topic_sum[cs_doc] + 1
    n_topic_term_count[new_topic , cs_word] <- n_topic_term_count[new_topic , cs_word] + 1
    n_topic_sum[new_topic] = n_topic_sum[new_topic] + 1
    
    
    # update current_state
    current_state$topic[j] <- new_topic
  }
}
```


