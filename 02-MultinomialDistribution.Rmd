# Multinomial Distribution

## Comparison of Dice vs. Words

The binomial distribution is a special case of the multinomial distribution 
where the number of possible outcomes is 2. A multinomial distribution can 
have 2 or more outcomes and therefore is normally shown through examples using 
a 6-sided die. Instead of using a die with numbers on each side, let's label the sides
with the following words:

"Latent" "Dirichlet" "Allocation" "has" "many" "peices"

---------------image placeholder-----------------------
Maybe draw a die with those words on it and consider it a 'document'
-------------------placeholder end--------------------


In the example above we would assume it is a fair die which would result in equal probabilities of 'rolling' any of the 6 unique words. Therefore each word has 1/6 chances of being randomly sampled from the die. 

Here is a quick empirical example where we take each word and assign it to a single side of a fair die. The experiment, a single roll of the die, is repeated 10,000 times. We can see each word comes up roughly as often as the others.  

```{r echo = FALSE, warning=FALSE, message=FALSE}
rm(list = ls())
library(MCMCpack)
library(tidyverse)
library(knitr)
```

```{r multinomialDist, echo = TRUE, warning=FALSE, message=FALSE, fig.cap = 'Sampling from Multinomial with Equal Parameters'}
# draw 1000 samples from multinomial distribution
outcomes <- replicate(10000, which(rmultinom(1,1,rep(1/6,6))==1))
words <- unlist(strsplit("Latent Dirichlet Allocation has many peices", ' '))
ds <- data.frame(id = outcomes, word = sapply(outcomes, function(x)words[x]))
ggplot(ds, aes(x= word)) + geom_histogram(stat='count',color='#1A384A', fill='#7A99AC') +
  scale_x_discrete(labels = words) +
  theme_minimal()

```

```
Side Note:
Let's think of how this will be used in the case of LDA. If I wanted to generate a document based on a model, I could use a multinomial distribution to determine what words would be in the document. If I knew the probability of a word I could use the example above to draw a new word with each sample. Obviously some words occur much more often than others, so the 'fair die' example wouldn't work for generation of document.  In later sections we will build on this concept, but its a good idea to start thinking about how this extends to language.
```


As I stated before, the binomial distribution is a special case of the multinomial distribution. The probability mass function for the multinomial distribution is shown below:


$$
f(x)=\dfrac{n!}{x_1!x_2!\cdots x_k!}\theta_1^{x_1} \theta_2^{x_2} \cdots \theta_k^{x_k}
\tag{20}
$$

* <i>k</i> - number of sides on the die
* <i>n</i> - number of times the die will be rolled

Therefore the multinomial representation of the distributions we've already discussed would use the following parameters:

* k sided die rolled n times
    * n = 1,  k = 2 is bernoulli distribution 
    * n > 1 , k = 2 is binomial distribution


## Relationship to Bernoulli 

Let's recall the bernoulli probability mass function from equation 1:

$$
f_{x}(x)=P(X=x)=\theta^{x}(1-\theta)^{1-x}, \hspace{1cm} x = \{0,1\}
\tag{21}
$$
Let's swap in some different terms. We can replace the first term, $\theta$, with $\theta_{1}$ and the second term, $(1-\theta)$ as $\theta_{2}$. Then we have:

$$
{\theta_{1}}^{x_{1}}{\theta_{2}}^{x_{2}}
\tag{22}
$$

You can see this looks a bit more like equation 21 if the case were _k_=2 and _n_=1. Since _n_=1, _x_'s can only have a value of  1 or zero. Therefore the factorial is always 1 for the case of a bernoulli trial. Factorial of _n_ is also 1 since it is a single trial, i.e. _n_=1, resulting in what we see in equation 22. 


## Conjugate Prior: Dirichlet
The conjugate prior for the multinomial distribution is the Dirichlet distribution. Similar
to the beta distribution, it can be though of as a distribution of distributions. Let's first 
take a look at the distribution of both the multinomial and Dirichlet distribution and see if we can find a similar pattern. Remember, conjugate priors have the same form as the estimated posterior of the original distribution. 

General notes on Dirichlet: higher values of beta (or whatever parameter name is), 
the more uniform the probability for each class, the lower the more more likely a specific class is going to be much larger than the rest. 

Let's try to rationalize this in the same way we do bernoulli. Bernoulli is 2 possible outcomes, beta is it's prior. 
Bernoulli is a special case of multinomial where k = 2, so I would assume that means beta is the special case of dirichlet where the number of shape parameters is 2. Think about this a bit more - in the case of beta distribution we use prior data (or assumption) of coin flips - 5 heads, 5 tails - means a = 5, b = 5. From this we make our distribution and we can randomly sample a valid value of theta (and remember we only use theta for bernoulli because if we have one probability, we can infer the other via subtraction from 1, but in reality there is a p value for heads, and a p value for tails... this makes it a bit eaiser to understand the transfer to multinomial) based on this prior information. So it would be no different if we have 3 different words in a topic, we use the prior info as the parameters for the Dirichlet distribution - 3 red, 2 blue, 1 green which would translate to alpha1 = 3, alpha2 = 2, alpha3 = 1, from there we can build our multidimensional distribution and select the most likely value for our thetas (or the probability of a specific outcome where all those p's sum to one).  

[May want to put a plot or two here showing the dirichlet code, but creating the same beta distributions, so 
this makes more sense]


## Gibbs Sampling - Multinomial & Dirichlet

Prior to getting into an example of Gibbs sampling as it applies to inferring the parameters of a multinomial distribution, let's first describe a model which generates words for a single document. As you can imagine this would be modeled as a multinomial distribution with parameters $\overrightarrow{\theta} = \theta_{1}, \theta_{2}, ... \theta_{n}$ for words 1 to n. The model would be capable of generating a bag of words representation of a document. The term <i>'bag of words'</i> refers to words in no particular order, i.e. the document we would be generating would not have structured sentences, but would contain all the components of the document. 

Let's start by defining our model. Let's start with a basic composition for our ideal document. We are going to have a document with only 3 distinct words `r '\U1F4D8'`,`r '\U1F4D5'`,`r '\U1F4D7'`. Remember a document is just a mixture of words, obviously this is not a fine work of literature and has a very limited vocabulary, but it is meant as a basic example of document composition. 

First we are going to create a seed document, i.e. the document that will be used as a basis of our $\alpha$'s for our prior. In order to do this we need to identify the mixture proportions for each word in the vocabulary. 

* `r '\U1F4D8'` : 10%
* `r '\U1F4D5'` : 10%
* `r '\U1F4D7'` : 80%

To clarify this means the document will contain 80% blue books, 10% green books, and 10% red books: 

```{r unigram_document ,echo = TRUE, warning=FALSE, message=FALSE}
# use letters function as your vocabulary
v <- c('red', 'green', 'blue')
nwords <- 10
doc_theta <- c(.1, .1, .8)
document<-rep(v, doc_theta*nwords)
books <- tibble(label = c('blue', 'red', 'green'), 
                    code = c('\U1F4D8', '\U1F4D5', '\U1F4D7'))

cat(sapply(document, function(x) books$code[which(books$label == x)]))
``` 

So what is the structure of the document generator? 

* Alpha -> Dirichlet -> Multinomial

(Maybe here is when you can introduce the terrible idea of block diagrams????,
It might be good to introduce them even though you aren't a fan, for all you know 
people might even understant them better this way...)

Do you recall the beta/bernoulli example? The way we informed our prior was using some prior information we had, i.e. the number of heads and tails previously obtained from flipping the two coins. We will use the document above as the basis of our $\alpha$ paramters for the Dirichlet distribution, i.e. our prior for the multinomial. In more general language, we want to generate documents similar to our 'ideal' document. 


So let's generate a new document using the word counts from our <i>ideal</i> document as our $\alpha$ values for the dirichlet prior. Then we use the $\theta$ values generated by the dirichlet prior as the parameters for a multinomial distribution to generate the next term in the document. 

```{r unigram_generative_model ,echo = TRUE, warning=FALSE, message=FALSE}
# generate text based only on document #1

# leave this code, I like the for loop for creating a single document, shows the process 
# in a straight forward manner, the next code chunk shows a faster way, which is good as well

words <- document
# lenght of new document
 #rep(1,length(unique(words)))
word_counts <- table(words)
alphas <-  word_counts
new_doc <- rep('', nwords)
for(i in 1:nwords){
  set.seed(i)
  p = rdirichlet(1,alphas)
  set.seed(i)
  new_doc[i] <- names(word_counts)[which(rmultinom(1, 1, p) == 1)]
}
table(new_doc)

cat('\n', sapply(new_doc, function(x) books$code[which(books$label == x)]))
```
It's not quite the same as the original, but that should be expected. This is a model that generates documents probabalistically based on some prior information. So let's make a few more and see how this changes. 

```{r multiple_docs_mul_dir ,echo = TRUE, warning=FALSE, message=FALSE}

word_counts <- table(words)
alphas <-  word_counts
nwords <- 10
ndocs  <- 5
word_encodings <- tibble(label = c('blue', 'red', 'green'), 
                code = c('\U1F4D8', '\U1F4D5', '\U1F4D7'), 
                word_props = c(.1, .1, .8))

thetas <- rdirichlet(ndocs*nwords, alphas)
print(head(thetas))

selected_words <- apply(thetas, 1, function(x) which(rmultinom(1,1,x)==1))

ds <- tibble(doc_id = rep(1:ndocs, each = nwords),
             word = word_encodings$label[selected_words], 
             word_uni = word_encodings$code[selected_words])


ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word_uni, collapse = ' ')
) %>% kable(col.names = c('Document', 'Words'))

```


As we can see each document composition is similar, but the word counts and order are different each time. This is to be expected (maybe say why? )

So now onto inferernce ....

The process above is known as a generative model. We created documents using a model with a given set of parameters. Inference is going to take this general concept and look at it from a different angle. Instead of generating documents with our model we are going to take a series of pre-existing documents and infer what model created them. We are going to make the assumption that the structure of the model is the same as the generative example, i.e. all documents are generated based on the same word mixture ratios. 

Let's use the 10 documents we previously generated as our basis and see if we can infer the parameters used to generate them. 

```{r}

# counts -> alphas
# for each of the 3 thetas iterate 1k times
# plot distributions
alphas <- rep(1,nrow(books))
# alphas <- 1:6 
n <- table(ds$word)
head(n)


niters = 2000
burnin = 500

thetas = matrix(0, nrow = (niters-burnin), ncol=nrow(books), 
                dimnames = list(NULL, c(names(n))))
for (i in 1:niters){
  theta = rdirichlet(1,n+alphas)

  
  if (i >= burnin){
    thetas[(i-burnin), ] = theta
  }
}

 # hist(thetas[, 1])
 # hist(thetas[, 2])
 # 
 df <- as.tibble(thetas) %>% 
   gather(word, theta)
# map book colors to each segment of plot to avoid bothering with the emoji labels (for now)
ggplot(df, aes(y=theta, x = word)) + geom_violin()
# apply(thetas, 2, median)
# n/sum(n)
```


We can see our mixture estimates are significantly different from the real model used to generate the documents. So why is this? One of the issues here is that our sample are documents with only 10 words. Therefore an average document has 8 ..., 1 ..., and 1..., but it is not unusual to see a slight variation which causes mixture shifts of 10% or more. And with  Let's try the same example but this time instead of only generating 5 documents we will genertate 500 and use this as our sample to draw inference from.  

```{r}

word_counts <- table(words)
alphas <-  word_counts
nwords <- 10
ndocs  <- 500
word_encodings <- tibble(label = c('blue', 'red', 'green'), 
                code = c('\U1F4D8', '\U1F4D5', '\U1F4D7'), 
                word_props = c(.1, .1, .8))

thetas <- rdirichlet(ndocs*nwords, alphas)
print(head(thetas))

selected_words <- apply(thetas, 1, function(x) which(rmultinom(1,1,x)==1))

ds <- tibble(doc_id = rep(1:ndocs, each = nwords),
             word = word_encodings$label[selected_words], 
             word_uni = word_encodings$code[selected_words])


# counts -> alphas
# for each of the 3 thetas iterate 1k times
# plot distributions



alphas <- rep(1,nrow(books))
# alphas <- 1:6 
n <- table(ds$word)
head(n)


niters = 2000
burnin = 500

thetas = matrix(0, nrow = (niters-burnin), ncol=nrow(books), 
                dimnames = list(NULL, c(names(n))))
for (i in 1:niters){
  theta = rdirichlet(1,n+alphas)

  
  if (i >= burnin){
    thetas[(i-burnin), ] = theta
  }
}

 # hist(thetas[, 1])
 # hist(thetas[, 2])
 # 
 df <- as.tibble(thetas) %>% 
   gather(word, theta)
# map book colors to each segment of plot to avoid bothering with the emoji labels (for now)
ggplot(df, aes(y=theta, x = word)) + geom_violin()
# apply(thetas, 2, median)
n/sum(n)
```

