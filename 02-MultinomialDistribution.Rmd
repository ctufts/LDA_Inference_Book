# Multinomial Distribution

Comparison of a Die vs. Words
The binomial distribution is a special case of the multinomeial distribution 
where the number of possible outcomes is 2. A multinomial distribution can 
have 2 or more outcomes and therefore is normally shown through examples using 
a 6-sided die. To get things on track for LDA, let's instead think of 1 
document (similar to the die), with 6 distinct words (the 6 sides of the die):

"Latent Dirichlet Allocation has many peices"

From the example above we would assume each word has an equal probability (think 
fair die) because each word is unique. Therefore each word has 1/6 chances of being randomly sampled from the document (think 1/6 chance of getting a '1' during a dice roll). 



k sided die n times (in our case n = 1, k = 6)
n = 1, k=2 is bernoulli distribution 
n >1 , k = 2 is binomial distribution

$$
f(x)=\dfrac{n!}{x_1!x_2!\cdots x_k!}\theta_1^{x_1} \theta_2^{x_2} \cdots \theta_k^{x_k}
$$

## Relationship to Bernoulli 

Let's recall the bernoulli probability mass function
$$
P(X_{1}=x_{1},X_{2}=x_{2},...,X_{N}=x_{N}) = \prod\limits_{n=1}^{N} \theta^{x}(1-\theta)^{1-x}
$$
Let's swap in some different terms. We can replace the first term, $\theta$, with $\theta_{1}$ and the second term, $(1-\theta)$ as $\theta_{2}$. Then we have:

$$
\prod\limits_{n=1}^{N} {\theta_{1}}^{x_{1}}{\theta_{2}}^{x_{2}}
$$
x's are 1 or zero, so the factorial is always 1 for this example (counts never higher that that). Factorial of _n_ is the same as the product from _n_=1 to _N_. 

```{r echo = FALSE, warning=FALSE, message=FALSE}
rm(list = ls())
library(MCMCpack)
library(tidyverse)
library(knitr)
```

```{r multinomialDist, echo = TRUE, warning=FALSE, message=FALSE, fig.cap = 'Sampling from Multinomial with Equal Parameters'}
# draw 1000 samples from multinomial distribution
outcomes <- replicate(10000, which(rmultinom(1,1,rep(1/6,6))==1))
words <- unlist(strsplit("Latent Dirichlet Allocation has many peices", ' '))
ds <- data.frame(id = outcomes, word = sapply(outcomes, function(x)words[x]))
ggplot(ds, aes(x= word)) + geom_histogram(stat='count')

```


## Conjugate Prior: Dirichlet
The conjugate prior for the multinomial distribution is the Dirichlet distribution. Similar
to the beta distribution, it can be though of as a distribution of distributions. Let's first 
take a look at the distribution of both the multinomial and Dirichlet distribution and see if we can find a similar pattern. Remember, conjugate priors have the same form as the estimated posterior of the original distribution. 

General notes on Dirichlet: higher values of beta (or whatever parameter name is), 
the more uniform the probability for each class, the lower the more more likely a specific class is going to be much larger than the rest. 

Let's try to rationalize this in the same way we do bernoulli. Bernoulli is 2 possible outcomes, beta is it's prior. 
Bernoulli is a special case of multinomial where n = 2, so I would assume that means beta is the special case of dirichlet where the number of shape parameters is 2. Think about this a bit more - in the case of beta distribution we use prior data (or assumption) of coin flips - 5 heads, 5 tails - means a = 5, b = 5. From this we make our distribution and we can randomly sample a valid value of theta (and remember we only use theta for bernoulli because if we have one probability, we can infer the other via subtraction from 1, but in reality there is a p value for heads, and a p value for tails... this makes it a bit eaiser to understand the transfer to multinomial) based on this prior information. So it would be no different if we have 3 different words in a topic, we use the prior info as the parameters for the Dirichlet distribution - 3 red, 2 blue, 1 green which would translate to alpha1 = 3, alpha2 = 2, alpha3 = 1, from there we can build our multidimensional distribution and select the most likely value for our thetas (or the probability of a specific outcome where all those p's sum to one).  

[May want to put a plot or two here showing the dirichlet code, but creating the same beta distributions, so 
this makes more sense]

## Gibbs Sampling - Multinomial & Dirichlet

Prior to getting into an example of Gibbs sampling as it applies to inferring the parameters of a multinomial distribution, let's first describe a model which generates words for a single document. As you can imagine this would be modeled as a multinomial distribution with parameters $\overrightarrow{\theta} = \theta_{1}, \theta_{2}, ... \theta_{n}$ for words 1 to n. The model would be capable of generating a bag of words representation of a document. The term <i>'bag of words'</i> refers to words in no particular order, i.e. the document we would be generating would not have structured sentences, but would contain all the components of the document. 

Let's start by defining our model. Let's start with a basic composition for our ideal document. We are going to have a document with only 3 distinct words `r '\U1F4D8'`,`r '\U1F4D5'`,`r '\U1F4D7'`. Remember a document is just a mixture of words, obviously this is not a fine work of literature and has a very limited vocabulary, but it is meant as a basic example of document composition. 

First we are going to create a seed document, i.e. the document that will be used as a basis of our $\alpha$'s for our prior. In order to do this we need to identify the mixture proportions for each word in the vocabulary. 

* `r '\U1F4D8'` : 10%
* `r '\U1F4D5'` : 10%
* `r '\U1F4D7'` : 80%

To clarify this means the document will contain 80% blue books, 10% green books, and 10% red books: 

```{r unigram_document ,echo = TRUE, warning=FALSE, message=FALSE}
# use letters function as your vocabulary
v <- c('red', 'green', 'blue')
nwords <- 10
doc_theta <- c(.1, .1, .8)
document<-rep(v, doc_theta*nwords)
books <- tibble(label = c('blue', 'red', 'green'), 
                    code = c('\U1F4D8', '\U1F4D5', '\U1F4D7'))

cat(sapply(document, function(x) books$code[which(books$label == x)]))
``` 

So what is the structure of the document generator? 

* Alpha -> Dirichlet -> Multinomial

(Maybe here is when you can introduce the terrible idea of block diagrams????,
It might be good to introduce them even though you aren't a fan, for all you know 
people might even understant them better this way...)

Do you recall the beta/bernoulli example? The way we informed our prior was using some prior information we had, i.e. the number of heads and tails previously obtained from flipping the two coins. We will use the document above as the basis of our $\alpha$ paramters for the Dirichlet distribution, i.e. our prior for the multinomial. In more general language, we want to generate documents similar to our 'ideal' document. 


So let's generate a new document using the word counts from our <i>ideal</i> document as our $\alpha$ values for the dirichlet prior. Then we use the $\theta$ values generated by the dirichlet prior as the parameters for a multinomial distribution to generate the next term in the document. 

```{r unigram_generative_model ,echo = TRUE, warning=FALSE, message=FALSE}
# generate text based only on document #1

# leave this code, I like the for loop for creating a single document, shows the process 
# in a straight forward manner, the next code chunk shows a faster way, which is good as well

words <- document
# lenght of new document
 #rep(1,length(unique(words)))
word_counts <- table(words)
alphas <-  word_counts
new_doc <- rep('', nwords)
for(i in 1:nwords){
  set.seed(i)
  p = rdirichlet(1,alphas)
  set.seed(i)
  new_doc[i] <- names(word_counts)[which(rmultinom(1, 1, p) == 1)]
}
table(new_doc)

cat('\n', sapply(new_doc, function(x) books$code[which(books$label == x)]))
```
It's not quite the same as the original, but that should be expected. This is a model that generates documents probabalistically based on some prior information. So let's make a few more and see how this changes. 

```{r multiple_docs_mul_dir ,echo = TRUE, warning=FALSE, message=FALSE}

word_counts <- table(words)
alphas <-  word_counts
nwords <- 10
ndocs  <- 5
word_encodings <- tibble(label = c('blue', 'red', 'green'), 
                code = c('\U1F4D8', '\U1F4D5', '\U1F4D7'), 
                word_props = c(.1, .1, .8))

thetas <- rdirichlet(ndocs*nwords, alphas)
print(head(thetas))

selected_words <- apply(thetas, 1, function(x) which(rmultinom(1,1,x)==1))

ds <- tibble(doc_id = rep(1:ndocs, each = nwords),
             word = word_encodings$label[selected_words], 
             word_uni = word_encodings$code[selected_words])


ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word_uni, collapse = ' ')
) %>% kable(col.names = c('Document', 'Words'))

```


As we can see each document composition is similar, but the word counts and order are different each time. This is to be expected (maybe say why? )

So now onto inferernce ....

The process above is known as a generative model. We created documents using a model with a given set of parameters. Inference is going to take this general concept and look at it from a different angle. Instead of generating documents with our model we are going to take a series of pre-existing documents and infer what model created them. We are going to make the assumption that the structure of the model is the same as the generative example, i.e. all documents are generated based on the same word mixture ratios. 

Let's use the 10 documents we previously generated as our basis and see if we can infer the parameters used to generate them. 

```{r}

# counts -> alphas
# for each of the 3 thetas iterate 1k times
# plot distributions
alphas <- rep(1,nrow(books))
# alphas <- 1:6 
n <- table(ds$word)
head(n)


niters = 2000
burnin = 500

thetas = matrix(0, nrow = (niters-burnin), ncol=nrow(books), 
                dimnames = list(NULL, c(names(n))))
for (i in 1:niters){
  theta = rdirichlet(1,n+alphas)

  
  if (i >= burnin){
    thetas[(i-burnin), ] = theta
  }
}

 # hist(thetas[, 1])
 # hist(thetas[, 2])
 # 
 df <- as.tibble(thetas) %>% 
   gather(word, theta)
# map book colors to each segment of plot to avoid bothering with the emoji labels (for now)
ggplot(df, aes(y=theta, x = word)) + geom_violin()
# apply(thetas, 2, median)
# n/sum(n)
```


We can see our mixture estimates are significantly different from the real model used to generate the documents. So why is this? One of the issues here is that our sample are documents with only 10 words. Therefore an average document has 8 ..., 1 ..., and 1..., but it is not unusual to see a slight variation which causes mixture shifts of 10% or more. And with  Let's try the same example but this time instead of only generating 5 documents we will genertate 500 and use this as our sample to draw inference from.  

```{r}

word_counts <- table(words)
alphas <-  word_counts
nwords <- 10
ndocs  <- 500
word_encodings <- tibble(label = c('blue', 'red', 'green'), 
                code = c('\U1F4D8', '\U1F4D5', '\U1F4D7'), 
                word_props = c(.1, .1, .8))

thetas <- rdirichlet(ndocs*nwords, alphas)
print(head(thetas))

selected_words <- apply(thetas, 1, function(x) which(rmultinom(1,1,x)==1))

ds <- tibble(doc_id = rep(1:ndocs, each = nwords),
             word = word_encodings$label[selected_words], 
             word_uni = word_encodings$code[selected_words])


# counts -> alphas
# for each of the 3 thetas iterate 1k times
# plot distributions



alphas <- rep(1,nrow(books))
# alphas <- 1:6 
n <- table(ds$word)
head(n)


niters = 2000
burnin = 500

thetas = matrix(0, nrow = (niters-burnin), ncol=nrow(books), 
                dimnames = list(NULL, c(names(n))))
for (i in 1:niters){
  theta = rdirichlet(1,n+alphas)

  
  if (i >= burnin){
    thetas[(i-burnin), ] = theta
  }
}

 # hist(thetas[, 1])
 # hist(thetas[, 2])
 # 
 df <- as.tibble(thetas) %>% 
   gather(word, theta)
# map book colors to each segment of plot to avoid bothering with the emoji labels (for now)
ggplot(df, aes(y=theta, x = word)) + geom_violin()
# apply(thetas, 2, median)
n/sum(n)
```

