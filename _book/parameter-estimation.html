<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>LDA Tutorial</title>
  <meta name="description" content="LDA Tutorial">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="LDA Tutorial" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="LDA Tutorial" />
  <meta name="twitter:site" content="@devlintufts" />
  
  

<meta name="author" content="Chris Tufts">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="what-is-lda.html">
<link rel="next" href="multinomial-distribution.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="" data-path="layout-of-book.html"><a href="layout-of-book.html"><i class="fa fa-check"></i>Layout of Book</a></li>
<li class="chapter" data-level="" data-path="package-references.html"><a href="package-references.html"><i class="fa fa-check"></i>Package References</a></li>
<li class="chapter" data-level="1" data-path="what-is-lda.html"><a href="what-is-lda.html"><i class="fa fa-check"></i><b>1</b> What is LDA?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#animal-generator"><i class="fa fa-check"></i><b>1.1</b> Animal Generator</a><ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#generating-the-mixtures"><i class="fa fa-check"></i><b>1.1.1</b> Generating the Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-lda.html"><a href="what-is-lda.html#inference"><i class="fa fa-check"></i><b>1.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#distributions"><i class="fa fa-check"></i><b>2.1</b> Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bernoulli"><i class="fa fa-check"></i><b>2.1.1</b> Bernoulli</a></li>
<li class="chapter" data-level="2.1.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#beta-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#inference-the-building-blocks"><i class="fa fa-check"></i><b>2.2</b> Inference: The Building Blocks</a></li>
<li class="chapter" data-level="2.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-likelihood"><i class="fa fa-check"></i><b>2.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>2.4</b> Maximum a Posteriori (MAP)</a></li>
<li class="chapter" data-level="2.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bayesian-inference"><i class="fa fa-check"></i><b>2.5</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.5.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#the-issue-of-intractability"><i class="fa fa-check"></i><b>2.5.1</b> The Issue of Intractability</a></li>
<li class="chapter" data-level="2.5.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#a-tale-of-two-mcs"><i class="fa fa-check"></i><b>2.5.2</b> A Tale of Two MC’s</a></li>
<li class="chapter" data-level="2.5.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#conjugate-priors"><i class="fa fa-check"></i><b>2.5.3</b> Conjugate Priors</a></li>
<li class="chapter" data-level="2.5.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.5.4</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.5.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bias-of-two-coins"><i class="fa fa-check"></i><b>2.5.5</b> Bias of Two Coins</a></li>
<li class="chapter" data-level="2.5.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html#change-point-example"><i class="fa fa-check"></i><b>2.5.6</b> Change Point Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html"><i class="fa fa-check"></i><b>3</b> Multinomial Distribution</a><ul>
<li class="chapter" data-level="3.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#comparison-of-dice-vs.words"><i class="fa fa-check"></i><b>3.1</b> Comparison of Dice vs. Words</a></li>
<li class="chapter" data-level="3.2" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#relationship-to-bernoulli"><i class="fa fa-check"></i><b>3.2</b> Relationship to Bernoulli</a></li>
<li class="chapter" data-level="3.3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#conjugate-prior-dirichlet"><i class="fa fa-check"></i><b>3.3</b> Conjugate Prior: Dirichlet</a></li>
<li class="chapter" data-level="3.4" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#gibbs-sampling---multinomial-dirichlet"><i class="fa fa-check"></i><b>3.4</b> Gibbs Sampling - Multinomial &amp; Dirichlet</a><ul>
<li class="chapter" data-level="3.4.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#derivation-of-gibbs-sampling-solution-of-word-distribution-single-doc"><i class="fa fa-check"></i><b>3.4.1</b> Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="word-representations.html"><a href="word-representations.html"><i class="fa fa-check"></i><b>4</b> Word Representations</a><ul>
<li class="chapter" data-level="4.1" data-path="word-representations.html"><a href="word-representations.html#bag-of-words"><i class="fa fa-check"></i><b>4.1</b> Bag of Words</a></li>
<li class="chapter" data-level="4.2" data-path="word-representations.html"><a href="word-representations.html#word-counts"><i class="fa fa-check"></i><b>4.2</b> Word Counts</a></li>
<li class="chapter" data-level="4.3" data-path="word-representations.html"><a href="word-representations.html#plug-and-play-lda"><i class="fa fa-check"></i><b>4.3</b> Plug and Play LDA</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html"><i class="fa fa-check"></i><b>5</b> LDA as a Generative Model</a><ul>
<li class="chapter" data-level="5.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#general-terminology"><i class="fa fa-check"></i><b>5.1</b> General Terminology</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#selecting-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Selecting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generative-model"><i class="fa fa-check"></i><b>5.2</b> Generative Model</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generating-documents"><i class="fa fa-check"></i><b>5.2.1</b> Generating Documents</a></li>
<li class="chapter" data-level="5.2.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#lda-generative-model"><i class="fa fa-check"></i><b>5.2.2</b> LDA Generative Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lda-inference.html"><a href="lda-inference.html"><i class="fa fa-check"></i><b>6</b> LDA Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="lda-inference.html"><a href="lda-inference.html#general-overview"><i class="fa fa-check"></i><b>6.1</b> General Overview</a></li>
<li class="chapter" data-level="6.2" data-path="lda-inference.html"><a href="lda-inference.html#mathematical-derivations-for-inference"><i class="fa fa-check"></i><b>6.2</b> Mathematical Derivations for Inference</a></li>
<li class="chapter" data-level="6.3" data-path="lda-inference.html"><a href="lda-inference.html#animal-farm---code-example"><i class="fa fa-check"></i><b>6.3</b> Animal Farm - Code Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">LDA Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parameter-estimation" class="section level1">
<h1><span class="header-section-number">2</span> Parameter Estimation</h1>
<p>LDA is a generative probabilistic model, so to understand exactly how this works we need to understand the underlying probability distibutions. In this chapter we will focus on the Bernoulli distribution and the Beta distribution. Both of these distributions are very closely related to (and also special cases of) the multinomial and Dirichlet distributions utilized by LDA, but they are a bit easier to comprehend. Once we have made our way through Bernoulli and beta, the following chapter will go into detail about multinomial and Dirichlet distributions and how all these peices are connected.</p>
<p>Throughout the chapter I’m going to build off of a simple example - a single coin flip. Let’s begin.</p>
<div id="distributions" class="section level2">
<h2><span class="header-section-number">2.1</span> Distributions</h2>
<div id="bernoulli" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Bernoulli</h3>
<p>When you flip a coin you get either heads or tails as an outcome (barring the possibility it lands on it’s side). This single coin flip is an example of a Bernoulli trial and we can use the Bernoulli distribution to calculate the probability of either outcome. Any <i>single</i> trial with two possible outcomes can be modeled as a Bernoulli trial: team wins/loses, pitch is a strike/ball, coin comes up heads or tails, etc.</p>
<div id="bernoulli-a-special-case-of-the-binomial-distribution" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Bernoulli: A Special Case of the Binomial Distribution</h4>
<p>You will often see Bernoulli distribution mentioned as a special case of the Binomial distribution. The binomial model consists of <em>n</em> Bernoulli trials, where each trial is independent and the probability of success does not change between trials.<span class="citation">(Kerns <a href="references.html#ref-kerns2010introduction">2010</a>)</span>. The Bernoulli distribution is the case of a single trial or <em>n</em>=1.</p>
<p><strong>NOTE:</strong> I will use the term <i>success</i> interchangably with the term <i>heads</i> when describing bernoulli distribution. In reality <i>success</i> could be <i>tails</i> if you choose to define it that way.</p>
<p>To clarify, if I want to calculate the probability of getting heads on a single coin flip I will use a bernoulli distribution. However, if I want to know the probability of getting 2 heads (or more) in a row, this is where the binomial distribution comes in. For our purposes we are only concerned about the outcome of a single coin flip and will therefore stick to the Bernoulli distribution.</p>
<div class="figure"><span id="fig:BernVSBin"></span>
<img src="Images/Bern_Binomial.png" alt="Bernoulli and Binomial Distributions"  />
<p class="caption">
Figure 2.1: Bernoulli and Binomial Distributions
</p>
</div>
</div>
<div id="bernoulli---distribution-notation" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> Bernoulli - Distribution Notation</h4>
<p>The probability mass function of the bernoulli distribution is shown in Equation <a href="parameter-estimation.html#eq:bernPMF">(2.1)</a>.</p>
<p><span class="math display" id="eq:bernPMF">\[
\begin{equation}
  f_{x}(x)=P(X=x)=\theta^{x}(1-\theta)^{1-x}, \hspace{1cm} x = \{0,1\} 
  \tag{2.1}
\end{equation}
\]</span></p>
<p>The only parameter of the bernoulli distribution is <span class="math inline">\(\theta\)</span>, which defines the probability of success during a bernoulli trial. The value of <em>x</em> is 0 for a failure and 1 for a success. In a practical example you can think of this as 0 for tails and 1 for heads during a coin flip. In Equation <a href="parameter-estimation.html#eq:bernPMFExample">(2.2)</a> the value of <span class="math inline">\(\theta\)</span> is set to 0.7. We can see the probability of getting a success is 0.7, while the probability of failure is 0.3.</p>
<p><span class="math display" id="eq:bernPMFExample">\[
\begin{equation}  
\begin{aligned}
P(X=1)&amp;=\theta^{1}(1-\theta)^{1-1}, \hspace{1cm} \theta=0.7 \\
P(X=1)&amp;=0.7*1=0.7 \\\\
P(X=0)&amp;=0.7^{0}(1-0.7)^{1-0}\\
P(X=0)&amp;=0.3
\end{aligned}
\tag{2.2}
\end{equation}
\]</span></p>
</div>
</div>
<div id="beta-distribution" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Beta Distribution</h3>
<p>The beta distribution can be thought of as a probability distribution of distributions<span class="citation">(Robinson <a href="references.html#ref-Robinson2014beta">2014</a>)</span>.</p>
<p>We know the bernoulli distribution has one parameter, <span class="math inline">\(\theta\)</span>. We can use the beta distribution to determine the probability of a specific value of <span class="math inline">\(\theta\)</span> based on prior information, in our case previous coin flips.</p>
<p>If you flip a coin 2 times resulting in 1 heads and 1 tails how sure are you that the coin is fair? Probably not all that sure, right? But what if you flipped the coin 200 times and it resulted in 100 heads and 100 tails? You would be much more confident that the coin is fair. This is the basis of the beta distribution.</p>
<p>The beta distribution has 2 shape parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. These can be though of as the results from the coin flips we just talked about. Below the probability density for different values of <span class="math inline">\(\theta\)</span> is displayed based on different values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. In general, the higher the value of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> the narrower the density curve is. This makes sense with our thought example above, the more information (<em>coin flip results</em>) we have, the more confident we are in our coin’s bias (i.e. is it fair, head heavy, etc.).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)

a &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>)
b &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>)
params &lt;-<span class="st"> </span><span class="kw">cbind</span>(a,b)
ds &lt;-<span class="st"> </span><span class="ot">NULL</span>
n &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(params)){
  ds &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> n, <span class="dt">y =</span> <span class="kw">dbeta</span>(n, params[i,<span class="dv">1</span>], params[i,<span class="dv">2</span>]),
                             <span class="dt">parameters =</span> <span class="kw">paste0</span>(<span class="st">&quot;\U03B1 = &quot;</span>,params[i,<span class="dv">1</span>],
                                                 <span class="st">&quot;, \U03B2 = &quot;</span>, params[i,<span class="dv">2</span>])), ds)
}

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">color=</span>parameters)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Probability Density&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">name=</span><span class="ot">NULL</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:betashape"></span>
<img src="_main_files/figure-html/betashape-1.png" alt="Beta Distribution" width="672" />
<p class="caption">
Figure 2.2: Beta Distribution
</p>
</div>
<p>What about the cases where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are not equal or close to equal? Well in those cases you would probably assume a bit of skew in the distribution, i.e. your coin may be biased toward head or tails as shown below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">2</span>)
b &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">8</span>)
params &lt;-<span class="st"> </span><span class="kw">cbind</span>(a,b)
ds &lt;-<span class="st"> </span><span class="ot">NULL</span>
n &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(params)){
  ds &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> n, <span class="dt">y =</span> <span class="kw">dbeta</span>(n, params[i,<span class="dv">1</span>], params[i,<span class="dv">2</span>]),
                             <span class="dt">parameters =</span> <span class="kw">paste0</span>(<span class="st">&quot;\U03B1 = &quot;</span>,params[i,<span class="dv">1</span>],
                                                 <span class="st">&quot;, \U03B2 = &quot;</span>, params[i,<span class="dv">2</span>])), ds)
}

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">color=</span>parameters)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Probability Density&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">name=</span><span class="ot">NULL</span>, <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#7A99AC&quot;</span>, <span class="st">&quot;#E4002B&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:betaShapeSkewed"></span>
<img src="_main_files/figure-html/betaShapeSkewed-1.png" alt="Beta Distribution - Skewed" width="672" />
<p class="caption">
Figure 2.3: Beta Distribution - Skewed
</p>
</div>
<p>The probability distribution function for the beta distribution can be found in Equation <a href="parameter-estimation.html#eq:betaPDF">(2.3)</a>.</p>
<p><span class="math display" id="eq:betaPDF">\[
\begin{equation}
f(\theta;\alpha,\beta) ={{\theta^{(\alpha-1)}(1-\theta)^{(\beta-1)}}\over B(\alpha,\beta)}
\tag{2.3}
\end{equation}
\]</span></p>
<p>Quick Note:</p>
<ul>
<li>The <em>Beta</em> function, <em>B</em> is the ratio of the product of the <em>Gamma</em> function, <span class="math inline">\(\Gamma\)</span>, of each parameter divided by the <em>Gamma</em> function of the sum of the parameters. The <em>Beta</em> function is <strong>not</strong> the same as the beta distribution. The <em>Beta</em> function is shown below along with the <em>Gamma</em> function, which is used in the <em>Beta</em> function.</li>
</ul>
<p><span class="math display" id="eq:betaFunction">\[
\begin{equation}
\beta(a,b) = {\Gamma(a)\Gamma(b) \over{\Gamma(a+b)}}
\tag{2.4}
\end{equation}
\]</span></p>
<ul>
<li>The <em>Gamma</em> function is the factorial of the parameter minus 1.</li>
</ul>
<p><span class="math display" id="eq:gammaFunction">\[
\begin{equation}
\Gamma(a) = (a-1)!
\tag{2.5}
\end{equation}
\]</span></p>
</div>
</div>
<div id="inference-the-building-blocks" class="section level2">
<h2><span class="header-section-number">2.2</span> Inference: The Building Blocks</h2>
<p>Equation <a href="parameter-estimation.html#eq:posterior">(2.6)</a> is a fundamental to understanding parameter estimation and inference.</p>
<p><span class="math display" id="eq:posterior">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior} = {\overbrace{p(D|\theta)}^{likelihood}
  \overbrace{p(\theta)}^{prior} \over \underbrace{p(D)}_{evidence}}
  \tag{2.6}
\end{equation}
\]</span></p>
<p>The 4 components are:</p>
<ul>
<li><strong>Prior</strong>: The probability of the parameter(s). Defines our prior beliefs of the parameter. Do we <em>believe</em> to a good degree of certainty that the coin is fair? Maybe take a step back and ask yourself ‘do I trust the manufacturer of this coin?’. If this manufacturer has always had great quality (i.e. fair coins) then you would have some confidence that your case is no different.</li>
<li><strong>Posterior</strong>: The probability of the parameter <strong>given</strong> the evidence. The only way to know this value is to already have the evidence. However we can estimate the posterior in various ways. Think of it this way, given 100 coin flips with 47 heads and 53 tails what is the probability that theta is 0.5 (coin is fair)?</li>
<li><strong>Likelihood</strong>: The probability of the evidence <strong>given</strong> the parameter. Given that we know the coin is fair (theta = 0.5) what is the probability of having 47 heads out of 100 flips?</li>
<li><strong>Evidence</strong>: The probability of all possible outcomes. Probability of 1/100 heads, 2/100 heads, …, 100/100 heads.</li>
</ul>
<p><strong>Conditioning your brain for LDA</strong> : We are starting with a coin flip, but the eventual goal is to link this back to words appearing in a document. Try to keep in mind that we think of a word similar to the outcome of a coin: a word exists in the document (heads!) or a word doesn’t exist in the document (tails!).</p>
</div>
<div id="maximum-likelihood" class="section level2">
<h2><span class="header-section-number">2.3</span> Maximum Likelihood</h2>
<p>The simplest method of parameter estimation is the maximum likelihood method. Effectively we calculate the parameter that maximizes the likelihood.</p>
<p><span class="math display" id="eq:posteriorLIKE">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior} = {\overbrace{\bf \Large p(D|\theta)}^{\bf \Large LIKELIHOOD}
  \overbrace{p(\theta)}^{prior} \over \underbrace{p(D)}_{evidence}}
\tag{2.7}
\end{equation}
\]</span></p>
<p>Let’s first discuss what the likelihood is. The likelihood can be described as the probability of getting observed data given a specified value of the parameter, <span class="math inline">\(\theta\)</span>. For example, let’s say I’ve flipped a coin 10 times and got 5 heads, 5 tails. Assuming the coin is fair, <span class="math inline">\(\theta\)</span> equals 0.5, what is the likelihood of observing 5 heads and 5 tails.</p>
<p>To calculate the likelihood of a parameter given a single outcome we would use the probability mass function:</p>
<p><span class="math display" id="eq:bernPMF2">\[
\begin{equation}
P(X=x)=\theta^{x}(1-\theta)^{1-x}, \hspace{1cm} x = \{0,1\} 
\tag{2.8}
\end{equation}
\]</span> Where an outcome of heads equal 1 and tails is 0. Now let’s say we have carried out the 10 flips as mentioned previously:</p>
<p><span class="math display" id="eq:bernL">\[
\begin{equation}
\begin{aligned}
P(X_{1}=x_{1},X_{2}=x_{2},...,X_{10}=x_{10}) &amp;= \prod\limits_{n=1}^{10} \theta^{x}(1-\theta)^{1-x}\\
L(\theta) &amp;= \prod\limits_{n=1}^{10} \theta^{x}(1-\theta)^{1-x}
\end{aligned}
\tag{2.9}
\end{equation}
\]</span> What is shown in Equation <a href="parameter-estimation.html#eq:bernL">(2.9)</a> is the joint probability mass function. Each coin flip is independent so we calculate the product of the PMF’s for each trial. This is known as the likelihood function - the likelihood of a value of <span class="math inline">\(\theta\)</span> given our observed data.</p>
<p>Back to the maximum likelihood….</p>
<p>Our goal is to find the value of <span class="math inline">\(\theta\)</span> which maximizes the likelihood of the observed data. To derive the maximum likelihood we start by taking the log of the likelihood, <span class="math inline">\(\mathcal{L}\)</span>.</p>
<p><span class="math display" id="eq:bernLogL">\[
\begin{equation}
\begin{aligned}
\mathcal{L} &amp;= log \prod\limits_{n=1}^N \theta^{x}(1-\theta)^{1-x} \\\\
 &amp;= \sum\limits_{n=1}^N log(\theta^{x}(1-\theta)^{1-x}) \\\\
 &amp;= n^{(1)}log(\theta) + n^{(0)}log(1-\theta)
\end{aligned}
\tag{2.10}
\end{equation}
\]</span></p>
<p>Differentiate with respect to <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display" id="eq:bernDiffL">\[
\begin{equation}
{d\mathcal{L} \over d\theta} =  {n^{(1)}\over \theta} - {n^{(0)}\over 1-\theta}
\tag{2.11}
\end{equation}
\]</span></p>
<p>Set it equal to zero and solve:</p>
<p><span class="math display" id="eq:bernLTheta">\[
\begin{equation}
\begin{aligned}
{n^{(1)}\over \theta} - {n^{(0)}\over 1-\theta} &amp;= 0  \\ \\
{n^{(1)}\over\theta} &amp;= {n^{(0)}\over 1-\theta} \\ \\
{n^{1} - \theta n^{1}} &amp;= {\theta n^{0}} \\ \\
n^{(1)} &amp;= \theta(n^{(1)} + n^{0}) \\ \\
\theta &amp;= {n^{(1)} \over N}
\end{aligned}
\tag{2.12}
\end{equation}
\]</span></p>
<p>The value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood is the number of heads over the number of flips.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">heads =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span>
flips =<span class="st"> </span><span class="dv">10</span>
ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(heads, <span class="dt">flips =</span> <span class="kw">rep</span>(flips, <span class="kw">length</span>(heads)))
ds<span class="op">$</span>theta &lt;-<span class="st"> </span>ds<span class="op">$</span>heads<span class="op">/</span>ds<span class="op">$</span>flips


<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x =</span> heads,
               <span class="dt">y =</span> theta)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color =</span><span class="st">&#39;#1520c1&#39;</span>, <span class="dt">size =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="kw">aes</span>(<span class="dt">x=</span>heads,
                     <span class="dt">y=</span><span class="ot">NULL</span>, <span class="dt">ymax=</span>theta,
                     <span class="dt">ymin=</span><span class="dv">0</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">2</span>), <span class="dt">labels =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">2</span>)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">x=</span><span class="st">&quot;Number of Heads&quot;</span>, <span class="dt">title =</span><span class="st">&quot;ML Parameter Estimation: 10 Bernoulli Trials&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:bernoulliml"></span>
<img src="_main_files/figure-html/bernoulliml-1.png" alt="Bernoulli Maximum Likelihood" width="672" />
<p class="caption">
Figure 2.4: Bernoulli Maximum Likelihood
</p>
</div>
</div>
<div id="maximum-a-posteriori-map" class="section level2">
<h2><span class="header-section-number">2.4</span> Maximum a Posteriori (MAP)</h2>
<p>MAP is similar to the method of maximum likelihood estimation, but it also let’s us include information about our prior beliefs. Unlike ML estimation, MAP estimates parameters by trying to maximize the posterior.</p>
<p><span class="math display" id="eq:bernMAP">\[
\begin{equation}
\theta_{MAP}=\underset{\theta}{\operatorname{argmax}} P(\theta|X)
\tag{2.13}
\end{equation}
\]</span></p>
<p><span class="math display" id="eq:bernPMFDropEvidence">\[
\begin{equation}
\require{enclose}
\theta_{MAP}=\underset{\theta}{\operatorname{argmax}}{\overbrace{p(D|\theta)}^{likelihood}
  \overbrace{p(\theta)}^{prior} \over \underbrace{\enclose{horizontalstrike}{p(D)}}_{evidence}}
\tag{2.14}
\end{equation}
\]</span></p>
<p>The evidence term is dropped during the calculation of <span class="math inline">\(\theta_{MAP}\)</span> since it is not a function of <span class="math inline">\(\theta\)</span> and our only concern is maximizing the posterior based on <span class="math inline">\(\theta\)</span>. Similar to calculating the Likelhood, the first step is to apply the log function to the remaining terms.</p>
<p><span class="math display" id="eq:bernPrior" id="eq:bernMAPLog">\[
\begin{equation}
\begin{aligned}
\theta_{MAP} &amp;=\underset{\theta}{\operatorname{argmax}}p(D|\theta)p(\theta) \\\\
&amp;= \mathcal{L}(\theta|D) + log(p(\theta))
\end{aligned}
\tag{2.15}
\end{equation}
\]</span></p>
<p>We have already derived the log likelihood during our derivation of the maximum likelihood, so let’s now focus on the prior. The prior for the Bernoulli distribution is the Beta distribution and can be used to describe <span class="math inline">\(p(\theta)\)</span>. The probability distribution function, PDF, for the Beta distribution is shown in Equation @ref\tag{2.16}.</p>
<p><span class="math display" id="eq:bernPrior">\[
\begin{equation}
p(\theta|\alpha,\beta) = {\theta^{\alpha-1}(1-\theta)^{\beta-1}\over{B(\alpha, \beta)}}
\tag{2.16}
\end{equation}
\]</span></p>
<p>Plugging in the PDF of the beta distribution for the prior:</p>
$$
<span class="math display" id="eq:bernMAPTheta">\[\begin{equation}
\begin{aligned}
\theta_{MAP}&amp;= \mathcal{L}(\theta|D) + log(p(\theta)) \\\\
\theta_{MAP}&amp;= n^{(1)}log(\theta) + n^{(0)}log(1-\theta) + log({\theta^{\alpha-1}(1-\theta)^{\beta-1}\over{B(\alpha, \beta)}}) \\\\

\theta_{MAP}&amp;= n^{(1)}log(\theta) + n^{(0)}log(1-\theta)  + \\ &amp;\quad log({\theta^{\alpha-1}) + log((1-\theta)^{\beta-1}) - log({B(\alpha, \beta)}}) \\\\
{d \over d\theta} \mathcal{L}(\theta|D) + log(p(\theta)) &amp;= {n^{(1)}\over \theta} - {n^{(0)}\over 1-\theta} + {\alpha - 1\over\theta} - {\beta - 1 \over 1-\theta}
\\\\
0 &amp;= {n^{(1)}\over \theta} - {n^{(0)}\over 1-\theta} + {\alpha - 1\over\theta} - {\beta - 1 \over 1-\theta}
\\\\
\theta_{MAP}&amp;= {{n^{(1)} + \alpha -1} \over {n^{(1)} + n^{0} + \alpha + \beta - 2}}
\end{aligned}
\tag{2.17}
\end{equation}\]</span>
<p>$$</p>
<p>Now that we know how to calculate the parameter <span class="math inline">\(\theta\)</span> that maximizes the posterior, lets take a look at how choices of different priors and different number of observed trials effects our outcome.</p>
<p>In the figure below we see MAP estimation of <span class="math inline">\(\theta\)</span> with a relatively uninformed prior and a small number of observed experiments (n=20). Uninformed means we are going to make a weak assumption about the prior. In more general terms, this means that we don’t have a strong intuition that our coin is fair or unfair. The Beta distribution has <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> parameters of 2 resulting in the blue density curve shown below.</p>
<p><strong>NOTE</strong>: The terms weak and uninformed are often used interchangeably when referencing priors. The same goes for the terms strong and informed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># description:</span>
<span class="co"># ml will yeild the expected average and is not effected by the prior</span>
<span class="co"># map uses a weak assumption - uniform density for all values of theta</span>
<span class="co"># this results in a theta_map value very similar to the ml value</span>
n &lt;-<span class="st"> </span><span class="dv">20</span>
heads &lt;-<span class="st"> </span><span class="dv">14</span>
tails &lt;-<span class="st"> </span><span class="dv">6</span>


<span class="co"># ml</span>
ml_theta &lt;-<span class="st"> </span>heads<span class="op">/</span>n
<span class="co"># map</span>
B &lt;-<span class="st"> </span><span class="dv">2</span>
alpha &lt;-<span class="st"> </span><span class="dv">2</span>

map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(heads <span class="op">+</span><span class="st"> </span>tails <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>B <span class="op">-</span><span class="dv">2</span>)
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">y=</span><span class="fl">0.5</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[MAP]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:mapSmallnUninformedPrior"></span>
<img src="_main_files/figure-html/mapSmallnUninformedPrior-1.png" alt="MAP: Small number of experiments and uninformed prior" width="672" />
<p class="caption">
Figure 2.5: MAP: Small number of experiments and uninformed prior
</p>
</div>
<p>In the next example we have the same number of samples, but we assume a much stronger prior. In particular, we assume the coin is most likely fair by selecting <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> parameters of 100. These parameters represent a Beta distribution that is centered at 0.5 and is very dense around that point. We can see the resuting MAP estimate is much closer to 0.5, which is the value of <span class="math inline">\(\theta\)</span> where the Beta distribution is most dense.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># description:</span>
<span class="co"># the strong assumption of a &#39;fair&#39; coin prior reduces the width of the </span>
<span class="co"># distribution, i.e. much higher probability density near theta (p) of 0.5</span>
<span class="co"># This forces the MAP theta value to stay much closer to the prior due to the </span>
<span class="co"># small amount of observed evidence</span>

n &lt;-<span class="st"> </span><span class="dv">20</span>
heads &lt;-<span class="st"> </span><span class="dv">14</span>
tails &lt;-<span class="st"> </span><span class="dv">6</span>


<span class="co"># ml</span>
ml_theta &lt;-<span class="st"> </span>heads<span class="op">/</span>n
<span class="co"># map</span>
B &lt;-<span class="st"> </span><span class="dv">100</span>
alpha &lt;-<span class="st"> </span><span class="dv">100</span>

map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(heads <span class="op">+</span><span class="st"> </span>tails <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>B <span class="op">-</span><span class="dv">2</span>)
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">y=</span><span class="dv">11</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[MAP]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:mapSmallnInformedPrior"></span>
<img src="_main_files/figure-html/mapSmallnInformedPrior-1.png" alt="MAP: Small number of experiments and informed prior" width="672" />
<p class="caption">
Figure 2.6: MAP: Small number of experiments and informed prior
</p>
</div>
<p>What happens when we have a much larger number of observed experiments and an uninformed prior? In general, the MAP estimate gives us a value that is very close to a likelihood, heads divide by number of trials.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># description</span>
<span class="co"># high number of observed samples (evidence)</span>
<span class="co"># weak prior - uniform </span>
<span class="co"># ml and map are close to one another - close... as expected</span>
n &lt;-<span class="st"> </span><span class="dv">1000</span>
heads &lt;-<span class="st"> </span><span class="dv">723</span>
tails &lt;-<span class="st"> </span>n<span class="op">-</span>heads


<span class="co"># ml</span>
ml_theta &lt;-<span class="st"> </span>heads<span class="op">/</span>n
<span class="co"># map</span>
B &lt;-<span class="st"> </span><span class="dv">2</span>
alpha &lt;-<span class="st"> </span><span class="dv">2</span>
map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(heads <span class="op">+</span><span class="st"> </span>tails <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>B <span class="op">-</span><span class="dv">2</span>)
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">y=</span><span class="fl">1.2</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[MAP]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:mapLargenUninformedPrior"></span>
<img src="_main_files/figure-html/mapLargenUninformedPrior-1.png" alt="MAP: Large number of experiments and uninformed prior" width="672" />
<p class="caption">
Figure 2.7: MAP: Large number of experiments and uninformed prior
</p>
</div>
<p>Now let’s assume a much stronger prior that the coin is fair while using the same number of experiments. Notice that when we use a larger number of experiments it overpowers the strong prior and gives us a very similar MAP estimate in comparison to the example with the uninformed prior and the same number of experiments.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
heads &lt;-<span class="st"> </span><span class="dv">723</span>
tails &lt;-<span class="st"> </span>n<span class="op">-</span>heads


<span class="co"># ml</span>
ml_theta &lt;-<span class="st"> </span>heads<span class="op">/</span>n
<span class="co"># map</span>
B &lt;-<span class="st"> </span><span class="dv">100</span>
alpha &lt;-<span class="st"> </span><span class="dv">100</span>
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(heads <span class="op">+</span><span class="st"> </span>tails <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>B <span class="op">-</span><span class="dv">2</span>)
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">y=</span><span class="fl">8.2</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[MAP]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:mapLargeInformedPrior"></span>
<img src="_main_files/figure-html/mapLargeInformedPrior-1.png" alt="MAP: Large number of experiments and informed prior" width="672" />
<p class="caption">
Figure 2.8: MAP: Large number of experiments and informed prior
</p>
</div>
<p>In summary:</p>
<ul>
<li>The stronger your prior assumptions are, the more observations you will need to overcome an incorrect prior.</li>
<li>Stronger prior - MAP estimate moves toward most dense area of prior distribution</li>
<li>Weaker prior - MAP looks more like a maximum likelihood</li>
</ul>
</div>
<div id="bayesian-inference" class="section level2">
<h2><span class="header-section-number">2.5</span> Bayesian Inference</h2>
<p>Another option we have for estimating parameters is to estimate the posterior of the distribution via Bayesian inference. In MAP estimation we included prior assumptions as part of our calculation. We are going to do the same via Bayesian Inference however instead of a point estimate of <span class="math inline">\(\theta\)</span>, we will be calculating the posterior distribution over all possible values of <span class="math inline">\(\theta\)</span>. From this we can take the expected value of <span class="math inline">\(\theta\)</span> as our estimated parameter.</p>
<p>In the case of the coin flip, Bayesian inference can be solved analytically. We have reviewed the likelihood and the prior, but when estimating the posterior we need to include the evidence term. This is somewhat tricky.</p>
<p><span class="math display" id="eq:PosteriorBI">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior} = { p(D|\theta)
  p(\theta) \over \underbrace{p(D)}_{evidence}}
\tag{2.18}
\end{equation}
\]</span></p>
<p><span class="math display" id="eq:evidence">\[
\begin{equation}
p(D) = \int_{\theta}p(D|\theta)p(\theta)d\theta
\tag{2.19}
\end{equation}
\]</span></p>
<p><span class="math display" id="eq:bernBI">\[
\begin{equation}
{p(\theta|z,N)} = {\overbrace{\theta^z(1-\theta)^{(N-z)}}^{likelihood} \overbrace{{\theta^{(a-1)}(1-\theta)^{(b-1)}}\over \beta(a,b)}^{prior}\over{\underbrace{\int_{0}^1 \theta^z(1-\theta)^{(N-z)}{{\theta^{(a-1)}(1-\theta)^{(b-1)}}\over \beta(a,b)}d\theta}_{Evidence}}}
\tag{2.20}
\end{equation}
\]</span></p>
<p>Note that the evidence term is actually a constant value since it is an integral over all values of <span class="math inline">\(\theta\)</span>. The evidence term is used as a scaling factor to ensure our probabilities sum to 1. We will plug in a generic placeholder of our evidence value since we have established that it is a constant.</p>
<p><span class="math display" id="eq:bernBIFinal">\[
\begin{equation}
\begin{aligned}
p(\theta|z, N) &amp;= {\theta^{z}(1-\theta)^{(N-z)}{{\theta^{(a-1)}(1-\theta)^{(b-1)}}\over \beta(a,b)}\over{C}} \\\\
 &amp;= {\theta^{(z+a-1)}(1-\theta)^{(N-z+b-1)} \over \beta(z+a, N-z+b)}\\\\
 &amp;= Beta(z+a, N-z+b)
\end{aligned}
\tag{2.21}
\end{equation}
\]</span></p>
<p>Notice what happens in the second step of equation 18. I combine the Beta function term and the constant evidence term. The Beta function in this equation is also a constant value. Therefore when we combine the likelihood and prior we alter the Beta function’s parameters and drop the constant for the evidence term. This leaves us with a Beta distribution for our posterior that includse our observed experiments and our prior’s hyperparameters.</p>
<p>????????????? Possibly rephrase the above to specify that we are preserving the scaling of the posterior… ??????????????</p>
<p>To estimate <span class="math inline">\(\theta\)</span>, we calculate the expected value of a Beta distribution as shown in Equation 19.</p>
<p><span class="math display" id="eq:bernBIExp">\[
\begin{equation}
E[\theta]={\alpha \over \alpha + \beta}
\tag{2.22}
\end{equation}
\]</span></p>
<p>Plugging in our parameters as per the derivation in Equation 18:</p>
<p><span class="math display" id="eq:bernBIExpFinal">\[
\begin{equation}
\begin{aligned}
E[\theta] &amp;= {z+a \over {z+a + n-z+b}}\\\\
&amp;={z+a \over {a+N+b}}\\\\
\end{aligned}
\tag{2.23}
\end{equation}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">20</span>
heads &lt;-<span class="st"> </span><span class="dv">14</span>
tails &lt;-<span class="st"> </span>n<span class="op">-</span>heads


<span class="co"># ml</span>
ml_theta &lt;-<span class="st"> </span>heads<span class="op">/</span>n
<span class="co"># map</span>
B &lt;-<span class="st"> </span><span class="dv">2</span>
alpha &lt;-<span class="st"> </span><span class="dv">2</span>
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha)<span class="op">/</span>(alpha <span class="op">+</span><span class="st"> </span>n <span class="op">+</span><span class="st"> </span>B) 
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.08</span>, <span class="dt">y=</span><span class="fl">1.4</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[BI]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:BISmallUninformedPrior"></span>
<img src="_main_files/figure-html/BISmallUninformedPrior-1.png" alt="Bayesian Inference: Analytical Solution (n=20)" width="672" />
<p class="caption">
Figure 2.9: Bayesian Inference: Analytical Solution (n=20)
</p>
</div>
<p>Keep in mind Bayesian inference is effected by the selection of a prior and the number of observations in the same way MAP estimation is.</p>
<div id="the-issue-of-intractability" class="section level3">
<h3><span class="header-section-number">2.5.1</span> The Issue of Intractability</h3>
<p>The coin flip example solved via Bayesian inference was capable of being solved analytically. However in many cases of Bayesian inference this is not possible due to the intractability of solving for the marginal likelihood (evidence term). We do have other options for solutions such as Gibbs sampling, expectation-maximization, and Metropolis-Hastings methods. This book will only focus on Gibbs Sampling, but be aware other types of solvers are used for Bayesian inference, and in particular LDA.</p>
</div>
<div id="a-tale-of-two-mcs" class="section level3">
<h3><span class="header-section-number">2.5.2</span> A Tale of Two MC’s</h3>
<p>Gibbs Sampling is a Markov Chain Monte Carlo (MCMC) technique for parameter estimation. Let’s break down the two MC’s. A markov chain is a process where the next state is determined by the current state. More importantly it does not rely on any information prior to the current state. ????? 1. This needs a diagram and possibly a general example. 2. LETS MAKE SURE TO REITERATE THIS AFTER SHOWING THE GENERAL GIBBS SAMPLING MATH AND DIAGRAM ???? The other MC, Monte Carlo, is a technique used to solve a variety of problems by repeated random sampling. A common example used to showcase Monte Carlo methods is the estimation of the value of <span class="math inline">\(\pi\)</span>. All we need is some chalk and a bucket of rice. I draw a circle on the ground. I then draw a square along the circumference of the square. Now for the bucket of rice, aka our random number generator. I stand over the square and uniformly pour rice over the area of the square. Now comes the monotonous part, counting the rice. First I count the number of pieces of rice inside the circle. Next I tally the number of pieces of rice remaining. From here I can infer the value of <span class="math inline">\(\pi\)</span> as follows:</p>
<p><span class="math display" id="eq:MCRice">\[
\begin{equation}
\begin{aligned}
{Rice_{circle}\over Rice_{circle + square}} &amp;= {\pi r^{2} \over (2r)^{2}}\\\\
\pi &amp;\approx {4Rice_{circle}\over Rice_{circle + square}}
\end{aligned}
\tag{2.24}
\end{equation}
\]</span></p>
</div>
<div id="conjugate-priors" class="section level3">
<h3><span class="header-section-number">2.5.3</span> Conjugate Priors</h3>
<p>Before wading into the deeper water that is Gibbs sampling I need to touch on the concept of conjugate priors. The general premise of conjugate priors is that the prior has the same form as the posterior distribution. Ok, but what does that mean for us? In a pragmatic sense, it means we need to look for a <strong>pattern</strong> when solving for our posterior.</p>
<p><strong>PATTERN MATCHING</strong></p>
<p>????????? Double check the ‘observed data’ statement here ???????????????</p>
<p>Let’s take the coin flip example. We want to estimate the posterior, the probability of <span class="math inline">\(\theta\)</span> given the the results of all experiments (this includes experiments that we haven’t yet completed). Obviously we don’t have data which does not yet exist (i.e. all experiments) so we will need to use the likelihood and the prior to estimate the posterior.</p>
<div id="bernoulli-beta" class="section level4">
<h4><span class="header-section-number">2.5.3.1</span> Bernoulli &amp; Beta</h4>
<p>In previous example we have used the Beta distribution to calculate the prior. What I did not point out is that the Beta distribution is the conjugate prior of the Bernoulli distribution.</p>
<p>We start with our base relationship between posterior, likelihood, prior, and posterior.</p>
<p><span class="math display" id="eq:posteriorGibbs">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior} = {\overbrace{p(D|\theta)}^{likelihood}
  \overbrace{p(\theta)}^{prior} \over \underbrace{p(D)}_{evidence}}
\tag{2.25}
\end{equation}
\]</span></p>
<p>When estimating the posterior we drop out the evidence term. To reiterate, we can drop out the evidence term because it is constant and is used as a normalizing factor for scaling our probabilities to so that they sum to 1. We will see in the upcoming section that Gibbs sampling accounts for the scaling issue and allows us to infer probabilities at the correct scale.</p>
<p><span class="math display" id="eq:posteriorPrompto">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior}  \propto  {\overbrace{p(D|\theta)}^{likelihood}
  \overbrace{p(\theta)}^{prior}}
\tag{2.26}
\end{equation}
\]</span></p>
<p>To estimate the posterior of the bernoulli distribution we plug in our likelihood and prior (<em>Beta distribution</em>) equations.</p>
<p><span class="math display" id="eq:bernGibbs">\[
\begin{equation}
p(\theta|z,N) \propto \overbrace{\theta^z(1-\theta)^{(N-z)}}^{likelihood} \overbrace{{\theta^{(a-1)}(1-\theta)^{(b-1)}}\over \beta(a,b)}^{prior}
\tag{2.27}
\end{equation}
\]</span></p>
<p>After combining terms we end up with Equation <a href="parameter-estimation.html#eq:bernGibbsFinal">(2.28)</a> which looks very similar to @(bernBIFinal). This is a very simple example where an analytical solution for the posterior is possible, so the equations are effectively the same with the exeption of the <span class="math inline">\(\propto\)</span> in place of the <span class="math inline">\(=\)</span>. The <span class="math inline">\(\propto\)</span> is used because we are going to base our posterior off of random samples from the Beta distribution with the parameters shown in <a href="parameter-estimation.html#eq:bernGibbsFinal">(2.28)</a>, unlike the BI example where the solution was found analytically.</p>
<p><span class="math display" id="eq:bernGibbsFinal">\[
\begin{equation}
\begin{aligned}
p(\theta|z,N) &amp;\propto {\overbrace{\theta^{(a + z -1)}(1-\theta)^{(N-z+b-1)}}^{Same \hspace{1 mm} Pattern \hspace{1 mm} as \hspace{1 mm} Prior}\over{\beta(a,b)}}\\
p(\theta|z,N) &amp;\propto Beta(a+z, N-z+b)
\end{aligned}
\tag{2.28}
\end{equation}
\]</span></p>
</div>
</div>
<div id="gibbs-sampling" class="section level3">
<h3><span class="header-section-number">2.5.4</span> Gibbs Sampling</h3>
<p>Gibbs sampling is a Markov Chain Monte Carlo technique that can be used for estimating parameters by walking through a given parameter space. A generalized way to think of Gibbs sampling is estimating a given parameter based on what we currently know about all other parameters. You may be thinking ‘What other parameters?’. Gibbs sampling is only really applicable when you are trying to estimate multiple parameters.</p>
<p>The coin flip example will now be expanded so that we have more than one parameter to estimate. We are going to compare the bias of two coins to see if the there is any discernable difference between the two coins.</p>
<p>Mathematical representation of what happens in Gibbs Sampling:</p>
<p><span class="math display" id="eq:GibbsGeneral">\[
\begin{equation}
\begin{aligned}
&amp;p(\theta_{1}^{i+1}) \sim p(\theta_{1}^{i}|\theta_{2}^{i}, \theta_{3}^{i},..., \theta{n}^{i}) \\
&amp;p(\theta_{2}^{i+1}) \sim p(\theta_{2}^{i}|\theta_{1}^{i+1}, \theta_{3}^{i},..., \theta{n}^{i}) \\
&amp;p(\theta_{3}^{i+1}) \sim p(\theta_{3}^{i}|\theta_{1}^{i+1}, \theta_{2}^{i+1},..., \theta{n}^{i}) \\
&amp;................................ \\ 
&amp;p(\theta_{n}^{i+1}) \sim p(\theta_{n}^{i}|\theta_{1}^{i+1}, \theta_{2}^{i+1},..., \theta_{n-1}^{i+1}) \\
\end{aligned}
\tag{2.29}
\end{equation}
\]</span></p>
<p>In Equation <a href="parameter-estimation.html#eq:GibbsGeneral">(2.29)</a> we see the next estimate, <em>i+1</em>, of <span class="math inline">\(\theta_{1}\)</span> is based on all other current parameter values. When estimating <span class="math inline">\(\theta_{2}\)</span> the <em>i+1</em> value of <span class="math inline">\(\theta_{1}\)</span> is used along with all of the current (<em>i</em>) parameter values. The is continues for all parameter values. After the the <em>nth</em> parameter is estimated, the process starts all over again until a specific stopping criteria is met.</p>
<p>Without the math it looks like this:</p>
<p>FIGURE OF 3 CIRCLES CROSSED OUT/OPEN/COLORED - BEING ESTIMATED, CURRENT STEP, PAST STEP</p>
</div>
<div id="bias-of-two-coins" class="section level3">
<h3><span class="header-section-number">2.5.5</span> Bias of Two Coins</h3>
<p>Below is an example to compare two coins. We use a fairly weak prior by setting <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to 2 for both coins. Coin 1 has 11 heads out of 14 flips while coin 2 has 7 heads out of 14 flips.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a =<span class="st"> </span><span class="dv">2</span>
b =<span class="st"> </span><span class="dv">2</span>

z1 =<span class="st"> </span><span class="dv">11</span>
N1 =<span class="st"> </span><span class="dv">14</span>
z2 =<span class="st"> </span><span class="dv">7</span>
N2 =<span class="st"> </span><span class="dv">14</span>


theta =<span class="st"> </span><span class="kw">rep</span>(<span class="fl">0.5</span>,<span class="dv">2</span>)
niters =<span class="st"> </span><span class="dv">10000</span>
burnin =<span class="st"> </span><span class="dv">500</span>

thetas =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> (niters<span class="op">-</span>burnin), <span class="dt">ncol=</span><span class="dv">2</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>niters){

  theta1 =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>z1, <span class="dt">shape2 =</span> b <span class="op">+</span><span class="st"> </span>N1 <span class="op">-</span><span class="st"> </span>z1)
  <span class="co"># get value theta2| all other vars</span>
  theta2 =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>z2, <span class="dt">shape2 =</span>b <span class="op">+</span><span class="st"> </span>N2 <span class="op">-</span><span class="st"> </span>z2)
  
  <span class="cf">if</span> (i <span class="op">&gt;=</span><span class="st"> </span>burnin){
    thetas[(i<span class="op">-</span>burnin), ] =<span class="st"> </span><span class="kw">c</span>(theta1, theta2)
  }
}


ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta1 =</span> thetas[,<span class="dv">1</span>], <span class="dt">theta2=</span> thetas[,<span class="dv">2</span>])
<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x=</span>theta1)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y=</span>..density..),<span class="dt">color=</span><span class="st">&#39;#1A384A&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(theta[<span class="dv">1</span>]<span class="op">~</span>Estimate), <span class="dt">x=</span><span class="kw">expression</span>(theta[<span class="dv">1</span>]), <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ds<span class="op">$</span>theta1), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:2coins"></span>
<img src="_main_files/figure-html/2coins-1.png" alt="Bias of Two Coins: Theta 1" width="672" />
<p class="caption">
Figure 2.10: Bias of Two Coins: Theta 1
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x=</span>theta2)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y=</span>..density..),<span class="dt">color=</span><span class="st">&#39;#1A384A&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(theta[<span class="dv">2</span>]<span class="op">~</span>Estimate), <span class="dt">x=</span><span class="kw">expression</span>(theta[<span class="dv">2</span>]), <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ds<span class="op">$</span>theta2), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:2coins2"></span>
<img src="_main_files/figure-html/2coins2-1.png" alt="Bias of Two Coins - Theta 2" width="672" />
<p class="caption">
Figure 2.11: Bias of Two Coins - Theta 2
</p>
</div>
<p>In the Figures above we can see the distribution of samples drawn from the Beta distribution using the prior parameters and the observed data we have. To compare the coins we look at the 95% confidence interval of the mean…????</p>
</div>
<div id="change-point-example" class="section level3">
<h3><span class="header-section-number">2.5.6</span> Change Point Example</h3>
<p>IMAGE PLACE HOLDER - MAKE A DRAWING OF FLIPPING TWO COINS ON A TIME SCALE</p>
<p>What if the problem we are solving is more complicated. Let’s say I flip a coin repeatedly, but at some point I switch to another coin with a different bias (<span class="math inline">\(\theta\)</span>). I want to detect the point in time when coin 1 was swapped out for coin 2. We can use Gibbs sampling to solve this problem however now is a good time to highlight the general structure for using your conjugate priors and how to get to the equations required for the sampling process.</p>
<p>The general process for derivation of our sampling distribution, as outlined in <span class="citation">(Yildirim <a href="references.html#ref-yildirim2012bayesian">2012</a>)</span>, is as follows:</p>
<ol style="list-style-type: decimal">
<li>Derive the full joint density.</li>
<li>Derive the posterior conditionals for each of the random variables in the model.</li>
<li>Simulate samples from the posterior joint distribution based on the posterior conditionals.</li>
</ol>
<p>In the previous example we only needed to get a posterior of a single variable (technically there were two variables, but both have the same posterior). In this case we have 3 variables that we need to estimate:</p>
<ol style="list-style-type: decimal">
<li>Coin bias for coin 1: <span class="math inline">\(\theta_{1}\)</span></li>
<li>Coin bias for coin 2: <span class="math inline">\(\theta_{2}\)</span></li>
<li>The point in time, i.e. on which flip, the coin was swapped from coin 1 to coin 2: <em>n</em></li>
</ol>
<p>We have three variables, but how do we describe the actual process we are modeling? In Equation <a href="parameter-estimation.html#eq:cptDistributions">(2.30)</a> the distrutions for each variable are displayed. The coinflips, <em>x</em>, are drawn from a bernoulli distribution, but are dependent on the coin being flipped and the bias that coin has. The change point, <em>n</em>, NEED TO FINISH THIS EXPLANATION</p>
<p><span class="math display" id="eq:cptDistributions">\[
\begin{equation}
\begin{aligned}
  x &amp;\sim
    \begin{cases}
      Bern(x_{i};\theta_{1}) \quad 1 \le i \le n \\
      Bern(x_{i};\theta_{1}) \quad n \lt i \lt N 
    \end{cases} \\
  n &amp;\sim Uniform(1...N) \\
  \theta_{i} &amp;\sim Beta(\theta_{i}, a,b)
\end{aligned}
\tag{2.30}
\end{equation}
\]</span></p>
<div id="derivation-of-joint-distribution" class="section level4">
<h4><span class="header-section-number">2.5.6.1</span> Derivation of Joint Distribution</h4>
<p><span class="math display" id="eq:cptJointDistribution">\[
\begin{equation}
p(\theta_{1}, \theta_{2}, n| x_{1:n}) \propto \overbrace{p(x_{1:N}|\theta_{1})
p(x_{n+1:N}|\theta_{2})}^{Likelihoods}
\overbrace{p(\theta_{1})p(\theta_{2})p(n)}^{Priors}
\tag{2.31}
\end{equation}
\]</span></p>
<p>Our goal here is to get one equation to estimate the posterior of each of our variables, aka the posterior conditionals. The easiest way to accomplish this task is to identify the terms of the joint distribution that contain the variable you want the posterior conditional for.</p>
<p>Let’s start by breaking <a href="parameter-estimation.html#eq:cptJointDistribution">(2.31)</a> a bit further.</p>
$$
<span class="math display" id="eq:cptJointDistExpand">\[\begin{equation}
\begin{aligned}
p(\theta_{1}, \theta_{2}, n| x_{1:n}) &amp;\propto 
  (\prod_{1}^{n}p(x_{i}|\theta_{1}))
  (\prod_{n+1}^{N}p(x_{i}|\theta_{2}))
  p(\theta_{1})p(\theta_{2})p(n)\\
&amp;\propto [\theta_{1}^{z_{1}}(1-\theta_{1})^{n-z_{1}}]
  [\theta_{2}^{z_{2}}(1-\theta_{2})^{N-(n+1)-z_{2}}]
  p(\theta_{1})p(\theta_{2})p(n)\\
  \\
&amp;\propto [\theta_{1}^{z_{1}}(1-\theta_{1})^{n-z_{1}}]
  [\theta_{2}^{z_{2}}(1-\theta_{2})^{N-(n+1)-z_{2}}]
  {{\theta_{1}^{(a_{1}-1)}(1-\theta_{1})^{(b_{1}-1)}}\over \beta(a_{1},b_{1})}
  {{\theta_{2}^{(a_{2}-1)}(1-\theta_{2})^{(b_{2}-1)}}\over \beta(a_{2},b_{2})}
  {1\over N}\\
  \\

\end{aligned}
\tag{2.32}
\end{equation}\]</span>
<p>$$</p>
<p>Let’s start by solving for <em>n</em>’s posterior conditional. In Equation <a href="parameter-estimation.html#eq:cptJointDistExpand">(2.32)</a> we see that only the likelihood terms and the priors for the <span class="math inline">\(\theta\)</span>’s contain <em>n</em>. Using these terms we can solve for the posterior conditional. While we are at it we will also take the log of the conditional posterior as is good practice to prevent issues such as underflow.</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
p(n| x_{1:n}, \theta_{1}, \theta_{2}) &amp;\propto  [\theta_{1}^{z_{1}}(1-\theta_{1})^{n-z_{1}}]
  [\theta_{2}^{z_{2}}(1-\theta_{2})^{N-(n+1)-z_{2}}]\\
log(p(n| x_{1:n}, \theta_{1}, \theta_{2})) &amp;\propto  
  log([\theta_{1}^{z_{1}}(1-\theta_{1})^{n-z_{1}}]) +
  log([\theta_{2}^{z_{2}}(1-\theta_{2})^{N-(n+1)-z_{2}}])
\end{aligned}
\end{equation}
\]</span></p>
<p>To get the posterior conditionals for the <span class="math inline">\(\theta\)</span> values we will need to utilize the conjugate prior relationship between the likelihoods and the priors. First we will collapse the priors and likelihoods for the <span class="math inline">\(\theta\)</span> values.</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
p(\theta_{1}, \theta_{2}, n| x_{1:n}) &amp;\propto            
  [\theta_{1}^{(z_{1}+a_{1}-1)}(1-\theta_{1})^{(n-z_{1}+b_{1}-1)}]
  [\theta_{2}^{(z_{2}+a_{2}-1)}(1-\theta_{2})^{(N-n-1-z_{2}+b_{2}-1)}]({1 \over N})\\
&amp;\propto Beta(a_{1}+z_{1}, n-z_{1}+b_{1}) Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})({1\over N})
\end{aligned}
\end{equation}
\]</span> Now we can solve for each of the <span class="math inline">\(\theta\)</span>’s posterior conditionals.</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
p(\theta_{1}| x_{1:n},\theta_{2}, n) &amp;\propto Beta(a_{1}+z_{1}, n-z_{1}+b_{1})\\
log(p(\theta_{1}| x_{1:n},\theta_{2}, n)) &amp;\propto log(Beta(a_{1}+z_{1}, n-z_{1}+b_{1}))
\end{aligned}
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
p(\theta_{2}| x_{1:n},\theta_{1}, n) &amp;\propto Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})\\
log(p(\theta_{2}| x_{1:n},\theta_{1}, n)) &amp;\propto log(Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2}))
\end{aligned}
\end{equation}
\]</span></p>
<p>Now let’s put our derived posteriors to work and Gibb’s sample our change point and coin biases.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">real_thetas &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.6</span>)
N &lt;-<span class="st"> </span><span class="dv">300</span>
a =<span class="st"> </span><span class="dv">2</span>
b =<span class="st"> </span><span class="dv">3</span>
change_point &lt;-<span class="st"> </span><span class="dv">100</span>
x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rbinom</span>(<span class="dv">1</span><span class="op">:</span>change_point, <span class="dv">1</span>, real_thetas[<span class="dv">1</span>]),<span class="kw">rbinom</span>((change_point<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>N, <span class="dv">1</span>, real_thetas[<span class="dv">2</span>]))


## Initialize all parameters

<span class="co"># n ~ uniform </span>
n &lt;-<span class="st"> </span><span class="kw">round</span>(N<span class="op">*</span><span class="kw">runif</span>(<span class="dv">1</span>))
<span class="co"># theta1 ~ beta(a,b)</span>
theta1 &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1</span>, a, b)
<span class="co"># theta2 ~ beta(a,b)</span>
theta2 &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1</span>, a, b)



niters =<span class="st"> </span><span class="dv">10000</span>
burnin =<span class="st"> </span><span class="dv">2000</span>
<span class="co"># sigma = np.diag([0.2,0.2])</span>

<span class="co"># thetas = np.zeros((niters-burnin,2), np.float)</span>
params =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> (niters<span class="op">-</span>burnin), <span class="dt">ncol=</span><span class="dv">3</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>niters){
  
  
  z1 &lt;-<span class="st"> </span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>n])
  <span class="cf">if</span>(n <span class="op">==</span><span class="st"> </span>N){
    z2 &lt;-<span class="st"> </span><span class="dv">0</span>
  }<span class="cf">else</span>{
    z2 &lt;-<span class="st"> </span><span class="kw">sum</span>(x[(n<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>N])
  }
  theta1 =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>z1, <span class="dt">shape2 =</span> b <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span>z1)
  <span class="co"># get value theta2| all other vars</span>
  theta2 =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>z2, <span class="dt">shape2 =</span>N<span class="op">-</span>n<span class="op">-</span><span class="dv">1</span><span class="op">-</span>z2<span class="op">+</span>b)
  
  
  ## 2 things: 1 - should I be summing all the values over these? 
  <span class="co"># No - the product is being calculated due to the sum - should be fine</span>
  n_multi &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)
  <span class="cf">for</span>(steps <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){
    <span class="cf">if</span>(steps<span class="op">==</span>N <span class="op">||</span><span class="st"> </span>theta2 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>){
      n_multi[steps] &lt;-<span class="st"> </span><span class="kw">log</span>(theta1<span class="op">^</span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>steps]) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>theta1)<span class="op">^</span>(steps<span class="op">-</span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>steps])))
    }<span class="cf">else</span>{
      n_multi[steps] &lt;-<span class="st"> </span><span class="kw">log</span>(theta1<span class="op">^</span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>steps]) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>theta1)<span class="op">^</span>(steps<span class="op">-</span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>steps]))) <span class="op">+</span>
<span class="st">        </span><span class="kw">log</span>(theta2<span class="op">^</span><span class="kw">sum</span>(x[(steps <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>N]) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>theta2)<span class="op">^</span>(N<span class="op">-</span>steps<span class="op">-</span><span class="dv">1</span><span class="op">-</span><span class="kw">sum</span>(x[(steps<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>N])))
    }
  }
  
  n_multi &lt;-<span class="st"> </span><span class="kw">exp</span>(n_multi <span class="op">-</span><span class="st"> </span><span class="kw">max</span>(n_multi))
  <span class="co"># what about for n? </span>
  n &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, n_multi<span class="op">/</span><span class="kw">sum</span>(n_multi))[,<span class="dv">1</span>] <span class="op">==</span><span class="dv">1</span>)
  <span class="cf">if</span> (i <span class="op">&gt;=</span><span class="st"> </span>burnin){
    params[(i<span class="op">-</span>burnin), ] =<span class="st"> </span><span class="kw">c</span>(theta1,theta2, n)
  }
}

ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">theta =</span> <span class="kw">c</span>(<span class="kw">rep</span>(real_thetas[<span class="dv">1</span>],N<span class="op">-</span>change_point),
                                  <span class="kw">rep</span>(real_thetas[<span class="dv">2</span>],change_point)), 
                 <span class="dt">sample_index =</span> <span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">params_df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(params)
<span class="kw">names</span>(params_df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;theta1&#39;</span>, <span class="st">&#39;theta2&#39;</span>, <span class="st">&#39;change_point&#39;</span>)

<span class="kw">ggplot</span>(params_df, <span class="kw">aes</span>(<span class="dt">x =</span> change_point)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill=</span><span class="st">&quot;#7A99AC&quot;</span>, <span class="dt">color =</span><span class="st">&#39;#1A384A&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(params_df<span class="op">$</span>change_point), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&#39;Change Point Estimate&#39;</span>, <span class="dt">x=</span><span class="st">&#39;Change Point&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) </code></pre></div>
<div class="figure"><span id="fig:bernChangePointN"></span>
<img src="_main_files/figure-html/bernChangePointN-1.png" alt="Estimated Change Point" width="672" />
<p class="caption">
Figure 2.12: Estimated Change Point
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(params_df, <span class="kw">aes</span>(<span class="dt">x =</span> theta1)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill=</span><span class="st">&quot;#7A99AC&quot;</span>, <span class="dt">color =</span><span class="st">&#39;#1A384A&#39;</span> ) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(params_df<span class="op">$</span>theta1), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(theta[<span class="dv">1</span>]<span class="op">~</span>Estimate), <span class="dt">x=</span><span class="kw">expression</span>(theta[<span class="dv">1</span>]), <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) </code></pre></div>
<div class="figure"><span id="fig:bernChangePointTheta1"></span>
<img src="_main_files/figure-html/bernChangePointTheta1-1.png" alt="Estimated Theta 1" width="672" />
<p class="caption">
Figure 2.13: Estimated Theta 1
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(params_df, <span class="kw">aes</span>(<span class="dt">x =</span> theta2)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill=</span><span class="st">&quot;#7A99AC&quot;</span>, <span class="dt">color =</span><span class="st">&#39;#1A384A&#39;</span> ) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(params_df<span class="op">$</span>theta2), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(theta[<span class="dv">2</span>]<span class="op">~</span>Estimate), <span class="dt">x=</span><span class="kw">expression</span>(theta[<span class="dv">2</span>]), <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) </code></pre></div>
<div class="figure"><span id="fig:bernChangePointTheta2"></span>
<img src="_main_files/figure-html/bernChangePointTheta2-1.png" alt="Estimated Theta 2" width="672" />
<p class="caption">
Figure 2.14: Estimated Theta 2
</p>
</div>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="what-is-lda.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multinomial-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
