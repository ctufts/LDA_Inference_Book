<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>LDA Tutorial</title>
  <meta name="description" content="LDA Tutorial">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="LDA Tutorial" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="LDA Tutorial" />
  <meta name="twitter:site" content="@devlintufts" />
  
  

<meta name="author" content="Chris Tufts">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="parameter-estimation.html">
<link rel="next" href="word-embeddings-and-representations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="" data-path="layout-of-book.html"><a href="layout-of-book.html"><i class="fa fa-check"></i>Layout of Book</a></li>
<li class="chapter" data-level="1" data-path="what-is-lda.html"><a href="what-is-lda.html"><i class="fa fa-check"></i><b>1</b> What is LDA?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#animal-generator"><i class="fa fa-check"></i><b>1.1</b> Animal Generator</a><ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#generating-the-mixtures"><i class="fa fa-check"></i><b>1.1.1</b> Generating the Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-lda.html"><a href="what-is-lda.html#inference"><i class="fa fa-check"></i><b>1.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#distributions"><i class="fa fa-check"></i><b>2.1</b> Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bernoulli"><i class="fa fa-check"></i><b>2.1.1</b> Bernoulli</a></li>
<li class="chapter" data-level="2.1.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#beta-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#inference-the-building-blocks"><i class="fa fa-check"></i><b>2.2</b> Inference: The Building Blocks</a></li>
<li class="chapter" data-level="2.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-likelihood"><i class="fa fa-check"></i><b>2.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>2.4</b> Maximum a Posteriori (MAP)</a></li>
<li class="chapter" data-level="2.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bayesian-inference"><i class="fa fa-check"></i><b>2.5</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.5.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#the-issue-of-intractability"><i class="fa fa-check"></i><b>2.5.1</b> The Issue of Intractability</a></li>
<li class="chapter" data-level="2.5.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#a-tale-of-two-mcs"><i class="fa fa-check"></i><b>2.5.2</b> A Tale of Two MC’s</a></li>
<li class="chapter" data-level="2.5.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#conjugate-priors"><i class="fa fa-check"></i><b>2.5.3</b> Conjugate Priors</a></li>
<li class="chapter" data-level="2.5.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.5.4</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.5.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bias-of-two-coins"><i class="fa fa-check"></i><b>2.5.5</b> Bias of Two Coins</a></li>
<li class="chapter" data-level="2.5.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html#change-point-example"><i class="fa fa-check"></i><b>2.5.6</b> Change Point Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html"><i class="fa fa-check"></i><b>3</b> Multinomial Distribution</a><ul>
<li class="chapter" data-level="3.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#comparison-of-dice-vs.words"><i class="fa fa-check"></i><b>3.1</b> Comparison of Dice vs. Words</a></li>
<li class="chapter" data-level="3.2" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#relationship-to-bernoulli"><i class="fa fa-check"></i><b>3.2</b> Relationship to Bernoulli</a></li>
<li class="chapter" data-level="3.3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#conjugate-prior-dirichlet"><i class="fa fa-check"></i><b>3.3</b> Conjugate Prior: Dirichlet</a></li>
<li class="chapter" data-level="3.4" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#gibbs-sampling---multinomial-dirichlet"><i class="fa fa-check"></i><b>3.4</b> Gibbs Sampling - Multinomial &amp; Dirichlet</a><ul>
<li class="chapter" data-level="3.4.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#derivation-of-gibbs-sampling-solution-of-word-distribution-single-doc"><i class="fa fa-check"></i><b>3.4.1</b> Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="word-embeddings-and-representations.html"><a href="word-embeddings-and-representations.html"><i class="fa fa-check"></i><b>4</b> Word Embeddings and Representations</a><ul>
<li class="chapter" data-level="4.1" data-path="word-embeddings-and-representations.html"><a href="word-embeddings-and-representations.html#bag-of-words"><i class="fa fa-check"></i><b>4.1</b> Bag of Words</a></li>
<li class="chapter" data-level="4.2" data-path="word-embeddings-and-representations.html"><a href="word-embeddings-and-representations.html#word-embeddings"><i class="fa fa-check"></i><b>4.2</b> Word Embeddings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html"><i class="fa fa-check"></i><b>5</b> LDA as a Generative Model</a><ul>
<li class="chapter" data-level="5.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#general-terminology"><i class="fa fa-check"></i><b>5.1</b> General Terminology</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#selecting-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Selecting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generative-model"><i class="fa fa-check"></i><b>5.2</b> Generative Model</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generating-documents"><i class="fa fa-check"></i><b>5.2.1</b> Generating Documents</a></li>
<li class="chapter" data-level="5.2.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#lda-generative-model"><i class="fa fa-check"></i><b>5.2.2</b> LDA Generative Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lda-inference.html"><a href="lda-inference.html"><i class="fa fa-check"></i><b>6</b> LDA Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="lda-inference.html"><a href="lda-inference.html#general-overview"><i class="fa fa-check"></i><b>6.1</b> General Overview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">LDA Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multinomial-distribution" class="section level1">
<h1><span class="header-section-number">3</span> Multinomial Distribution</h1>
<div id="comparison-of-dice-vs.words" class="section level2">
<h2><span class="header-section-number">3.1</span> Comparison of Dice vs. Words</h2>
<p>The binomial distribution is a special case of the multinomial distribution where the number of possible outcomes is 2. A multinomial distribution can have 2 or more outcomes and therefore is normally shown through examples using a 6-sided die. Instead of using a die with numbers on each side, let’s label the sides with the following words:</p>
<p>“Latent” “Dirichlet” “Allocation” “has” “many” “peices”</p>
<p>—————image placeholder———————– Maybe draw a die with those words on it and consider it a ‘document’ ——————-placeholder end——————–</p>
<p>In the example above we would assume it is a fair die which would result in equal probabilities of ‘rolling’ any of the 6 unique words. Therefore each word has 1/6 chances of being randomly sampled from the die.</p>
<p>Below is an empirical example where we take each word and assign it to a single side of a fair die. The experiment, a single roll of the die, is repeated 10,000 times. We can see each word comes up at roughly the same frequency.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw 1000 samples from multinomial distribution</span>
outcomes &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10000</span>, <span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">1</span>/<span class="dv">6</span>,<span class="dv">6</span>))==<span class="dv">1</span>))
words &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">strsplit</span>(<span class="st">&quot;Latent Dirichlet Allocation has many peices&quot;</span>, <span class="st">&#39; &#39;</span>))
ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> outcomes, <span class="dt">word =</span> <span class="kw">sapply</span>(outcomes, function(x)words[x]))
<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x=</span> word)) +<span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">stat=</span><span class="st">&#39;count&#39;</span>,<span class="dt">color=</span><span class="st">&#39;#1A384A&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#7A99AC&#39;</span>) +
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">labels =</span> words) +
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:multinomialDist"></span>
<img src="_main_files/figure-html/multinomialDist-1.png" alt="Sampling from Multinomial with Equal Parameters" width="672" />
<p class="caption">
Figure 3.1: Sampling from Multinomial with Equal Parameters
</p>
</div>
<pre><code>Side Note:
Let&#39;s think of how this will be used in the case of LDA. If I wanted to generate a document based on a model, I could use a multinomial distribution to determine what words would be in the document. If I knew the probability of a word I could use the example above to draw a new word with each sample. Obviously some words occur much more often than others, so the &#39;fair die&#39; example wouldn&#39;t work for generation of document.  In later sections we will build on this concept, but its a good idea to start thinking about how this extends to language.</code></pre>
<p>As stated before, the binomial distribution is a special case of the multinomial distribution. The probability mass function for the multinomial distribution is shown in <a href="multinomial-distribution.html#eq:multiPMF">(3.1)</a>:</p>
<p><span class="math display" id="eq:multiPMF">\[
\begin{equation}
f(x)=\dfrac{n!}{x_1!x_2!\cdots x_k!}\theta_1^{x_1} \theta_2^{x_2} \cdots \theta_k^{x_k}
\tag{3.1}
\end{equation}
\]</span></p>
<ul>
<li><i>k</i> - number of sides on the die</li>
<li><i>n</i> - number of times the die will be rolled</li>
</ul>
<p>Therefore the multinomial representation of the distributions we’ve already discussed, binomial and bernoulli, would use the following parameters:</p>
<ul>
<li>k sided die rolled n times
<ul>
<li>n = 1, k = 2 is bernoulli distribution</li>
<li>n &gt; 1 , k = 2 is binomial distribution</li>
</ul></li>
</ul>
</div>
<div id="relationship-to-bernoulli" class="section level2">
<h2><span class="header-section-number">3.2</span> Relationship to Bernoulli</h2>
<p>Time to connect the dots between Bernoulli and Multinomial distributions . Recall the bernoulli probability mass function shown in Equation <a href="multinomial-distribution.html#eq:bernPMFmulti">(3.2)</a>.</p>
<p><span class="math display" id="eq:bernPMFmulti">\[
\begin{equation}
f_{x}(x)=P(X=x)=\theta^{x}(1-\theta)^{1-x}, \hspace{1cm} x = \{0,1\}
\tag{3.2}
\end{equation}
\]</span> Let’s swap in some different terms. We can replace the first term, <span class="math inline">\(\theta\)</span>, with <span class="math inline">\(\theta_{1}\)</span> and the second term, <span class="math inline">\((1-\theta)\)</span> as <span class="math inline">\(\theta_{2}\)</span>. Then we have:</p>
<p><span class="math display" id="eq:bern2Multi">\[
\begin{equation}
{\theta_{1}}^{x_{1}}{\theta_{2}}^{x_{2}}
\tag{3.3}
\end{equation}
\]</span></p>
<p>You can see this looks a bit more like Equation <a href="multinomial-distribution.html#eq:bernPMFmulti">(3.2)</a> if the case were <em>k</em>=2 and <em>n</em>=1. Since <em>n</em>=1, <em>x</em>’s can only have a value of 1 or zero. Therefore the factorial is always 1 for the case of a bernoulli trial. Factorial of <em>n</em> is also 1 since it is a single trial, i.e. <em>n</em>=1, resulting in what we see in equation <a href="multinomial-distribution.html#eq:multiPMF">(3.1)</a>.</p>
</div>
<div id="conjugate-prior-dirichlet" class="section level2">
<h2><span class="header-section-number">3.3</span> Conjugate Prior: Dirichlet</h2>
<p>The conjugate prior for the multinomial distribution is the Dirichlet distribution. Similar to the beta distribution, Dirichlet can be though of as a distribution of distributions. Also note that the beta distribution is the special case of a Dirichlet distribution where the number of possible outcome is 2 similar to the connection between the binomial and multinomial distributions.</p>
<p>The probability distribution function for the Dirichlet distribution is shown in Equation <a href="multinomial-distribution.html#eq:dirPDF">(3.4)</a>.</p>
<p><span class="math display" id="eq:dirPDF">\[
\begin{equation}
Dir(\overrightarrow{p}|\overrightarrow{\alpha})=
{ 
  {\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}
  \over{\prod _{i=1}^{K}\Gamma (\alpha _{i})}
}
\prod _{i=1}^{K}x_{i}^{\alpha _{i}-1}
\tag{3.4}
\end{equation}
\]</span></p>
<p>THINK THIS CAN BE REMOVED - IS IN BETA A few notes about equation <a href="multinomial-distribution.html#eq:dirPDF">(3.4)</a>.</p>
<ol style="list-style-type: decimal">
<li>The Gamma function is the factorial of the parameter minus 1.</li>
</ol>
<p><span class="math display" id="eq:gammaFunction2">\[
\begin{equation}
\Gamma (\alpha _{i}) = (\alpha_{i}-1)!
\tag{3.5}
\end{equation}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The Dirichlet distribution function is often written using the Beta function in place of the first term as seen below:</li>
</ol>
<p><span class="math display">\[
Dir(\overrightarrow{p}|\overrightarrow{\alpha})=
{ 
  1 \over B(\alpha)
}
\prod _{i=1}^{K}x_{i}^{\alpha _{i}-1} 
\]</span></p>
<p>Where: <span class="math display">\[
{1\over B(\alpha)}={ 
  {\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}
  \over{\prod _{i=1}^{K}\Gamma (\alpha _{i})}
}
\]</span></p>
<p>The Dirichlet distribution is an extension of the beta distribution for <i>k</i> categories. To get a better sense of what the distributions look like let’s visualize a few examples at <i>k</i>=3, think 3 sided die, with varying alpha values. In both of the box plots below 10,000 random samples were drawn from a Dirichlet distribution where <i>k</i>=3 and <span class="math inline">\(\alpha\)</span> is the same for each <i>k</i> in the given plot. The first plot shows the distribution of values drawn when <span class="math inline">\(\alpha\)</span> = 100.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">100</span>,<span class="dv">100</span>)
trials &lt;-<span class="st"> </span><span class="dv">10000</span>
x &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(trials, alpha)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;theta_1&#39;</span>, <span class="st">&#39;theta_2&#39;</span>, <span class="st">&#39;theta_3&#39;</span>)
ds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.tibble</span>(x), <span class="dt">trial =</span> <span class="dv">1</span>:trials) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(theta, word, -trial)

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">color =</span> theta, <span class="dt">fill =</span> theta, <span class="dt">x =</span> theta, <span class="dt">y =</span> word)) +<span class="st"> </span><span class="kw">geom_boxplot</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">x =</span> <span class="st">&#39;&#39;</span>, <span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;\U03B1 = &quot;</span>,<span class="kw">unique</span>(alpha)) ) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">1</span>]),
                              <span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">2</span>]),
                              <span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">3</span>]))) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>)+<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<div class="figure"><span id="fig:DirichletAlpha100"></span>
<img src="_main_files/figure-html/DirichletAlpha100-1.png" alt="Sampling from Dirichlet: α=100" width="672" />
<p class="caption">
Figure 3.2: Sampling from Dirichlet: α=100
</p>
</div>
<p>Below the process is repeated, but this time the <span class="math inline">\(\alpha\)</span> values are set to 1 for each category. We can see the range of distribution of values sampled with the higher <span class="math inline">\(\alpha\)</span> value is much narrower than the distribution of values sampled using <span class="math inline">\(\alpha\)</span> values of 1. This is the same pattern we saw with the beta distribution, as the shape parameters increased the distribution became more dense and the shape of the distribution narrowed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
  x &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(trials, alpha)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;theta_1&#39;</span>, <span class="st">&#39;theta_2&#39;</span>, <span class="st">&#39;theta_3&#39;</span>)
ds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.tibble</span>(x), <span class="dt">trial =</span> <span class="dv">1</span>:trials) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(theta, word, -trial)

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">color =</span> theta, <span class="dt">fill =</span> theta, <span class="dt">x =</span> theta, <span class="dt">y =</span> word)) +<span class="st"> </span><span class="kw">geom_boxplot</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">x =</span> <span class="st">&#39;&#39;</span>, <span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;\U03B1 = &quot;</span>,<span class="kw">unique</span>(alpha)) ) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">1</span>]),
                              <span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">2</span>]),
                              <span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">3</span>]))) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<div class="figure"><span id="fig:DirichletAlpha1"></span>
<img src="_main_files/figure-html/DirichletAlpha1-1.png" alt="Sampling from Dirichlet - θ=1" width="672" />
<p class="caption">
Figure 3.3: Sampling from Dirichlet - θ=1
</p>
</div>
<p>So what happens when the <span class="math inline">\(\alpha\)</span> values are not the same, i.e. the distribution is non-symmetrical? In the histogram below, you can see the distribution of values sampled from the dirichlet distribution for each category. Recall the beta distribution shape skews as the difference between the two parameters grows.</p>
<p>You recall we only need to estimate one value, <span class="math inline">\(\theta_{1}\)</span> generated from the beta distribution with 2 parameters because we can infer <span class="math inline">\(\theta_{2}\)</span> from this value (<span class="math inline">\(\theta_{2} = 1-\theta_{1}\)</span>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">50</span>,<span class="dv">20</span>)
alpha_prop &lt;-<span class="st"> </span>alpha/<span class="kw">sum</span>(alpha)
x &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(trials, alpha)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;theta_1&#39;</span>, <span class="st">&#39;theta_2&#39;</span>, <span class="st">&#39;theta_3&#39;</span>)
ds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.tibble</span>(x), <span class="dt">trial =</span> <span class="dv">1</span>:trials) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(theta, word, -trial)

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">color =</span> theta, <span class="dt">fill=</span>theta, <span class="dt">x =</span> word)) +<span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">position=</span><span class="st">&#39;identity&#39;</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>) +
<span class="st">  </span><span class="co"># geom_line(stat=&#39;density&#39;) + </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;\U03B8&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Count&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">label =</span> alpha,
                       <span class="dt">name =</span> <span class="st">&quot;\U03B1&quot;</span> ) +
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">label =</span> alpha,
                       <span class="dt">name =</span> <span class="st">&quot;\U03B1&quot;</span> )</code></pre></div>
<div class="figure"><span id="fig:DirichletAlphaMixed"></span>
<img src="_main_files/figure-html/DirichletAlphaMixed-1.png" alt="Sampling from Dirichlet - θ=[10,50,20]" width="672" />
<p class="caption">
Figure 3.4: Sampling from Dirichlet - θ=[10,50,20]
</p>
</div>
<p>General notes on Dirichlet: higher values of beta (or whatever parameter name is), the more uniform the probability for each class, the lower the more more likely a specific class is going to be much larger than the rest.</p>
<p>Let’s try to rationalize this in the same way we do bernoulli. Bernoulli is 2 possible outcomes, beta is it’s prior. Bernoulli is a special case of multinomial where k = 2, so I would assume that means beta is the special case of dirichlet where the number of shape parameters is 2. Think about this a bit more - in the case of beta distribution we use prior data (or assumption) of coin flips - 5 heads, 5 tails - means a = 5, b = 5. From this we make our distribution and we can randomly sample a valid value of theta (and remember we only use theta for bernoulli because if we have one probability, we can infer the other via subtraction from 1, but in reality there is a p value for heads, and a p value for tails… this makes it a bit eaiser to understand the transfer to multinomial) based on this prior information. So it would be no different if we have 3 different words in a topic, we use the prior info as the parameters for the Dirichlet distribution - 3 red, 2 blue, 1 green which would translate to alpha1 = 3, alpha2 = 2, alpha3 = 1, from there we can build our multidimensional distribution and select the most likely value for our thetas (or the probability of a specific outcome where all those p’s sum to one).</p>
<p>[May want to put a plot or two here showing the dirichlet code, but creating the same beta distributions, so this makes more sense]</p>
</div>
<div id="gibbs-sampling---multinomial-dirichlet" class="section level2">
<h2><span class="header-section-number">3.4</span> Gibbs Sampling - Multinomial &amp; Dirichlet</h2>
<p>Prior to getting into an example of Gibbs sampling as it applies to inferring the parameters of a multinomial distribution, let’s first describe a model which generates words for a single document. As you can imagine this would be modeled as a multinomial distribution with parameters <span class="math inline">\(\overrightarrow{\theta} = \theta_{1}, \theta_{2}, ... \theta_{n}\)</span> for words 1 to n. The model would be capable of generating a bag of words representation of a document. The term <i>‘bag of words’</i> refers to words in no particular order, i.e. the document we would be generating would not have structured sentences, but would contain all the components of the document.</p>
<p>Let’s start by defining our model. Let’s start with a basic composition for our ideal document. We are going to have a document with only 3 distinct words 📘,📕,📗. Remember a document is just a mixture of words, obviously this is not a fine work of literature and has a very limited vocabulary, but it is meant as a basic example of document composition.</p>
<p>First we are going to create a seed document, i.e. the document that will be used as a basis of our <span class="math inline">\(\alpha\)</span>’s for our prior. In order to do this we need to identify the mixture proportions for each word in the vocabulary.</p>
<ul>
<li>📘 : 10%</li>
<li>📕 : 10%</li>
<li>📗 : 80%</li>
</ul>
<p>To clarify this means the document will contain 80% blue books, 10% green books, and 10% red books:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># use letters function as your vocabulary</span>
v &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>, <span class="st">&#39;blue&#39;</span>)
nwords &lt;-<span class="st"> </span><span class="dv">10</span>
doc_theta &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">8</span>)
document&lt;-<span class="kw">rep</span>(v, doc_theta*nwords)
books &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">label =</span> <span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>), 
                    <span class="dt">code =</span> <span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>))

<span class="kw">cat</span>(<span class="kw">sapply</span>(document, function(x) books$code[<span class="kw">which</span>(books$label ==<span class="st"> </span>x)]))</code></pre></div>
<pre><code>## 📕 📗 📘 📘 📘 📘 📘 📘 📘 📘</code></pre>
<p>So what is the structure of the document generator?</p>
<ul>
<li>Alpha -&gt; Dirichlet -&gt; Multinomial</li>
</ul>
<p>(Maybe here is when you can introduce the terrible idea of block diagrams????, It might be good to introduce them even though you aren’t a fan, for all you know people might even understant them better this way…)</p>
<p>Do you recall the beta/bernoulli example? The way we informed our prior was using some prior information we had, i.e. the number of heads and tails previously obtained from flipping the two coins. We will use the document above as the basis of our <span class="math inline">\(\alpha\)</span> paramters for the Dirichlet distribution, i.e. our prior for the multinomial. In more general language, we want to generate documents similar to our ‘ideal’ document.</p>
<p>So let’s generate a new document using the word counts from our <i>ideal</i> document as our <span class="math inline">\(\alpha\)</span> values for the dirichlet prior. Then we use the <span class="math inline">\(\theta\)</span> values generated by the dirichlet prior as the parameters for a multinomial distribution to generate the next term in the document.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate text based only on document #1</span>

<span class="co"># leave this code, I like the for loop for creating a single document, shows the process </span>
<span class="co"># in a straight forward manner, the next code chunk shows a faster way, which is good as well</span>

words &lt;-<span class="st"> </span>document
<span class="co"># lenght of new document</span>
 <span class="co">#rep(1,length(unique(words)))</span>
word_counts &lt;-<span class="st"> </span><span class="kw">table</span>(words)
alphas &lt;-<span class="st">  </span>word_counts
new_doc &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="st">&#39;&#39;</span>, nwords)
for(i in <span class="dv">1</span>:nwords){
  <span class="kw">set.seed</span>(i)
  p =<span class="st"> </span><span class="kw">rdirichlet</span>(<span class="dv">1</span>,alphas)
  <span class="kw">set.seed</span>(i)
  new_doc[i] &lt;-<span class="st"> </span><span class="kw">names</span>(word_counts)[<span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, p) ==<span class="st"> </span><span class="dv">1</span>)]
}
<span class="kw">table</span>(new_doc)</code></pre></div>
<pre><code>## new_doc
## blue  red 
##    9    1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>, <span class="kw">sapply</span>(new_doc, function(x) books$code[<span class="kw">which</span>(books$label ==<span class="st"> </span>x)]))</code></pre></div>
<pre><code>## 
##  📘 📘 📘 📘 📘 📘 📕 📘 📘 📘</code></pre>
<p>It’s not quite the same as the original, but that should be expected. This is a model that generates documents probabalistically based on some prior information. So let’s make a few more and see how this changes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_counts &lt;-<span class="st"> </span><span class="kw">table</span>(words)
alphas &lt;-<span class="st">  </span>word_counts
nwords &lt;-<span class="st"> </span><span class="dv">10</span>
ndocs  &lt;-<span class="st"> </span><span class="dv">5</span>
word_encodings &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">label =</span> <span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>), 
                <span class="dt">code =</span> <span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>), 
                <span class="dt">word_props =</span> <span class="kw">c</span>(.<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">8</span>))

thetas &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(ndocs*nwords, alphas)
<span class="kw">print</span>(<span class="kw">head</span>(thetas))</code></pre></div>
<pre><code>##           [,1]       [,2]        [,3]
## [1,] 0.6259047 0.33535589 0.038739455
## [2,] 0.8641536 0.07229474 0.063551631
## [3,] 0.8292271 0.02713615 0.143636766
## [4,] 0.9399257 0.05494752 0.005126745
## [5,] 0.7641922 0.19525569 0.040552159
## [6,] 0.9015058 0.02198210 0.076512137</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">selected_words &lt;-<span class="st"> </span><span class="kw">apply</span>(thetas, <span class="dv">1</span>, function(x) <span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,x)==<span class="dv">1</span>))

ds &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">doc_id =</span> <span class="kw">rep</span>(<span class="dv">1</span>:ndocs, <span class="dt">each =</span> nwords),
             <span class="dt">word =</span> word_encodings$label[selected_words], 
             <span class="dt">word_uni =</span> word_encodings$code[selected_words])


ds %&gt;%<span class="st"> </span><span class="kw">group_by</span>(doc_id) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(
  <span class="dt">tokens =</span> <span class="kw">paste</span>(word_uni, <span class="dt">collapse =</span> <span class="st">&#39; &#39;</span>)
) %&gt;%<span class="st"> </span><span class="kw">kable</span>(<span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;Document&#39;</span>, <span class="st">&#39;Words&#39;</span>))</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">Document</th>
<th align="left">Words</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">📘 📕 📘 📘 📕 📘 📕 📕 📘 📘</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">📘 📘 📘 📘 📘 📘 📘 📕 📘 📘</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">📘 📘 📘 📘 📗 📘 📘 📘 📕 📘</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">📘 📘 📗 📕 📘 📘 📘 📘 📘 📘</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">📕 📘 📗 📘 📕 📘 📘 📘 📘 📘</td>
</tr>
</tbody>
</table>
<p>As we can see each document composition is similar, but the word counts and order are different each time. This is to be expected (maybe say why? )</p>
<p>So now onto inferernce ….</p>
<p>The process above is known as a generative model. We created documents using a model with a given set of parameters. Inference is going to take this general concept and look at it from a different angle. Instead of generating documents with our model we are going to take a series of pre-existing documents and infer what model created them. We are going to make the assumption that the structure of the model is the same as the generative example, i.e. all documents are generated based on the same word mixture ratios.</p>
<p>Let’s use the 10 documents we previously generated as our basis and see if we can infer the parameters used to generate them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># counts -&gt; alphas</span>
<span class="co"># for each of the 3 thetas iterate 1k times</span>
<span class="co"># plot distributions</span>
alphas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(books))
<span class="co"># alphas &lt;- 1:6 </span>
n &lt;-<span class="st"> </span><span class="kw">table</span>(ds$word)
<span class="kw">head</span>(n)</code></pre></div>
<pre><code>## 
##  blue green   red 
##    38     3     9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">niters =<span class="st"> </span><span class="dv">2000</span>
burnin =<span class="st"> </span><span class="dv">500</span>

thetas =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> (niters-burnin), <span class="dt">ncol=</span><span class="kw">nrow</span>(books), 
                <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="ot">NULL</span>, <span class="kw">c</span>(<span class="kw">names</span>(n))))
for (i in <span class="dv">1</span>:niters){
  theta =<span class="st"> </span><span class="kw">rdirichlet</span>(<span class="dv">1</span>,n+alphas)

  
  if (i &gt;=<span class="st"> </span>burnin){
    thetas[(i-burnin), ] =<span class="st"> </span>theta
  }
}

 <span class="co"># hist(thetas[, 1])</span>
 <span class="co"># hist(thetas[, 2])</span>
 <span class="co"># </span>
 df &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(thetas) %&gt;%<span class="st"> </span>
<span class="st">   </span><span class="kw">gather</span>(word, theta)
<span class="co"># map book colors to each segment of plot to avoid bothering with the emoji labels (for now)</span>
<span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">y=</span>theta, <span class="dt">x =</span> word)) +<span class="st"> </span><span class="kw">geom_violin</span>()</code></pre></div>
<p><img src="_main_files/figure-html/unigramInference10-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># apply(thetas, 2, median)</span>
<span class="co"># n/sum(n)</span></code></pre></div>
<div id="derivation-of-gibbs-sampling-solution-of-word-distribution-single-doc" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc)</h3>
<p>Below is a general overview of how inferrence can be carried out using Gibbs sampling. Recall conjugate priors have the same posterior form as their conjugate distribution. In equation 24 we start with the now familiar proportional solution for estimating a posterior through sampling. We need the likelihood, which is derived from the multinomial distribution, and the prior, which is derived from the dirichlet distribution. Once we plug in the prior and likelihood and simplify, we find that we are left with a Dirichlet PDF with the input parameters of <span class="math inline">\(\overrightarrow{\alpha} + \overrightarrow{n}\)</span> where <em>n</em> are the observed word counts.</p>
<p><span class="math display">\[
\begin{aligned}
p(\theta|D) &amp;\propto p(D|\theta)p(\theta)\\
&amp;\propto \prod _{i=1}^{K}\theta^{n(k)} { 
  {\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}
  \over{\prod _{i=1}^{K}\Gamma (\alpha _{i})}
}
\prod _{i=1}^{K}\theta_{i}^{\alpha _{i}-1} \\
&amp;\propto{ 
  {\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}
  \over{\prod _{i=1}^{K}\Gamma (\alpha _{i})}
}\prod _{i=1}^{K}\theta_{i}^{\alpha _{i}+n_{k}-1} \\
&amp;\propto Dir(\overrightarrow{\alpha} + \overrightarrow{n})
\end{aligned}
\tag{24}
\]</span></p>
<p>We can see our mixture estimates are significantly different from the real model used to generate the documents. So why is this? One of the issues here is that our sample are documents with only 10 words. Therefore an average document has 8 …, 1 …, and 1…, but it is not unusual to see a slight variation which causes mixture shifts of 10% or more. And with Let’s try the same example but this time instead of only generating 5 documents we will genertate 500 and use this as our sample to draw inference from.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_counts &lt;-<span class="st"> </span><span class="kw">table</span>(words)
alphas &lt;-<span class="st">  </span>word_counts
nwords &lt;-<span class="st"> </span><span class="dv">10</span>
ndocs  &lt;-<span class="st"> </span><span class="dv">500</span>
word_encodings &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">label =</span> <span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>), 
                <span class="dt">code =</span> <span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>), 
                <span class="dt">word_props =</span> <span class="kw">c</span>(.<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">8</span>))

thetas &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(ndocs*nwords, alphas)
<span class="kw">print</span>(<span class="kw">head</span>(thetas))</code></pre></div>
<pre><code>##           [,1]       [,2]       [,3]
## [1,] 0.5154606 0.40401130 0.08052810
## [2,] 0.8526076 0.12103847 0.02635393
## [3,] 0.7801023 0.14641427 0.07348343
## [4,] 0.8444930 0.06278609 0.09272094
## [5,] 0.7224804 0.11597181 0.16154776
## [6,] 0.8639552 0.01028122 0.12576353</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">selected_words &lt;-<span class="st"> </span><span class="kw">apply</span>(thetas, <span class="dv">1</span>, function(x) <span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,x)==<span class="dv">1</span>))

ds &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">doc_id =</span> <span class="kw">rep</span>(<span class="dv">1</span>:ndocs, <span class="dt">each =</span> nwords),
             <span class="dt">word =</span> word_encodings$label[selected_words], 
             <span class="dt">word_uni =</span> word_encodings$code[selected_words])


<span class="co"># counts -&gt; alphas</span>
<span class="co"># for each of the 3 thetas iterate 1k times</span>
<span class="co"># plot distributions</span>



alphas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(books))
<span class="co"># alphas &lt;- 1:6 </span>
n &lt;-<span class="st"> </span><span class="kw">table</span>(ds$word)
<span class="kw">head</span>(n)</code></pre></div>
<pre><code>## 
##  blue green   red 
##  4008   507   485</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">niters =<span class="st"> </span><span class="dv">2000</span>
burnin =<span class="st"> </span><span class="dv">500</span>

thetas =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> (niters-burnin), <span class="dt">ncol=</span><span class="kw">nrow</span>(books), 
                <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="ot">NULL</span>, <span class="kw">c</span>(<span class="kw">names</span>(n))))
for (i in <span class="dv">1</span>:niters){
  theta =<span class="st"> </span><span class="kw">rdirichlet</span>(<span class="dv">1</span>,n+alphas)

  
  if (i &gt;=<span class="st"> </span>burnin){
    thetas[(i-burnin), ] =<span class="st"> </span>theta
  }
}

 <span class="co"># hist(thetas[, 1])</span>
 <span class="co"># hist(thetas[, 2])</span>
 <span class="co"># </span>
 df &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(thetas) %&gt;%<span class="st"> </span>
<span class="st">   </span><span class="kw">gather</span>(word, theta)
<span class="co"># map book colors to each segment of plot to avoid bothering with the emoji labels (for now)</span>
<span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">y=</span>theta, <span class="dt">x =</span> word)) +<span class="st"> </span><span class="kw">geom_violin</span>()</code></pre></div>
<p><img src="_main_files/figure-html/unigramInference500-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># apply(thetas, 2, median)</span>
n/<span class="kw">sum</span>(n)</code></pre></div>
<pre><code>## 
##   blue  green    red 
## 0.8016 0.1014 0.0970</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="parameter-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="word-embeddings-and-representations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
