[
["index.html", "LDA Tutorial Preface", " LDA Tutorial Chris Tufts Preface DISCLAIMER: THIS BOOK IS NOT COMPLETE (WORK IN PROGRESS): Please note this is a work in progress and is only being made available for viewing to provide an overview of the project. Please do not share. The purpose of this book is to provide a step by step guide of LDA utilizing Gibbs Sampling. It is heavily inspired by the Gregor Heinrich’s Parameter Estimation for Text Analysis(Heinrich 2008). "],
["background.html", "Background", " Background I’ve often found that resources covering LDA are either hard to follow, do to a heavy reliance on calculus derivations, or the resources are extremely high level so you get the idea of what LDA accomplishes, but you never fully grasp how it works. The book focuses on LDA for inference via Gibbs Sampling. To aid in understanding both LDA and Gibbs sampling all probability distributions used in LDA will be reviewed along with a variety of different approaches for parameter estimation. Following the introduction of these components, LDA will be presented as a generative model. This will lay the groundwork for understanding how LDA can be used for inference of topics in a corpus. I have tried my best to relay an explanation of LDA that fills in the gaps and questions that are sometimes left out of publications. The book contains many code examples, but I do not shy away from walking through mathematical derivations. Where applicable I state mathematical properties used in the derivations so that the reader doesn’t have to ‘take my word for it’, but instead can go from A to B on their own. You will find code examples written in R in the case you would like to try them out at home. I will warn you that my implmentation of LDA is not optimized and if you are doing analysis for any reason other than trying to learn I would suggest using one of the many great peices of open source software available - Mallet, Gensim, LDA(R), tidytext?, scikit? etc. ADD LINKS FOR THESE OR ADD APPENDIX WITH THESE TOOLS AND COMPARISON NOTES (I.E WHAT LANGUAGE, ETC.) "],
["layout-of-book.html", "Layout of Book", " Layout of Book What is LDA? - High level overview Parameter Estimation Methods (general overview using Bernoulli distribution) - ML, MAP, Bayesian Inference, Gibbs Sampling Multinomial Distribution - Explains the relationship of Bernoulli to Multinomial, then goes into Gibbs Sampling Word/Document Structures - Bag of Words, Word Document Matrix LDA - A Generative Model LDA - Inference "],
["package-references.html", "Package References", " Package References This book only came together due to the amazing open source work so many have done in the R community. The list of packages utilized to create this book can be found in the reference section and are as follows: (R Core Team 2016), (Müller 2017), (Xie 2017a), (Plummer et al. 2016), (Wickham et al. 2017), (Wickham and Chang 2016), (Zhu 2017), (Xie 2017b),(Wild 2015), (Ripley 2017), (Martin et al. 2017), (Henry and Wickham 2017), (Eddelbuettel et al. 2017), (Wickham, Hester, and Francois 2017), (Bouchet-Valat 2014),(Müller and Wickham 2017), (Wickham and Henry 2017), (Wickham 2017) "],
["what-is-lda.html", "1 What is LDA? 1.1 Animal Generator 1.2 Inference", " 1 What is LDA? Latent Dirichlet Allocation (LDA) is a generative probablistic model for collections of discrete data developed by Blei, Ng, and Jordan. (Blei, Ng, and Jordan 2003) One of the most common uses of LDA is for modeling collections of text. LDA is often used for what is known as topic modeling. A topic is a probability distribution over words.(Steyvers and Griffiths 2007) Imagine you have a bag that has a bunch of little squares in it with a word printed on each (similar to the game Scrabble, but words instead of letters). Any word not in the bag has a probability of being drawn equal to zero. However all other words in the bag have a probability greater than zero. Let’s say we have 2 chips with the word ‘philadelphia’ and 1 with the word ‘eagles’ on it. We would say you have a 1/3 chance of drawing ‘eagles’, 2/3 chance of drawing ‘philadelphia’, and 0 for any other word. This is effectively what a topic is; it provides us with the probabilities of a set of words for the given topic. —————–IMAGE PLACE HOLDER - different bags of topics————————- The general idea of LDA is that each document is generated from a mixture of topics and each of those topics is a mixture of words. This can be used as a mechanism for generating new documents, i.e. we know the topics a priori, or for inferring topics present in a set of documents we already have. In regards to the model name, you can think of it as follows: Latent: Topic structures in a document are ‘latent’ meaning they are hidden structures in the text. Dirichlet: The Dirichlet distribution determines the mixture proportions of the topics in the documents and the words in each topic. Allocation: Allocation of words to a given topic. To review: we have latent structures in a corpus (topics), with topic distributions in each document and word distributions in each topic based on the Dirichlet distribution, to allocate words to a given topic and topics to a given document. I realize many reading this may be unfamiliar with some of the terminology and distributions mentioned in the opening paragraphs. Please keep reading, all the nuts an bolts will be addressed in the following chapters, but to help get an understanding of what LDA is and why it is useful, I will offer a quick example and we will get to the math and technical concepts in the following chapters. 1.1 Animal Generator The majority of this book is about words, topics, and documents, but lets start with something a bit different: animals and where they live. One of the ways you can classify animals is by where they spend the majority of their time - land, air, sea. Obviously there are some animals that only dwell in one place; a cow only lives on land and a fish only lives in the sea. However, there are other animals, such as some birds, that split their time between land, sea, and air. You are probably asking yourself ‘where is he going with this?’. We can think of land, air, and sea as topics that contain a distribution of animals. In this case we can equate animals with words. For example, on land I am much more likely to see a cow than a whale, but in the sea it would be the reverse. If I quantify these probabilities into a distribution over all the animals (words) for each type of habitat (land,sea, air - topics) I can use them to generate sets of animals (words) to populate a given location (document) which may contain a mix of land, sea, and air (topics). So let’s move on to generating a specific location. We know that different locations will vary in terms of which habitats are present. For example, a beach contains land, sea, and air, but some areas inland may only contain air and land like a desert. We can define the mixture of these types of habitats in each location. For example, a beach is 1/3 land, 1/3 sea, and 1/3 air. We can think of the beach as a single document. To review: a given location (document) contains a mixture of land, air, and sea (topics) and each of those contain different mixtures of animals (words). Let’s work through some examples using our animals and habitats. The examples provided in this chapter are oversimplified so that we can get a general idea how LDA works. We’ll start by generating our beach location with 1/3 land animals, 1/3 sea animals, and 1/3 air animals. Below you can see our collection of animals and their probability in each topic. Note that some animals have zero probabilities in a given topic, i.e. a cow is never in the ocean, where some have higher probabilities than others; a crab is in the sea sometimes, but a fish is always in the sea. You may notice that there is only 1 animal in the air category. There are several birds, but only 1 of them is cabable of flight in our vocabulary. (NOTE: These are the probability of a word given the topic and therefore the probabilities of each habitat (column) sum to 1.) Table 1.1: Animal Distributions in Each Habitat vocab land sea air 🐋 0.00 0.12 0 🐳 0.00 0.12 0 🐟 0.00 0.12 0 🐠 0.00 0.12 0 🐙 0.00 0.12 0 🦀 0.05 0.06 0 🐊 0.05 0.06 0 🐢 0.05 0.06 0 🐍 0.05 0.06 0 🐓 0.10 0.00 0 🦃 0.10 0.00 0 🐦 0.05 0.06 1 🐧 0.05 0.06 0 🐿 0.10 0.00 0 🐘 0.10 0.00 0 🐂 0.10 0.00 0 🐑 0.10 0.00 0 🐪 0.10 0.00 0 To generate a beach (document) based off the description we would use those probabilities in a straightforward manner: words_per_topic &lt;- 3 equal_doc &lt;- c(vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$land, replace = T)], vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$sea, replace = T)], vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$air, replace = T)]) cat(equal_doc) ## 🦃 🐊 🐘 🐠 🐊 🐧 🐦 🐦 🐦 NOTE: In the above example the topic mixtures are static and equal, so each habitat (topic) contributes 3 animals to the beach. Before proceeding, I want to take a moment to give recognition to Tim Hopper for his presentation utilizing emoji to shed some light on how generative LDA works (Hopper 2016). Ok, now let’s make an ocean setting. In the case of the ocean we only have sea and air present, so our topic distribution in the document would be 50% sea, 50% air, and 0% land. words_per_topic &lt;- 3 ocean_doc &lt;- c(vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$sea, replace = T)], vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$air, replace = T)]) cat(ocean_doc) ## 🐋 🐢 🐋 🐦 🐦 🐦 NOTE: In the example above only the air and land contribute to the ocean location. Therefore they both contribute an equal number of animals to the location. 1.1.1 Generating the Mixtures It is important to note the examples above use static word and topic mixtures that were predetermined, but these mixtures could just as easily be created by sampling from a Dirichlet distribution. This is an important distinction to make as it is the foundation of how we can use LDA to infer topic structures in our documents. The Dirichlet distribution and it’s role in LDA is discussed in detail in the coming chapters. 1.2 Inference We have seen that we can generate collections of animals that are representative of the given location. What if we have thousands of locations and we want to know the mixture of land, air, and sea that are present? And what if we had no idea where each animal spends its time? LDA allows us to infer both of these peices of information. Similar to the locations (documents) generated above, I will create 100 random documents with varying length and various habitat mixtures. Table 1.2: Animals at the First Two Locations Document Animals 1 🐠 🐦 🐑 🐙 🐧 🐦 🐧 🐂 🦀 🐑 🐟 🐟 🐂 🐙 🐦 🐳 🐍 🐦 🐢 🦀 🐦 🐦 🐧 🐳 🐙 🐦 🐋 🐑 🐳 🐦 🦃 🐦 🐟 🐙 🐦 🐊 🐦 🐦 🐊 🐓 🐠 🐠 🐪 🐪 🐋 🐟 🐦 🐓 🐍 🐢 🐊 🐳 🐦 🐳 🐢 🐧 🐟 🐙 🐧 🐋 🐟 🐊 🐋 🐦 🐠 🐠 🐦 🐢 🐘 🐦 🐿 🐘 🐂 🐦 🐦 🐙 🐦 🐦 🐧 🦀 🐿 🐟 🐳 🐙 🐦 🐙 🐦 🐘 🐧 🐙 🐦 🐂 🐠 🐋 🐟 🐢 🐧 🐦 🐳 🐦 🐙 🐑 🐦 🐙 🐳 🐦 🦀 🐋 🐦 2 🐦 🐊 🐦 🐦 🐦 🐦 🐢 🐦 🦀 🐦 🐘 🐠 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐿 🐦 🐦 🐦 🐙 🐟 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐟 🐦 🐦 🐳 🐦 🐦 🐦 🐳 🐦 🐦 🐦 🐦 🐿 🐋 🐦 🐦 🐦 🐠 🐦 🐦 🐢 🐦 🦃 🐦 🦀 🐦 🐦 🐦 🐦 🐿 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🦀 🐋 🐦 🐦 🐦 🐦 🐦 🐦 🐓 🐦 🐦 🐦 🐦 🐦 🐦 🐋 🐋 🐓 🐟 🐦 🐦 🐟 🐦 🐦 🐳 🐦 🐟 🐦 🐧 The topic word distributions shown in Table 1.1 were used to generate our sample documents. The true habitat (topic) mixtures used to generate the first couple of documents are shown in Table 1.3: Table 1.3: Distribution of Habitats in the First Two Locations Document land air sea 1 0.20 0.14 0.67 2 0.08 0.61 0.30 With the help of LDA we can go through all of our documents and estimate the topic/word (habitat/animal) distributions and the topic/document (habitat/location) distributions. The true and estimated topic word distributions are shown in Table 1.4. Table 1.4: True and Estimated Word Distribution for Each Topic land estimated land air estimated air sea estimated sea 🐋 0.00 0.00 0.01 0 0.11 0.12 🐳 0.00 0.00 0.01 0 0.11 0.12 🐟 0.01 0.00 0.01 0 0.11 0.12 🐠 0.00 0.00 0.00 0 0.11 0.12 🐙 0.00 0.00 0.00 0 0.13 0.12 🦀 0.06 0.05 0.01 0 0.06 0.06 🐊 0.05 0.05 0.01 0 0.06 0.06 🐢 0.03 0.05 0.01 0 0.08 0.06 🐍 0.07 0.05 0.00 0 0.04 0.06 🐓 0.10 0.10 0.01 0 0.00 0.00 🦃 0.10 0.10 0.01 0 0.00 0.00 🐦 0.01 0.05 0.87 1 0.10 0.06 🐧 0.05 0.05 0.01 0 0.07 0.06 🐿 0.10 0.10 0.02 0 0.00 0.00 🐘 0.11 0.10 0.00 0 0.00 0.00 🐂 0.11 0.10 0.01 0 0.01 0.00 🐑 0.10 0.10 0.00 0 0.00 0.00 🐪 0.11 0.10 0.01 0 0.01 0.00 The document topic mixtures and the estimated mixtures are shown below for the first 5 documents: Table 1.5: The Estimated Topic Distributions for the First 5 Documents Location land estimated land air estimated air sea estimated sea 1 0.18 0.20 0.24 0.14 0.58 0.67 2 0.04 0.08 0.76 0.61 0.20 0.30 3 0.64 0.68 0.27 0.18 0.09 0.14 4 0.08 0.18 0.60 0.51 0.31 0.31 5 0.02 0.05 0.27 0.30 0.72 0.65 6 0.16 0.25 0.51 0.49 0.33 0.26 The results of our estimations of both the word topic distributions and the document topic distributions have some variation from the true distributions used to generate the documents. The cosine similarity between the estimated and true topic proportions in each document are shown below. Table 1.6: Cosine Similarity between Estimated Document Topic Distributions and Real Distributions land air sea 0.99 0.99 0.99 Table 1.7: Cosine Similarity between Estimated Topic Word Distributions and Real Distributions land air sea 0.98 1.00 0.98 "],
["parameter-estimation.html", "2 Parameter Estimation 2.1 Distributions 2.2 Inference: The Building Blocks 2.3 Maximum Likelihood 2.4 Maximum a Posteriori 2.5 Bayesian Inference 2.6 Gibbs Sampling", " 2 Parameter Estimation LDA is a generative probabilistic model, so to understand exactly how this works we need to understand the underlying probability distributions. In this chapter we will focus on the Bernoulli distribution and the Beta distribution. Both of these distributions are very closely related to (and also special cases of) the multinomial and Dirichlet distributions utilized by LDA, but they are a bit easier to comprehend. Once we have made our way through Bernoulli and beta, we will go over how they are linked to the multinomial and Dirichlet distributions. Throughout the chapter I’m going to build off of a simple example - a single coin flip. Let’s begin. 2.1 Distributions 2.1.1 Bernoulli When you flip a coin you get either heads or tails as an outcome (barring the possibility it lands on it’s side). This single coin flip is an example of a Bernoulli trial and we can use the Bernoulli distribution to calculate the probability of either outcome. Any single trial with two possible outcomes can be modeled as a Bernoulli trial: team wins/loses, pitch is a strike/ball, coin comes up heads or tails, etc. 2.1.1.1 Bernoulli: A Special Case of the Binomial Distribution You will often see Bernoulli distribution mentioned as a special case of the Binomial distribution. The binomial model consists of n Bernoulli trials, where each trial is independent and the probability of success does not change between trials.(Kerns 2010) The Bernoulli distribution is the case of a single trial (n=1). Quick Note: I will use the term success interchangably with the term heads when describing Bernoulli distribution. In reality success could be tails if you choose to define it that way. To clarify, if I want to calculate the probability of getting heads on a single coin flip I will use a bernoulli distribution. However, if I want to know the probability of getting 2 heads (or more) in a row, we would use the binomial distribution. For our purposes we are only concerned about the outcome of a single coin flip and will therefore stick to the Bernoulli distribution. Figure 2.1: Bernoulli and Binomial Distributions 2.1.1.2 Bernoulli - Distribution Notation The probability mass function of the bernoulli distribution is shown in Equation (2.1). \\[ \\begin{equation} f_{x}(x)=P(X=x)=\\theta^{x}(1-\\theta)^{1-x}, \\hspace{1cm} x = \\{0,1\\} \\tag{2.1} \\end{equation} \\] The only parameter of the bernoulli distribution is \\(\\theta\\), which defines the probability of success during a bernoulli trial. The value of x is 0 for a failure and 1 for a success. In a practical example you can think of this as 0 for tails and 1 for heads during a coin flip. (2.2) is an example where the value of \\(\\theta\\) is set to 0.7. We can see the probability of getting a success is 0.7, while the probability of failure is 0.3. \\[ \\begin{equation} \\begin{aligned} P(X=1)&amp;=\\theta^{1}(1-\\theta)^{1-1}, \\hspace{1cm} \\theta=0.7 \\\\ P(X=1)&amp;=0.7*1=0.7 \\\\\\\\ P(X=0)&amp;=0.7^{0}(1-0.7)^{1-0}\\\\ P(X=0)&amp;=0.3 \\end{aligned} \\tag{2.2} \\end{equation} \\] 2.1.2 Beta Distribution The beta distribution can be thought of as a probability distribution of probabilities (Robinson 2014). The Bernoulli distribution is a distribution that gives us a probability of success (coin comes up heads). The beta distribution provides a distribution of these probabilities. To clear this up we will be working through some examples. If you flip a coin 2 times resulting in 1 heads and 1 tails how sure are you that the coin is fair? Probably not all that sure. But what if you flipped the coin 200 times and it resulted in 100 heads and 100 tails? You would be much more confident that the coin is fair. The beta distribution gives us a way to quantify the probabilities (our confidence in the coin being fair). The beta distribution has 2 shape parameters, \\(\\alpha\\) and \\(\\beta\\). These can be thought of as the results from the coin flips we just talked about (i.e. \\(\\alpha=200\\), \\(\\beta=200\\)). Below the probability density for different values of \\(\\theta\\) is displayed based on different values of \\(\\alpha\\) and \\(\\beta\\). In general, the higher the value of \\(\\alpha\\) and \\(\\beta\\) the narrower the density curve is. Another way to think of this is that the more information (coin flip results) we have, the more confident we can be in our coin’s bias value (i.e. is it fair, head heavy, etc.). a &lt;- c(1, 10, 100) b &lt;- c(1, 10, 100) params &lt;- cbind(a,b) ds &lt;- NULL n &lt;- seq(0,1,0.01) for(i in 1:nrow(params)){ ds &lt;- rbind(data.frame(x = n, y = dbeta(n, params[i,1], params[i,2]), parameters = paste0(&quot;\\U03B1 = &quot;,params[i,1], &quot;, \\U03B2 = &quot;, params[i,2])), ds) } ggplot(ds, aes(x = x, y = y, color=parameters)) + geom_line() + labs(x = &#39;\\U03B8&#39;, y = &#39;Probability Density&#39;) + scale_color_discrete(name=NULL) + theme_minimal() Figure 2.2: Beta Distribution What about the cases where \\(\\alpha\\) and \\(\\beta\\) are not equal or close to equal? In those cases you would probably assume a bit of skew in the distribution, i.e. your coin may be biased toward head (density skewed toward 1) or tails (density skewed toward 0) as shown below. a &lt;- c(8, 2) b &lt;- c(2, 8) params &lt;- cbind(a,b) ds &lt;- NULL n &lt;- seq(0,1,0.01) for(i in 1:nrow(params)){ ds &lt;- rbind(data.frame(x = n, y = dbeta(n, params[i,1], params[i,2]), parameters = paste0(&quot;\\U03B1 = &quot;,params[i,1], &quot;, \\U03B2 = &quot;, params[i,2])), ds) } ggplot(ds, aes(x = x, y = y, color=parameters)) + geom_line() + labs(x = &#39;\\U03B8&#39;, y = &#39;Probability Density&#39;) + scale_color_manual(name=NULL, values = c(&quot;#7A99AC&quot;, &quot;#E4002B&quot;)) + theme_minimal() Figure 2.3: Beta Distribution - Skewed The probability distribution function for the beta distribution can be found in Equation (2.3). \\[ \\begin{equation} f(\\theta;\\alpha,\\beta) ={{\\theta^{(\\alpha-1)}(1-\\theta)^{(\\beta-1)}}\\over B(\\alpha,\\beta)} \\tag{2.3} \\end{equation} \\] Quick Note: The Beta function, B is the ratio of the product of the Gamma function, \\(\\Gamma\\), of each parameter divided by the Gamma function of the sum of the parameters. The Beta function is not the same as the beta distribution. The Beta function is shown below along with the Gamma function, which is used in the Beta function. \\[ \\begin{equation} \\beta(a,b) = {\\Gamma(a)\\Gamma(b) \\over{\\Gamma(a+b)}} \\tag{2.4} \\end{equation} \\] The Gamma function is the factorial of the parameter minus 1. \\[ \\begin{equation} \\Gamma(a) = (a-1)! \\tag{2.5} \\end{equation} \\] 2.2 Inference: The Building Blocks Equation (2.6) is a fundamental to understanding parameter estimation and inference. \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} = {\\overbrace{p(D|\\theta)}^{likelihood} \\overbrace{p(\\theta)}^{prior} \\over \\underbrace{p(D)}_{evidence}} \\tag{2.6} \\end{equation} \\] The 4 components are: Prior: The probability of the parameter(s). Defines our prior beliefs of the parameter. Do we believe to a good degree of certainty that the coin is fair? Maybe take a step back and ask yourself ‘do I trust the manufacturer of this coin?’. If this manufacturer has always had great quality (i.e. fair coins) then you would have confidence your coin is fair. If you know nothing about where the coin came from you would be more skeptical of the coin’s bias. Posterior: The probability of the parameter given the evidence. Think of it this way, given 100 coin flips with 47 heads and 53 tails what is the probability that theta is 0.5 (coin is fair)? The only way to know this value is if you already obtained the evidence. However we can estimate the posterior in various ways some of which will be covered in this chapter. Likelihood: The probability of the evidence given the parameter. Given that we know the coin is fair (theta = 0.5) what is the probability of having 47 heads out of 100 flips? Evidence: The probability of all possible outcomes. Probability of 1/100 heads, 2/100 heads, …, 100/100 heads. Conditioning your brain for LDA : We are starting with a coin flip, but the eventual goal is to link this back to words appearing in a document. Try to keep in mind that we think of a word similar to the outcome of a coin: a word exists in the document (heads!) or a word doesn’t exist in the document (tails!). 2.3 Maximum Likelihood The simplest method of parameter estimation is the maximum likelihood (ML) method. Effectively we calculate the parameter (\\(\\theta\\)) that maximizes the likelihood. \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} = {\\overbrace{\\bf \\Large p(D|\\theta)}^{\\bf \\Large LIKELIHOOD} \\overbrace{p(\\theta)}^{prior} \\over \\underbrace{p(D)}_{evidence}} \\tag{2.7} \\end{equation} \\] Let’s first discuss what the likelihood is. The likelihood can be described as the probability of getting observed data given a specified value of the parameter, \\(\\theta\\). For example, let’s say I’ve flipped a coin 10 times and got 5 heads, 5 tails. Assuming the coin is fair, \\(\\theta\\) equals 0.5, what is the likelihood of observing 5 heads and 5 tails. To calculate the likelihood of a parameter given an outcome we would use the probability mass function in Equation (2.8). \\[ \\begin{equation} P(X=x)=\\theta^{x}(1-\\theta)^{1-x}, \\hspace{1cm} x = \\{0,1\\} \\tag{2.8} \\end{equation} \\] Where an outcome, x, of heads equals 1 and an outcome of tails is 0. Now let’s say we have carried out 10 coin flips: \\[ \\begin{equation} \\begin{aligned} P(X_{1}=x_{1},X_{2}=x_{2},...,X_{10}=x_{10}) &amp;= \\prod\\limits_{n=1}^{10} \\theta^{x}(1-\\theta)^{1-x}\\\\ L(\\theta) &amp;= \\prod\\limits_{n=1}^{10} \\theta^{x}(1-\\theta)^{1-x} \\end{aligned} \\tag{2.9} \\end{equation} \\] What is shown in Equation (2.9) is the joint probability mass function. Each coin flip is independent so we calculate the product of the PMF’s for each trial. This is known as the likelihood function - the likelihood of a value of \\(\\theta\\) given our observed data. Back to the maximum likelihood…. Our goal is to find the value of \\(\\theta\\) which maximizes the likelihood of the observed data. To derive the maximum likelihood we start by taking the log of the likelihood, \\(\\mathcal{L}\\). \\[ \\begin{equation} \\begin{aligned} \\mathcal{L} &amp;= log \\prod\\limits_{n=1}^N \\theta^{x}(1-\\theta)^{1-x} \\\\\\\\ &amp;= \\sum\\limits_{n=1}^N log(\\theta^{x}(1-\\theta)^{1-x}) \\\\\\\\ &amp;= n^{(1)}log(\\theta) + n^{(0)}log(1-\\theta) \\end{aligned} \\tag{2.10} \\end{equation} \\] Then differentiate with respect to \\(\\theta\\): \\[ \\begin{equation} {d\\mathcal{L} \\over d\\theta} = {n^{(1)}\\over \\theta} - {n^{(0)}\\over 1-\\theta} \\tag{2.11} \\end{equation} \\] Then set the left side of the equation equal to zero and solve: \\[ \\begin{equation} \\begin{aligned} 0 &amp;= {n^{(1)}\\over \\theta} - {n^{(0)}\\over 1-\\theta} \\\\ \\\\ {n^{(1)}\\over\\theta} &amp;= {n^{(0)}\\over 1-\\theta} \\\\ \\\\ {n^{1} - \\theta n^{1}} &amp;= {\\theta n^{0}} \\\\ \\\\ n^{(1)} &amp;= \\theta(n^{(1)} + n^{0}) \\\\ \\\\ \\theta &amp;= {n^{(1)} \\over N} \\end{aligned} \\tag{2.12} \\end{equation} \\] The value of \\(\\theta\\) that maximizes the likelihood is the number of heads over the number of flips. The maximum likelihood value for 1 out of 10 flips equal to head up to 10 heads out of 10 flips is shown in Figure 2.4. heads = 1:10 flips = 10 ds &lt;- data.frame(heads, flips = rep(flips, length(heads))) ds$theta &lt;- ds$heads/ds$flips ggplot(ds, aes(x = heads, y = theta)) + geom_point(color =&#39;#1520c1&#39;, size = 3) + geom_linerange(aes(x=heads, y=NULL, ymax=theta, ymin=0)) + scale_x_continuous(breaks = seq(0,10,2), labels = seq(0,10,2)) + labs(y=&#39;\\U03B8&#39;, x=&quot;Number of Heads&quot;, title =&quot;ML Parameter Estimation: 10 Bernoulli Trials&quot;) + theme(plot.title = element_text(hjust = 0.5)) + theme_minimal() Figure 2.4: Bernoulli Maximum Likelihood 2.4 Maximum a Posteriori Maximum a Posteriorir (MAP) is similar to the method of maximum likelihood estimation, but it also includes information about our prior beliefs. Unlike ML estimation, MAP estimates parameters by maximizing the posterior. \\[ \\begin{equation} \\theta_{MAP}=\\underset{\\theta}{\\operatorname{argmax}} P(\\theta|X) \\tag{2.13} \\end{equation} \\] \\[ \\begin{equation} \\require{enclose} \\theta_{MAP}=\\underset{\\theta}{\\operatorname{argmax}}{\\overbrace{p(D|\\theta)}^{likelihood} \\overbrace{p(\\theta)}^{prior} \\over \\underbrace{\\enclose{horizontalstrike}{p(D)}}_{evidence}} \\tag{2.14} \\end{equation} \\] The formula for the MAP estimation of \\(\\theta\\) is shown in (2.14). The evidence term is dropped during the calculation of \\(\\theta_{MAP}\\) since it is not a function of \\(\\theta\\) and our only concern is maximizing the posterior based on \\(\\theta\\). Similar to calculating the ML estimate, the first step is to apply the log function to the remaining terms (Equation (2.15)). \\[ \\begin{equation} \\begin{aligned} \\theta_{MAP} &amp;=\\underset{\\theta}{\\operatorname{argmax}}p(D|\\theta)p(\\theta) \\\\\\\\ &amp;= \\mathcal{L}(\\theta|D) + log(p(\\theta)) \\end{aligned} \\tag{2.15} \\end{equation} \\] We have already derived the log likelihood during our derivation of the maximum likelihood, so let’s now focus on the prior. The prior for the Bernoulli distribution is the Beta distribution and can be used to describe \\(p(\\theta)\\). The probability distribution function, PDF, for the Beta distribution is shown in Equation (2.16). \\[ \\begin{equation} p(\\theta|\\alpha,\\beta) = {\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\over{B(\\alpha, \\beta)}} \\tag{2.16} \\end{equation} \\] Next, we plug in the PDF of the beta distribution in the place of the prior (Equation (2.17)). $$ \\[\\begin{equation} \\begin{aligned} \\theta_{MAP}&amp;= \\mathcal{L}(\\theta|D) + log(p(\\theta)) \\\\\\\\ \\theta_{MAP}&amp;= n^{(1)}log(\\theta) + n^{(0)}log(1-\\theta) + log({\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\over{B(\\alpha, \\beta)}}) \\\\\\\\ \\theta_{MAP}&amp;= n^{(1)}log(\\theta) + n^{(0)}log(1-\\theta) + \\\\ &amp;\\quad log({\\theta^{\\alpha-1}) + log((1-\\theta)^{\\beta-1}) - log({B(\\alpha, \\beta)}}) \\\\\\\\ {d \\over d\\theta} \\mathcal{L}(\\theta|D) + log(p(\\theta)) &amp;= {n^{(1)}\\over \\theta} - {n^{(0)}\\over 1-\\theta} + {\\alpha - 1\\over\\theta} - {\\beta - 1 \\over 1-\\theta} \\\\\\\\ 0 &amp;= {n^{(1)}\\over \\theta} - {n^{(0)}\\over 1-\\theta} + {\\alpha - 1\\over\\theta} - {\\beta - 1 \\over 1-\\theta} \\\\\\\\ \\theta_{MAP}&amp;= {{n^{(1)} + \\alpha -1} \\over {n^{(1)} + n^{0} + \\alpha + \\beta - 2}} \\end{aligned} \\tag{2.17} \\end{equation}\\] $$ Now that we know how to calculate the parameter \\(\\theta\\) that maximizes the posterior, lets take a look at how choices of different priors and different numbers of observed trials effects our outcome. In the Figure 2.5 we see the MAP estimation of \\(\\theta\\) with a relatively uninformed prior, low \\(\\alpha\\) and \\(\\beta\\) values and a small number of observed experiments (n=20). Uninformed means we are going to make a weak assumption about the prior. In more general terms, this means that we don’t have a strong intuition that our coin is fair or unfair. In a mathematical sense, uninformed means a low sum of the two parameters, if you had \\(\\alpha=1000\\) and \\(\\beta=1\\) this would not be considered as uninformed whereas \\(\\alpha\\) and \\(\\beta\\) both equal to 1 is normally referred to as uniformed or naive. The Beta distribution has \\(\\alpha\\) and \\(\\beta\\) parameters of 2 resulting in the blue density curve shown in Figure 2.5. NOTE: The terms weak, uninformed, and naive are often used interchangeably when referencing priors. The same goes for the terms strong and informed. # description: # ml will yeild the expected average and is not effected by the prior # map uses a weak assumption - uniform density for all values of theta # this results in a theta_map value very similar to the ml value n &lt;- 20 heads &lt;- 14 tails &lt;- 6 # ml ml_theta &lt;- heads/n # map B &lt;- 2 alpha &lt;- 2 map_theta &lt;- (heads + alpha - 1)/(heads + tails + alpha + B -2) possible_theta &lt;- seq(0,1,0.01) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) ggplot(beta_ds, aes(x = theta, y = density)) + geom_point(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.1, y=0.5, label= paste(&quot;\\U03B8[MAP]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;) + theme_minimal() Figure 2.5: MAP: Small number of experiments and uninformed prior In the next example we have the same number of samples, but we assume a much stronger prior. In particular, we assume the coin is most likely fair by selecting \\(\\alpha\\) and \\(\\beta\\) parameters both equal to 100. These parameters represent a Beta distribution that is centered at 0.5 and is very dense around that point. We can see the resuting MAP estimate is much closer to 0.5 than our previous example with the uniformed prior. # description: # the strong assumption of a &#39;fair&#39; coin prior reduces the width of the # distribution, i.e. much higher probability density near theta (p) of 0.5 # This forces the MAP theta value to stay much closer to the prior due to the # small amount of observed evidence n &lt;- 20 heads &lt;- 14 tails &lt;- 6 # ml ml_theta &lt;- heads/n # map B &lt;- 100 alpha &lt;- 100 map_theta &lt;- (heads + alpha - 1)/(heads + tails + alpha + B -2) possible_theta &lt;- seq(0,1,0.001) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) ggplot(beta_ds, aes(x = theta, y = density)) + geom_line(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.1, y=11, label= paste(&quot;\\U03B8[MAP]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;, y = &#39;Density&#39;) + theme_minimal() Figure 2.6: MAP: Small number of experiments and informed prior What happens when we have a much larger number of observed experiments and an uninformed prior? In this case the MAP estimate will give us a value that is very close to a maximum likelihood estimate, i.e. heads divide by number of trials. # description # high number of observed samples (evidence) # weak prior - uniform # ml and map are close to one another - close... as expected n &lt;- 1000 heads &lt;- 723 tails &lt;- n-heads # ml ml_theta &lt;- heads/n # map B &lt;- 2 alpha &lt;- 2 map_theta &lt;- (heads + alpha - 1)/(heads + tails + alpha + B -2) possible_theta &lt;- seq(0,1,0.001) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) ggplot(beta_ds, aes(x = theta, y = density)) + geom_line(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.1, y=1.2, label= paste(&quot;\\U03B8[MAP]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;, y = &#39;Density&#39;) + theme_minimal() Figure 2.7: MAP: Large number of experiments and uninformed prior Now let’s assume a much stronger prior, i.e. more confidence a priori the coin is fair, while using the same number of experiments. Notice that when we use a larger number of experiments it overpowers the strong prior and gives us a very similar MAP estimate in comparison to the example with the uninformed prior and the same number of experiments (Uninformed - Figure 2.7 and Informed - Figure 2.8). n &lt;- 1000 heads &lt;- 723 tails &lt;- n-heads # ml ml_theta &lt;- heads/n # map B &lt;- 100 alpha &lt;- 100 possible_theta &lt;- seq(0,1,0.001) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) map_theta &lt;- (heads + alpha - 1)/(heads + tails + alpha + B -2) ggplot(beta_ds, aes(x = theta, y = density)) + geom_line(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.1, y=8.2, label= paste(&quot;\\U03B8[MAP]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;, y = &#39;Density&#39;) + theme_minimal() Figure 2.8: MAP: Large number of experiments and informed prior In summary: The stronger your prior assumptions are, the more observations you will need to overcome an incorrect prior estimation/belief. Stronger prior - MAP estimate moves toward most dense area of prior distribution Weaker prior - MAP looks more like a maximum likelihood 2.5 Bayesian Inference 2.5.1 Analytical Solution Another option we have for estimating parameters is to estimate the posterior of the distribution via Bayesian inference. In our MAP estimation example we included prior assumptions as part of our calculation. We are going to do the same via Bayesian inference however instead of a point estimate of \\(\\theta\\), we will be calculating the posterior distribution over all possible values of \\(\\theta\\). From this we can take the expected value of \\(\\theta\\) as our estimated parameter. In the case of the coin flip, Bayesian inference can be solved analytically. We have reviewed the likelihood and the prior, but when estimating the posterior we need to include the evidence term. This is somewhat tricky. \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} = { p(D|\\theta) p(\\theta) \\over \\underbrace{p(D)}_{evidence}} \\tag{2.18} \\end{equation} \\] \\[ \\begin{equation} p(D) = \\int_{\\theta}p(D|\\theta)p(\\theta)d\\theta \\tag{2.19} \\end{equation} \\] \\[ \\begin{equation} {p(\\theta|z,N)} = {\\overbrace{\\theta^z(1-\\theta)^{(N-z)}}^{likelihood} \\overbrace{{\\theta^{(a-1)}(1-\\theta)^{(b-1)}}\\over \\beta(a,b)}^{prior}\\over{\\underbrace{\\int_{0}^1 \\theta^z(1-\\theta)^{(N-z)}{{\\theta^{(a-1)}(1-\\theta)^{(b-1)}}\\over \\beta(a,b)}d\\theta}_{Evidence}}} \\tag{2.20} \\end{equation} \\] In Equation (2.19) we see the evidence term is an integral over all values of \\(\\theta\\), 0 to 1. This means the value is a constant. Recall that the evidence term acts as a scaling factor to ensure our probabilities sum to 1. We will plug in a generic placeholder of our evidence value in (2.21) since we have established that it is a constant. \\[ \\begin{equation} \\begin{aligned} p(\\theta|z, N) &amp;= {\\theta^{z}(1-\\theta)^{(N-z)}{{\\theta^{(a-1)}(1-\\theta)^{(b-1)}}\\over \\beta(a,b)}\\over{C}} \\\\\\\\ &amp;= {\\theta^{(z+a-1)}(1-\\theta)^{(N-z+b-1)} \\over \\beta(z+a, N-z+b)}\\\\\\\\ &amp;= Beta(z+a, N-z+b) \\end{aligned} \\tag{2.21} \\end{equation} \\] In the second step of Equation @(eq:bernBIFinal) the beta function (the denomitor of the prior) takes the place of the constant (i.e. the evidence value). The beta function is a constant value and we need it to do the same job as the evidence term; it needs to ensure the probabilities sum to 1. To ensure this we alter the input parameters to the beta function, so that we are left with a beta distribution for our posterior that includes our observed experiments and our prior’s hyperparameters. To estimate \\(\\theta\\), we calculate the expected value of a Beta distribution as shown in Equation @(eq:bernBIExp). \\[ \\begin{equation} E[\\theta]={\\alpha \\over \\alpha + \\beta} \\tag{2.22} \\end{equation} \\] Plugging in our parameters, as per the derivation in Equation @(eq:bernBIFinal), we get the resulting expected value of \\(\\theta\\) shown in @(eq:bernBIExpFinal) \\[ \\begin{equation} \\begin{aligned} E[\\theta] &amp;= {z+a \\over {z+a + n-z+b}}\\\\\\\\ &amp;={z+a \\over {a+N+b}}\\\\\\\\ \\end{aligned} \\tag{2.23} \\end{equation} \\] Figure 2.9 shows an example of the analytical Bayesian inference solution for a small number of flips, 20, resulting in 14 heads while assuming a weak prior (\\(\\alpha\\)=\\(\\beta\\) = 2). It is worth noting that Bayesian inference is effected by the selection of a prior and the number of observations in the same way MAP estimation is. n &lt;- 20 heads &lt;- 14 tails &lt;- n-heads # ml ml_theta &lt;- heads/n # map B &lt;- 2 alpha &lt;- 2 possible_theta &lt;- seq(0,1,0.001) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) map_theta &lt;- (heads + alpha)/(alpha + n + B) ggplot(beta_ds, aes(x = theta, y = density)) + geom_line(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.08, y=1.4, label= paste(&quot;\\U03B8[BI]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;, y = &#39;Density&#39;) + theme_minimal() Figure 2.9: Bayesian Inference: Analytical Solution (n=20) 2.6 Gibbs Sampling The remainder of this chapter will tackle the concept of Gibbs sampling for estimating a posterior distribution. Before getting into specifics let’s touch on why a person may need to use a method such as Gibbs sampling. 2.6.1 The Issue of Intractability The coin flip example solved via Bayesian inference was capable of being solved analytically. However in many cases of Bayesian inference this is not possible due to the intractability of solving for the marginal likelihood (evidence term). We do have other options for solutions such as Gibbs sampling, expectation-maximization, and Metropolis-Hastings methods. This book will only focus on Gibbs Sampling, but be aware other types of solvers are used for Bayesian inference problems including LDA. 2.6.2 A Tale of Two MC’s Gibbs Sampling is a Markov Chain Monte Carlo (MCMC) technique for parameter estimation. Let’s break down the two MC’s. A markov chain is a process where the next state is determined by the current state. More importantly it does not rely on any information prior to the current state. ????? 1. This needs a diagram and possibly a general example. * Weather - Sunny/Rainy (it’s always sunny in philadelphia), Next word in sentence (probably too complicated), board game (dice rolls) LETS MAKE SURE TO REITERATE THIS AFTER SHOWING THE GENERAL GIBBS SAMPLING MATH AND DIAGRAM ???? The other MC, Monte Carlo, is a technique used to solve a variety of problems by repeated random sampling. A common example used to showcase Monte Carlo methods is the estimation of the value of \\(\\pi\\). (Resnik and Hardisty 2010) All we need is some chalk and a bucket of rice. First I draw a circle on the ground. Then I then draw a square along the circumference of the square. Now for the bucket of rice, aka our random number generator. I stand over the square and uniformly pour rice over the area of the square. Now comes the monotonous part, counting the rice. First I count the number of pieces of rice inside the circle. Next I tally the number of pieces of rice remaining. (Side note: you could save yourself a bunch of time and just weight the two proportions of rice instead of straining your vision and wasting your time.) From here I can infer the value of \\(\\pi\\) as follows: \\[ \\begin{equation} \\begin{aligned} {\\pi r^{2} \\over (2r)^{2}} &amp;= {Rice_{circle}\\over Rice_{circle + square}} \\\\\\\\ \\pi &amp;\\approx {4Rice_{circle}\\over Rice_{circle + square}} \\end{aligned} \\tag{2.24} \\end{equation} \\] 2.6.3 Conjugate Distributions and Priors Before wading into the deeper water that is Gibbs sampling I need to touch on the concept of conjugate distributions and priors. A conjugate distribution pair is a posterior distribution that have the same form as the prior distribution. Bernoulli and beta distributions are conjugate distributions. The posterior of the bernoulli distribution has the same general form as the prior, the beta distribution. Therefore we call the beta distribution a conjugate prior of the bernoulli distribution. We will cover this specific conjugate relationship in detail in the following section, but you can look ahead at Equations (2.27) (2.28) to see the similar form of the posterior compared to the prior. Ok, but what does that mean for us? We can use the conjugate relationship to: Simplify our posterior estimation to the same from as the prior. Step 1 allows us to then sample from a known distribution to estimate our posterior distribution. Step 2 is a crucial component in Gibbs sampling - it is the Monte Carlo piece of the method. To help reinforce the concept of conjugate priors we will go through an example. 2.6.3.1 Bernoulli &amp; Beta We return to our coin flip example. We want to estimate the posterior, the probability of \\(\\theta\\) given the the results of all experiments (this includes experiments that we haven’t yet completed). Obviously we don’t have data which does not yet exist (i.e. all experiments) so we will need to use the likelihood and the prior to estimate the posterior. In the previous example we used the Beta distribution for our prior. As we now know, the Beta distribution is the conjugate prior of the Bernoulli distribution. We will take a closer look at this relationship now. We start with our base relationship between posterior, likelihood, prior, and posterior as shown in Equation (2.25). \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} = {\\overbrace{p(D|\\theta)}^{likelihood} \\overbrace{p(\\theta)}^{prior} \\over \\underbrace{p(D)}_{evidence}} \\tag{2.25} \\end{equation} \\] When estimating the posterior we drop out the evidence term. To reiterate, we can drop out the evidence term because it is constant and is used as a normalizing factor for scaling our probabilities to so that they sum to 1. We will see in the upcoming section that Gibbs sampling accounts for the scaling issue and allows us to infer probabilities at the correct scale. MAYBE ADD SOMETHING HERE ALONG THESE LINES OTHERWISE JUST SAVE IT FOR GIBBS SECTION….(Gibbs sampling accomplishes this normalization by taking a large number of samples and then dividing by the samples by the sum of all samples resulting in a proportion between 0 and 1. ) \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} \\propto {\\overbrace{p(D|\\theta)}^{likelihood} \\overbrace{p(\\theta)}^{prior}} \\tag{2.26} \\end{equation} \\] To estimate the posterior of the bernoulli distribution we plug in our likelihood and prior (Beta distribution) equations in Equation (2.27). \\[ \\begin{equation} p(\\theta|z,N) \\propto \\overbrace{\\theta^z(1-\\theta)^{(N-z)}}^{likelihood} \\overbrace{{\\theta^{(a-1)}(1-\\theta)^{(b-1)}}\\over \\beta(a,b)}^{prior} \\tag{2.27} \\end{equation} \\] After combining terms we end up with Equation (2.28) which looks very similar to @(bernBIFinal). This is a very simple example where an analytical solution for the posterior is possible, so the equations are effectively the same with the exeption of the \\(\\propto\\) in place of the \\(=\\). The \\(\\propto\\) is used because we are going to base our posterior off of random samples from the Beta distribution with the parameters shown in (2.28). \\[ \\begin{equation} \\begin{aligned} p(\\theta|z,N) &amp;\\propto {\\overbrace{\\theta^{(a + z -1)}(1-\\theta)^{(N-z+b-1)}}^{Same \\hspace{1 mm} Pattern \\hspace{1 mm} as \\hspace{1 mm} Prior}\\over{\\beta(a,b)}}\\\\ p(\\theta|z,N) &amp;\\propto Beta(a+z, N-z+b) \\end{aligned} \\tag{2.28} \\end{equation} \\] 2.6.4 Gibbs Sampling Gibbs sampling is a Markov Chain Monte Carlo technique that can be used for estimating parameters by walking through a given parameter space. A generalized way to think of Gibbs sampling is estimating a given parameter based on what we currently know about observed data and the other parameters of the model. You may be thinking ‘What other parameters?’. Gibbs sampling is only really applicable when you are trying to estimate multiple parameters otherwise you would use ML, MAP, or analytical Bayesian inference. Since Gibbs sampling is suited for the estimation of multiple parameters our coin flip example will be expanded slightly. We will now attempt to calculate the bias of two coins and determine if there is a difference in bias between the two using Gibbs sampling. For the purposes of Gibbs Sampling we need to do a bit of math to determine the equations for our posterior conditional equations. The posterior conditional is the posterior of a single parameter, given the current values of all other parameters in the model. This is where the Markov Chain component of Gibbs sampling comes into play. The conditional posterior of a parameter, i.e. the next state, is determined based on all the current parameter values, i.e. the current state. Now is a good time to highlight the general structure for using your conjugate priors and how to get to the equations required for the sampling process. The general process for derivation of our sampling distribution, as outlined in (Yildirim 2012), is as follows: Derive the full joint density. Derive the posterior conditionals for each of the random variables in the model. Simulate samples from the posterior joint distribution based on the posterior conditionals. So how do we get the posterior conditionals for our two coin problem? First let’s go over what we need to know: \\(\\theta_{1}\\) : Bias of coin 1 \\(\\theta_{2}\\) : Bias of coin 2 The full joint density that defines our problem is shown in Equation (2.29). \\[ \\begin{equation} \\begin{aligned} p(\\theta_{1}, \\theta_{2}|z_{1}, z_{2}, N) &amp;\\propto p(\\theta_{1}|z_{1}, N)p(\\theta_{2}|z_{2}, N) \\\\ p(\\theta_{1}, \\theta_{2}|z_{1}, z_{2}, N) &amp;\\propto p(z_{1}|\\theta_{1})p(z_{2}|\\theta_{2})p(\\theta_{1})p(\\theta_{2}) \\tag{2.29} \\end{aligned} \\end{equation} \\] Then we break down the full joint density into our posterior conditionals: \\[ \\begin{equation} \\begin{aligned} p(\\theta_{1}|z_{1}, N) &amp;\\propto p(z_{1}|\\theta_{1})p(\\theta_{1}) \\\\ p(\\theta_{2}|z_{2}, N) &amp;\\propto p(z_{2}|\\theta_{2})p(\\theta_{2}) (\\#eq:bernFullJointStep2) \\end{aligned} \\end{equation} \\] We plug in posterior conditional for a single coin, which we derived in Equation (2.28). The resulting posterior conditionals are shown below in Equation (2.30). \\[ \\begin{equation} \\begin{aligned} p(\\theta_{1}|z_{1},N) &amp;\\propto Beta(a_{1}+z_{1}, N-z_{1}+b_{1}) \\\\ p(\\theta_{2}|z_{2},N) &amp;\\propto Beta(a_{2}+z_{2}, N-z_{2}+b_{2}) \\end{aligned} \\tag{2.30} \\end{equation} \\] Now that we have our posterior conditionals we can move on to estimating our parameters via Gibbs sampling. The general form of Gibbs Sampling is shown in Equation (2.31). \\[ \\begin{equation} \\begin{aligned} For \\ i \\ in \\ iterations:\\\\ &amp;p(\\theta_{1}^{i+1}) \\sim p(\\theta_{1}^{i}|\\theta_{2}^{i}, \\theta_{3}^{i},..., \\theta{n}^{i}) \\\\ &amp;p(\\theta_{2}^{i+1}) \\sim p(\\theta_{2}^{i}|\\theta_{1}^{i+1}, \\theta_{3}^{i},..., \\theta{n}^{i}) \\\\ &amp;p(\\theta_{3}^{i+1}) \\sim p(\\theta_{3}^{i}|\\theta_{1}^{i+1}, \\theta_{2}^{i+1},..., \\theta{n}^{i}) \\\\ &amp;................................ \\\\ &amp;p(\\theta_{n}^{i+1}) \\sim p(\\theta_{n}^{i}|\\theta_{1}^{i+1}, \\theta_{2}^{i+1},..., \\theta_{n-1}^{i+1}) \\\\ \\end{aligned} \\tag{2.31} \\end{equation} \\] Gibbs sampling works by estimating all parameters via the posterior conditional iteratively for a set number of iterations or a distinct stopping criteria/convergence measure. For the sake of our example we will stick with a set number of iterations. In Equation (2.31) we see the next estimate, i+1, of \\(\\theta_{1}\\) is based on all other current parameter values. When estimating \\(\\theta_{2}\\) the i+1 value of \\(\\theta_{1}\\) is used along with all of the current (i) parameter values. This continues for all parameter values. After the the nth parameter is estimated, the process starts all over again for the next iteration. Without the math it looks like this: FIGURE OF 3 CIRCLES CROSSED OUT/OPEN/COLORED - BEING ESTIMATED, CURRENT STEP, PAST STEP The calculations done in our example are shown in Equation (2.32). Note that since the two coins are independent of one another, we are sampling from a Beta distribution based on each coin’s priors and evidence then repeating. \\[ \\begin{equation} \\begin{aligned} For \\ i \\ in \\ iterations:\\\\ p(\\theta_{1}^{i+1}|z_{1},N) &amp;\\propto Beta(a_{1}+z_{1}, N-z_{1}+b_{1}) \\\\ p(\\theta_{2}^{i+1}|z_{2},N) &amp;\\propto Beta(a_{2}+z_{2}, N-z_{2}+b_{2}) \\end{aligned} \\tag{2.32} \\end{equation} \\] Burn In Period The purpose of Gibbs Sampling is to sample from the posterior and estimate a parameter value assuming our sampling converges on the true parameter. However it often takes time, i.e. many samples, to move into an area of convergence. To be clear, this is a non-issue for the current example due to the independence between the coins. In the change point example that follows, the parameters are not independent of one another and therefore a burn-in period will be necessary. 2.6.5 Bias of Two Coins The following example will generate samples from the posterior distributions of two different coins via Gibbs sampling for the purposes of estimating each coin’s bias. We use a fairly weak prior by setting \\(\\alpha\\) and \\(\\beta\\) to a value of 2 for each coins. We have the results of 20 coin flips for each coin. Coin 1 has 10 heads out of 20 flips while coin 2 has 3 heads out of 20 flips. The Gibbs sampling code below is based on the example provided on Duke’s Computational Statistics and Statistical Computing course website. (Chan and McCarthy 2017) a = 2 b = 2 z1 = 10 N1 = 20 z2 = 3 N2 = 20 theta = rep(0.5,2) niters = 10000 burnin = 500 thetas = matrix(0, nrow = (niters-burnin), ncol=2) for (i in 1:niters){ theta1 = rbeta(n = 1, shape1 = a + z1, shape2 = b + N1 - z1) # get value theta2| all other vars theta2 = rbeta(n = 1, shape1 = a + z2, shape2 =b + N2 - z2) if (i &gt;= burnin){ thetas[(i-burnin), ] = c(theta1, theta2) } } ds &lt;- data.frame(theta1 = thetas[,1], theta2= thetas[,2]) ggplot(ds, aes(x=theta1)) + geom_histogram(aes(y=..density..),color=&#39;#1A384A&#39;, fill=&#39;#7A99AC&#39;) + labs(title = expression(theta[1]~Estimate), x=expression(theta[1]), y = &#39;Density&#39;) + geom_vline(xintercept = mean(ds$theta1), color=&#39;#b7091a&#39;) + theme_minimal() Figure 2.10: Bias of Two Coins: Theta 1 ggplot(ds, aes(x=theta2)) + geom_histogram(aes(y=..density..),color=&#39;#1A384A&#39;, fill=&#39;#7A99AC&#39;) + labs(title = expression(theta[2]~Estimate), x=expression(theta[2]), y = &#39;Density&#39;) + geom_vline(xintercept = mean(ds$theta2), color=&#39;#b7091a&#39;) + theme_minimal() Figure 2.11: Bias of Two Coins - Theta 2 In the Figures 2.11 and 2.10 we can see the distribution of samples drawn from the Beta distribution using the prior parameters and the observed data we have. The red line represents the mean of the samples for each coin. Our resulting bias values are: \\(\\theta_{1}\\) = 0.5 \\(\\theta_{2}\\) = 0.21 2.6.6 Change Point Example Figure 2.12: Change Point Example What if the problem we are solving is more complicated? Let’s say I flip a coin repeatedly, but at some point I switch to another coin with a different bias (\\(\\theta\\)) as shown in Figure 2.12. I want to detect the point in time when coin 1 was swapped out for coin 2. We can use Gibbs sampling to solve this problem. In the previous example we only needed to get a posterior of a single variable (technically there were two variables, but both have the same posterior conditional and were independent of one another). In this case we have 3 variables that we need to estimate: Coin bias for coin 1: \\(\\theta_{1}\\) Coin bias for coin 2: \\(\\theta_{2}\\) The point in time, i.e. on which flip, the coin was swapped from coin 1 to coin 2: n We have three variables, but how do we describe the actual process we are modeling? In Equation (2.33) the distrutions for each variable are displayed. The coinflips, x, are drawn from a bernoulli distribution, but are dependent on the coin being flipped and the bias of that coin, \\(\\theta_{i}\\). From the previous examples we know \\(\\theta\\) is modeled using a beta distribution. The change point, n, is the time when coin 2 replaces coin 1. Therefore the possible values of n are between 2, the second sample in discrete time, and N, the final discrete time in our example. All values of n are equally probable and therefore can be modeled as a uniform distribution. \\[ \\begin{equation} \\begin{aligned} x &amp;\\sim \\begin{cases} Bern(x_{i};\\theta_{1}) \\quad 1 \\le i \\le n \\\\ Bern(x_{i};\\theta_{2}) \\quad n \\lt i \\lt N \\end{cases} \\\\ n &amp;\\sim Uniform(2...N) \\\\ \\theta_{i} &amp;\\sim Beta(\\theta_{i}, a,b) \\end{aligned} \\tag{2.33} \\end{equation} \\] 2.6.6.1 Derivation of Joint Distribution Now that we have defined all the unknowns of our model, we derive the full joint distribution. \\[ \\begin{equation} p(\\theta_{1}, \\theta_{2}, n| x_{1:n}) \\propto \\overbrace{p(x_{1:N}|\\theta_{1}) p(x_{n+1:N}|\\theta_{2})}^{Likelihoods} \\overbrace{p(\\theta_{1})p(\\theta_{2})p(n)}^{Priors} \\tag{2.34} \\end{equation} \\] Our goal here is to get one equation to estimate the posterior of each of our variables, aka the posterior conditionals. The easiest way to accomplish this task is to identify the terms of the joint distribution that contain the variable you want the posterior conditional for. Let’s start by breaking (2.34) a bit further. $$ \\[\\begin{equation} \\begin{aligned} p(\\theta_{1}, \\theta_{2}, n| x_{1:n}) &amp;\\propto (\\prod_{1}^{n}p(x_{i}|\\theta_{1})) (\\prod_{n+1}^{N}p(x_{i}|\\theta_{2})) p(\\theta_{1})p(\\theta_{2})p(n)\\\\ &amp;\\propto [\\theta_{1}^{z_{1}}(1-\\theta_{1})^{n-z_{1}}] [\\theta_{2}^{z_{2}}(1-\\theta_{2})^{N-(n+1)-z_{2}}] p(\\theta_{1})p(\\theta_{2})p(n)\\\\ \\\\ &amp;\\propto [\\theta_{1}^{z_{1}}(1-\\theta_{1})^{n-z_{1}}] [\\theta_{2}^{z_{2}}(1-\\theta_{2})^{N-(n+1)-z_{2}}] {{\\theta_{1}^{(a_{1}-1)}(1-\\theta_{1})^{(b_{1}-1)}}\\over \\beta(a_{1},b_{1})} {{\\theta_{2}^{(a_{2}-1)}(1-\\theta_{2})^{(b_{2}-1)}}\\over \\beta(a_{2},b_{2})} {1\\over N}\\\\ \\\\ \\end{aligned} \\tag{2.35} \\end{equation}\\] $$ Then we move on to solving for n’s posterior conditional. In Equation (2.35) we see that only the likelihood terms and the priors for the \\(\\theta\\)’s contain n. Using these terms we can solve for the posterior conditional. While we are at it we will also take the log of the conditional posterior as is good practice to prevent issues such as underflow. \\[ \\begin{equation} \\begin{aligned} p(n| x_{1:n}, \\theta_{1}, \\theta_{2}) &amp;\\propto [\\theta_{1}^{z_{1}}(1-\\theta_{1})^{n-z_{1}}] [\\theta_{2}^{z_{2}}(1-\\theta_{2})^{N-(n+1)-z_{2}}]\\\\ log(p(n| x_{1:n}, \\theta_{1}, \\theta_{2})) &amp;\\propto log([\\theta_{1}^{z_{1}}(1-\\theta_{1})^{n-z_{1}}]) + log([\\theta_{2}^{z_{2}}(1-\\theta_{2})^{N-(n+1)-z_{2}}]) \\end{aligned} \\end{equation} \\] To get the posterior conditionals for the \\(\\theta\\) values we will need to utilize the conjugate prior relationship between the likelihoods and the priors. First we will collapse the priors and likelihoods for the \\(\\theta\\) values. \\[ \\begin{equation} \\begin{aligned} p(\\theta_{1}, \\theta_{2}, n| x_{1:n}) &amp;\\propto [\\theta_{1}^{(z_{1}+a_{1}-1)}(1-\\theta_{1})^{(n-z_{1}+b_{1}-1)}] [\\theta_{2}^{(z_{2}+a_{2}-1)}(1-\\theta_{2})^{(N-n-1-z_{2}+b_{2}-1)}]({1 \\over N})\\\\ &amp;\\propto Beta(a_{1}+z_{1}, n-z_{1}+b_{1}) Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})({1\\over N}) \\end{aligned} \\end{equation} \\] Now we can solve for each of the \\(\\theta\\)’s posterior conditionals. \\[ \\begin{equation} \\begin{aligned} p(\\theta_{1}| x_{1:n},\\theta_{2}, n) &amp;\\propto Beta(a_{1}+z_{1}, n-z_{1}+b_{1})\\\\ log(p(\\theta_{1}| x_{1:n},\\theta_{2}, n)) &amp;\\propto log(Beta(a_{1}+z_{1}, n-z_{1}+b_{1})) \\end{aligned} \\end{equation} \\] \\[ \\begin{equation} \\begin{aligned} p(\\theta_{2}| x_{1:n},\\theta_{1}, n) &amp;\\propto Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})\\\\ log(p(\\theta_{2}| x_{1:n},\\theta_{1}, n)) &amp;\\propto log(Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})) \\end{aligned} \\end{equation} \\] Now let’s put our derived posteriors to work use Gibbs sampling to estimate our change point and coin biases. real_thetas &lt;- c(0.2, 0.6) N &lt;- 300 a = 2 b = 3 change_point &lt;- 100 x &lt;- c(rbinom(1:change_point, 1, real_thetas[1]),rbinom((change_point+1):N, 1, real_thetas[2])) ## Initialize all parameters # n ~ uniform n &lt;- round(N*runif(1)) # theta1 ~ beta(a,b) theta1 &lt;- rbeta(1, a, b) # theta2 ~ beta(a,b) theta2 &lt;- rbeta(1, a, b) niters = 3000 burnin = 1000 params = matrix(0, nrow = (niters-burnin), ncol=3) for (i in 1:niters){ z1 &lt;- sum(x[1:n]) if(n == N){ z2 &lt;- 0 }else{ z2 &lt;- sum(x[(n+1):N]) } theta1 = rbeta(n = 1, shape1 = a + z1, shape2 = b + n - z1) # get value theta2| all other vars theta2 = rbeta(n = 1, shape1 = a + z2, shape2 =N-n-1-z2+b) ## 2 things: 1 - should I be summing all the values over these? # No - the product is being calculated due to the sum - should be fine n_multi &lt;- rep(0, N) for(steps in 2:N){ if(steps==N || theta2 == 1){ n_multi[steps] &lt;- log(theta1^sum(x[1:steps]) * (1-theta1)^(steps-sum(x[1:steps]))) }else{ n_multi[steps] &lt;- log(theta1^sum(x[1:steps]) * (1-theta1)^(steps-sum(x[1:steps]))) + log(theta2^sum(x[(steps + 1):N]) * (1-theta2)^(N-steps-1-sum(x[(steps+1):N]))) } } n_multi &lt;- exp(n_multi[2:N] - max(n_multi[2:N])) # offset by 1 # you n is equally probably between 2 and N and zero at n=1 # we only calculate p(n) from n=2 to N n &lt;- which(rmultinom(1, 1, n_multi/sum(n_multi))[,1] ==1) + 1 if (i &gt;= burnin){ params[(i-burnin), ] = c(theta1,theta2, n) } } ds &lt;- data.frame(x = x, theta = c(rep(real_thetas[1],N-change_point), rep(real_thetas[2],change_point)), sample_index = seq(1:length(x))) params_df &lt;- as.data.frame(params) names(params_df) &lt;- c(&#39;theta1&#39;, &#39;theta2&#39;, &#39;change_point&#39;) ggplot(params_df, aes(x = change_point)) + geom_histogram(fill=&quot;#7A99AC&quot;, color =&#39;#1A384A&#39;, binwidth = 5, bins = floor(N/5)) + theme_minimal() + scale_x_continuous(limits = c(0,N)) + geom_vline(xintercept = mean(params_df$change_point), color=&#39;#b7091a&#39;) + labs(title = &#39;Change Point Estimate&#39;, x=&#39;Change Point&#39;, y = &#39;Density&#39;) Figure 2.13: Estimated Change Point ggplot(params_df, aes(x = theta1)) + geom_histogram(fill=&quot;#7A99AC&quot;, color =&#39;#1A384A&#39; , binwidth = 0.025) + theme_minimal() + scale_x_continuous(limits = c(0,1)) + geom_vline(xintercept = mean(params_df$theta1), color=&#39;#b7091a&#39;) + labs(title = expression(theta[1]~Estimate), x=expression(theta[1]), y = &#39;Density&#39;) Figure 2.14: Estimated Theta 1 ggplot(params_df, aes(x = theta2)) + geom_histogram(fill=&quot;#7A99AC&quot;, color =&#39;#1A384A&#39; , binwidth = 0.025) + theme_minimal() + scale_x_continuous(limits = c(0,1)) + geom_vline(xintercept = mean(params_df$theta2), color=&#39;#b7091a&#39;) + labs(title = expression(theta[2]~Estimate), x=expression(theta[2]), y = &#39;Density&#39;) Figure 2.15: Estimated Theta 2 The resulting estimates for each of our parameters is shown below along with the real value used to generate the dataset. data.frame(theta_1 = c(real_thetas[1], mean(params_df$theta1)), theta_2 = c(real_thetas[2], mean(params_df$theta2)), n = c(change_point, floor(mean(params_df$change_point))), row.names = c(&#39;True&#39;, &#39;Estimated&#39;)) %&gt;% kable(col.names = c(&#39;\\u03b8\\u2081&#39;, &#39;\\u03b8\\u2082&#39;, &#39;n&#39;), digits = 2, row.names = TRUE, caption = &#39;Change Point Parameters and Estimates&#39;) Table 2.1: Change Point Parameters and Estimates θ₁ θ₂ n True 0.20 0.60 100 Estimated 0.25 0.58 100 \\(\\hat{\\theta_{1}}\\) = 0.24, 0.28, 0.21, 0.29, 0.27, 0.27, 0.32, 0.32, 0.3, 0.2, 0.22, 0.27, 0.28, 0.19, 0.24, 0.24, 0.2, 0.26, 0.33, 0.31, 0.25, 0.22, 0.21, 0.27, 0.2, 0.26, 0.3, 0.28, 0.2, 0.27, 0.3, 0.27, 0.29, 0.36, 0.27, 0.27, 0.23, 0.25, 0.23, 0.24, 0.32, 0.25, 0.35, 0.28, 0.22, 0.28, 0.22, 0.19, 0.22, 0.23, 0.24, 0.23, 0.28, 0.2, 0.28, 0.28, 0.26, 0.29, 0.27, 0.29, 0.26, 0.22, 0.28, 0.32, 0.25, 0.28, 0.37, 0.23, 0.27, 0.18, 0.24, 0.29, 0.28, 0.34, 0.26, 0.25, 0.33, 0.32, 0.32, 0.27, 0.27, 0.25, 0.27, 0.25, 0.31, 0.21, 0.24, 0.23, 0.31, 0.27, 0.19, 0.23, 0.35, 0.25, 0.23, 0.22, 0.24, 0.29, 0.26, 0.21, 0.22, 0.33, 0.24, 0.24, 0.25, 0.22, 0.25, 0.24, 0.26, 0.31, 0.25, 0.18, 0.25, 0.29, 0.3, 0.19, 0.24, 0.16, 0.18, 0.29, 0.3, 0.29, 0.25, 0.29, 0.23, 0.26, 0.33, 0.21, 0.27, 0.22, 0.19, 0.25, 0.25, 0.31, 0.23, 0.28, 0.3, 0.22, 0.23, 0.31, 0.23, 0.2, 0.24, 0.26, 0.23, 0.31, 0.28, 0.3, 0.22, 0.21, 0.22, 0.26, 0.22, 0.25, 0.27, 0.23, 0.24, 0.33, 0.23, 0.2, 0.26, 0.25, 0.3, 0.29, 0.23, 0.28, 0.23, 0.26, 0.23, 0.33, 0.22, 0.25, 0.18, 0.3, 0.24, 0.25, 0.21, 0.22, 0.23, 0.26, 0.25, 0.21, 0.26, 0.26, 0.32, 0.2, 0.29, 0.23, 0.28, 0.2, 0.25, 0.25, 0.31, 0.25, 0.28, 0.25, 0.22, 0.25, 0.27, 0.25, 0.31, 0.25, 0.2, 0.26, 0.27, 0.3, 0.19, 0.19, 0.26, 0.2, 0.18, 0.28, 0.3, 0.21, 0.24, 0.21, 0.25, 0.31, 0.27, 0.27, 0.31, 0.25, 0.28, 0.29, 0.24, 0.3, 0.27, 0.22, 0.28, 0.28, 0.27, 0.24, 0.26, 0.17, 0.21, 0.28, 0.29, 0.26, 0.21, 0.29, 0.26, 0.18, 0.24, 0.27, 0.23, 0.25, 0.26, 0.2, 0.31, 0.24, 0.28, 0.21, 0.21, 0.22, 0.21, 0.2, 0.38, 0.25, 0.24, 0.22, 0.27, 0.2, 0.24, 0.27, 0.2, 0.27, 0.26, 0.23, 0.25, 0.22, 0.24, 0.28, 0.25, 0.29, 0.27, 0.24, 0.23, 0.26, 0.31, 0.21, 0.23, 0.22, 0.24, 0.27, 0.22, 0.14, 0.32, 0.28, 0.24, 0.25, 0.18, 0.21, 0.29, 0.28, 0.28, 0.31, 0.25, 0.24, 0.31, 0.23, 0.26, 0.28, 0.25, 0.21, 0.22, 0.22, 0.28, 0.21, 0.24, 0.27, 0.3, 0.35, 0.27, 0.18, 0.32, 0.3, 0.32, 0.22, 0.23, 0.28, 0.25, 0.31, 0.24, 0.28, 0.18, 0.21, 0.23, 0.26, 0.27, 0.21, 0.24, 0.3, 0.24, 0.22, 0.27, 0.22, 0.23, 0.24, 0.2, 0.29, 0.26, 0.33, 0.3, 0.2, 0.27, 0.28, 0.26, 0.22, 0.27, 0.28, 0.34, 0.23, 0.21, 0.35, 0.27, 0.29, 0.25, 0.25, 0.27, 0.24, 0.24, 0.32, 0.28, 0.27, 0.25, 0.27, 0.31, 0.29, 0.21, 0.24, 0.33, 0.27, 0.22, 0.28, 0.23, 0.35, 0.31, 0.35, 0.29, 0.25, 0.29, 0.24, 0.22, 0.23, 0.21, 0.24, 0.32, 0.28, 0.23, 0.24, 0.23, 0.2, 0.29, 0.3, 0.24, 0.33, 0.29, 0.23, 0.23, 0.25, 0.21, 0.27, 0.23, 0.26, 0.29, 0.23, 0.23, 0.23, 0.25, 0.17, 0.24, 0.26, 0.26, 0.26, 0.26, 0.32, 0.26, 0.32, 0.31, 0.21, 0.31, 0.23, 0.21, 0.27, 0.19, 0.28, 0.24, 0.33, 0.2, 0.22, 0.33, 0.31, 0.27, 0.28, 0.22, 0.22, 0.32, 0.33, 0.27, 0.23, 0.29, 0.23, 0.3, 0.18, 0.3, 0.24, 0.25, 0.18, 0.32, 0.26, 0.21, 0.32, 0.22, 0.19, 0.26, 0.27, 0.32, 0.32, 0.22, 0.29, 0.28, 0.21, 0.3, 0.27, 0.27, 0.21, 0.28, 0.22, 0.22, 0.19, 0.14, 0.26, 0.25, 0.19, 0.21, 0.21, 0.3, 0.19, 0.26, 0.25, 0.33, 0.19, 0.17, 0.19, 0.21, 0.19, 0.29, 0.2, 0.3, 0.29, 0.22, 0.19, 0.23, 0.23, 0.24, 0.26, 0.27, 0.3, 0.22, 0.28, 0.24, 0.26, 0.2, 0.19, 0.28, 0.34, 0.25, 0.15, 0.32, 0.31, 0.26, 0.28, 0.28, 0.24, 0.2, 0.23, 0.24, 0.21, 0.25, 0.27, 0.25, 0.25, 0.31, 0.28, 0.2, 0.27, 0.19, 0.23, 0.24, 0.24, 0.24, 0.38, 0.24, 0.35, 0.27, 0.25, 0.23, 0.26, 0.22, 0.21, 0.26, 0.24, 0.25, 0.28, 0.23, 0.26, 0.23, 0.23, 0.2, 0.25, 0.25, 0.27, 0.21, 0.26, 0.32, 0.22, 0.2, 0.23, 0.24, 0.28, 0.22, 0.3, 0.22, 0.24, 0.21, 0.3, 0.22, 0.24, 0.26, 0.27, 0.25, 0.28, 0.31, 0.21, 0.25, 0.21, 0.17, 0.25, 0.28, 0.23, 0.27, 0.3, 0.18, 0.25, 0.32, 0.2, 0.29, 0.28, 0.22, 0.29, 0.35, 0.27, 0.23, 0.28, 0.23, 0.24, 0.24, 0.23, 0.25, 0.29, 0.23, 0.27, 0.34, 0.17, 0.24, 0.24, 0.26, 0.31, 0.21, 0.26, 0.34, 0.23, 0.23, 0.33, 0.27, 0.28, 0.28, 0.31, 0.25, 0.35, 0.25, 0.24, 0.27, 0.28, 0.28, 0.25, 0.21, 0.12, 0.22, 0.2, 0.25, 0.21, 0.25, 0.27, 0.23, 0.24, 0.19, 0.25, 0.24, 0.26, 0.25, 0.23, 0.39, 0.27, 0.26, 0.22, 0.21, 0.16, 0.17, 0.22, 0.26, 0.29, 0.23, 0.25, 0.3, 0.3, 0.25, 0.2, 0.25, 0.23, 0.19, 0.24, 0.13, 0.19, 0.22, 0.21, 0.25, 0.24, 0.22, 0.27, 0.27, 0.21, 0.28, 0.28, 0.23, 0.18, 0.25, 0.23, 0.24, 0.23, 0.23, 0.21, 0.29, 0.27, 0.28, 0.24, 0.31, 0.19, 0.28, 0.19, 0.2, 0.19, 0.27, 0.26, 0.33, 0.3, 0.21, 0.33, 0.25, 0.2, 0.31, 0.25, 0.24, 0.28, 0.25, 0.21, 0.18, 0.27, 0.19, 0.2, 0.28, 0.29, 0.25, 0.28, 0.22, 0.19, 0.21, 0.3, 0.33, 0.29, 0.26, 0.19, 0.25, 0.23, 0.22, 0.26, 0.22, 0.19, 0.26, 0.26, 0.18, 0.18, 0.23, 0.25, 0.2, 0.31, 0.17, 0.23, 0.19, 0.26, 0.34, 0.39, 0.2, 0.32, 0.3, 0.27, 0.27, 0.22, 0.25, 0.23, 0.22, 0.32, 0.25, 0.29, 0.26, 0.21, 0.26, 0.22, 0.2, 0.25, 0.25, 0.33, 0.25, 0.27, 0.2, 0.31, 0.26, 0.19, 0.27, 0.21, 0.22, 0.26, 0.23, 0.31, 0.35, 0.21, 0.25, 0.32, 0.25, 0.27, 0.23, 0.24, 0.25, 0.2, 0.28, 0.23, 0.29, 0.27, 0.18, 0.24, 0.23, 0.29, 0.23, 0.24, 0.24, 0.22, 0.19, 0.31, 0.21, 0.24, 0.29, 0.26, 0.31, 0.28, 0.26, 0.2, 0.29, 0.25, 0.26, 0.38, 0.2, 0.19, 0.32, 0.17, 0.27, 0.25, 0.32, 0.28, 0.25, 0.24, 0.23, 0.22, 0.2, 0.3, 0.28, 0.19, 0.28, 0.19, 0.3, 0.41, 0.25, 0.13, 0.19, 0.25, 0.23, 0.27, 0.29, 0.23, 0.26, 0.2, 0.24, 0.32, 0.27, 0.21, 0.29, 0.17, 0.27, 0.25, 0.3, 0.21, 0.29, 0.34, 0.29, 0.2, 0.25, 0.27, 0.19, 0.23, 0.19, 0.27, 0.26, 0.27, 0.26, 0.23, 0.28, 0.25, 0.27, 0.31, 0.24, 0.28, 0.24, 0.27, 0.22, 0.22, 0.22, 0.22, 0.29, 0.19, 0.23, 0.27, 0.28, 0.33, 0.26, 0.24, 0.23, 0.28, 0.27, 0.19, 0.16, 0.16, 0.27, 0.23, 0.26, 0.28, 0.17, 0.31, 0.25, 0.26, 0.34, 0.25, 0.27, 0.32, 0.29, 0.3, 0.27, 0.18, 0.24, 0.3, 0.27, 0.36, 0.22, 0.22, 0.32, 0.24, 0.2, 0.27, 0.22, 0.26, 0.25, 0.27, 0.2, 0.22, 0.17, 0.13, 0.35, 0.27, 0.21, 0.34, 0.25, 0.18, 0.17, 0.28, 0.19, 0.27, 0.36, 0.27, 0.29, 0.2, 0.25, 0.24, 0.26, 0.24, 0.18, 0.24, 0.25, 0.24, 0.27, 0.19, 0.27, 0.32, 0.22, 0.24, 0.23, 0.26, 0.26, 0.2, 0.24, 0.25, 0.21, 0.2, 0.29, 0.25, 0.24, 0.27, 0.19, 0.21, 0.3, 0.24, 0.18, 0.29, 0.24, 0.25, 0.32, 0.28, 0.22, 0.33, 0.21, 0.23, 0.19, 0.21, 0.29, 0.23, 0.19, 0.23, 0.23, 0.3, 0.26, 0.25, 0.23, 0.2, 0.29, 0.22, 0.22, 0.16, 0.29, 0.22, 0.28, 0.3, 0.29, 0.27, 0.18, 0.31, 0.29, 0.25, 0.24, 0.23, 0.31, 0.25, 0.3, 0.3, 0.23, 0.28, 0.23, 0.23, 0.25, 0.3, 0.27, 0.24, 0.25, 0.24, 0.27, 0.28, 0.21, 0.26, 0.24, 0.32, 0.26, 0.29, 0.32, 0.22, 0.3, 0.27, 0.28, 0.28, 0.26, 0.21, 0.26, 0.27, 0.22, 0.2, 0.25, 0.16, 0.21, 0.25, 0.22, 0.28, 0.25, 0.24, 0.21, 0.21, 0.32, 0.23, 0.2, 0.25, 0.32, 0.24, 0.24, 0.32, 0.26, 0.2, 0.27, 0.26, 0.28, 0.32, 0.21, 0.18, 0.23, 0.17, 0.26, 0.23, 0.27, 0.26, 0.21, 0.24, 0.24, 0.27, 0.27, 0.22, 0.21, 0.28, 0.31, 0.28, 0.19, 0.28, 0.34, 0.27, 0.21, 0.2, 0.28, 0.26, 0.21, 0.23, 0.22, 0.24, 0.19, 0.25, 0.21, 0.23, 0.23, 0.19, 0.23, 0.23, 0.21, 0.18, 0.25, 0.26, 0.27, 0.22, 0.27, 0.23, 0.24, 0.23, 0.25, 0.3, 0.26, 0.23, 0.26, 0.21, 0.23, 0.27, 0.37, 0.31, 0.21, 0.22, 0.21, 0.23, 0.31, 0.2, 0.26, 0.25, 0.28, 0.23, 0.34, 0.22, 0.26, 0.27, 0.23, 0.25, 0.22, 0.36, 0.24, 0.21, 0.3, 0.29, 0.18, 0.23, 0.24, 0.28, 0.31, 0.28, 0.28, 0.23, 0.3, 0.25, 0.26, 0.34, 0.26, 0.27, 0.28, 0.34, 0.2, 0.2, 0.22, 0.28, 0.24, 0.26, 0.17, 0.29, 0.19, 0.28, 0.27, 0.2, 0.22, 0.24, 0.18, 0.2, 0.28, 0.25, 0.24, 0.25, 0.21, 0.25, 0.3, 0.22, 0.24, 0.2, 0.24, 0.22, 0.33, 0.22, 0.24, 0.26, 0.33, 0.24, 0.28, 0.18, 0.22, 0.27, 0.19, 0.26, 0.26, 0.23, 0.22, 0.19, 0.23, 0.22, 0.28, 0.27, 0.29, 0.31, 0.35, 0.24, 0.26, 0.16, 0.18, 0.22, 0.34, 0.32, 0.29, 0.25, 0.17, 0.25, 0.28, 0.2, 0.25, 0.25, 0.22, 0.28, 0.16, 0.28, 0.38, 0.24, 0.25, 0.25, 0.27, 0.26, 0.23, 0.21, 0.27, 0.22, 0.33, 0.33, 0.21, 0.26, 0.27, 0.22, 0.2, 0.18, 0.26, 0.29, 0.21, 0.3, 0.2, 0.15, 0.27, 0.26, 0.22, 0.22, 0.36, 0.25, 0.26, 0.13, 0.26, 0.25, 0.25, 0.27, 0.28, 0.3, 0.26, 0.27, 0.27, 0.26, 0.28, 0.34, 0.21, 0.28, 0.24, 0.26, 0.26, 0.33, 0.32, 0.36, 0.43, 0.16, 0.29, 0.25, 0.3, 0.3, 0.19, 0.24, 0.32, 0.26, 0.16, 0.24, 0.26, 0.26, 0.26, 0.22, 0.25, 0.28, 0.34, 0.28, 0.24, 0.23, 0.27, 0.25, 0.34, 0.28, 0.32, 0.28, 0.22, 0.25, 0.26, 0.24, 0.28, 0.24, 0.23, 0.39, 0.18, 0.27, 0.2, 0.24, 0.17, 0.19, 0.23, 0.25, 0.27, 0.29, 0.22, 0.22, 0.24, 0.29, 0.18, 0.26, 0.3, 0.25, 0.3, 0.22, 0.28, 0.27, 0.23, 0.2, 0.22, 0.32, 0.32, 0.26, 0.32, 0.28, 0.19, 0.24, 0.25, 0.19, 0.24, 0.24, 0.21, 0.35, 0.23, 0.16, 0.21, 0.32, 0.3, 0.24, 0.22, 0.3, 0.28, 0.27, 0.24, 0.22, 0.29, 0.27, 0.24, 0.34, 0.24, 0.24, 0.22, 0.26, 0.22, 0.25, 0.28, 0.21, 0.17, 0.25, 0.28, 0.32, 0.26, 0.24, 0.19, 0.29, 0.32, 0.25, 0.22, 0.19, 0.25, 0.26, 0.25, 0.19, 0.17, 0.31, 0.27, 0.27, 0.26, 0.24, 0.32, 0.23, 0.21, 0.29, 0.23, 0.18, 0.23, 0.22, 0.3, 0.17, 0.27, 0.26, 0.24, 0.21, 0.27, 0.18, 0.21, 0.32, 0.27, 0.25, 0.3, 0.17, 0.22, 0.21, 0.21, 0.26, 0.2, 0.24, 0.31, 0.31, 0.28, 0.2, 0.26, 0.27, 0.27, 0.31, 0.27, 0.23, 0.25, 0.21, 0.24, 0.23, 0.27, 0.21, 0.23, 0.19, 0.26, 0.19, 0.19, 0.26, 0.21, 0.27, 0.26, 0.22, 0.24, 0.27, 0.18, 0.27, 0.26, 0.28, 0.29, 0.22, 0.31, 0.19, 0.18, 0.23, 0.29, 0.23, 0.26, 0.36, 0.22, 0.29, 0.26, 0.17, 0.26, 0.35, 0.25, 0.21, 0.26, 0.35, 0.29, 0.2, 0.26, 0.27, 0.27, 0.21, 0.25, 0.19, 0.29, 0.26, 0.3, 0.24, 0.2, 0.2, 0.22, 0.24, 0.21, 0.27, 0.21, 0.18, 0.18, 0.29, 0.24, 0.27, 0.22, 0.28, 0.24, 0.3, 0.24, 0.25, 0.28, 0.32, 0.26, 0.25, 0.25, 0.19, 0.2, 0.24, 0.23, 0.26, 0.26, 0.25, 0.2, 0.29, 0.33, 0.29, 0.18, 0.26, 0.25, 0.18, 0.27, 0.3, 0.17, 0.25, 0.22, 0.28, 0.31, 0.26, 0.23, 0.22, 0.29, 0.32, 0.29, 0.3, 0.29, 0.24, 0.25, 0.25, 0.28, 0.25, 0.27, 0.27, 0.25, 0.24, 0.25, 0.21, 0.22, 0.35, 0.27, 0.3, 0.25, 0.24, 0.25, 0.39, 0.23, 0.23, 0.21, 0.34, 0.32, 0.25, 0.29, 0.18, 0.24, 0.29, 0.28, 0.23, 0.29, 0.17, 0.34, 0.29, 0.23, 0.26, 0.18, 0.24, 0.26, 0.28, 0.19, 0.26, 0.23, 0.28, 0.25, 0.22, 0.22, 0.2, 0.24, 0.21, 0.23, 0.24, 0.18, 0.21, 0.27, 0.22, 0.26, 0.31, 0.3, 0.27, 0.24, 0.29, 0.25, 0.18, 0.19, 0.18, 0.26, 0.2, 0.2, 0.27, 0.27, 0.23, 0.29, 0.28, 0.32, 0.23, 0.22, 0.22, 0.22, 0.2, 0.24, 0.27, 0.22, 0.28, 0.24, 0.28, 0.27, 0.3, 0.25, 0.25, 0.3, 0.27, 0.25, 0.26, 0.25, 0.26, 0.23, 0.21, 0.21, 0.25, 0.25, 0.28, 0.22, 0.21, 0.31, 0.34, 0.3, 0.3, 0.25, 0.28, 0.31, 0.25, 0.24, 0.28, 0.26, 0.27, 0.27, 0.27, 0.24, 0.16, 0.28, 0.25, 0.21, 0.28, 0.28, 0.31, 0.21, 0.24, 0.23, 0.27, 0.3, 0.22, 0.26, 0.3, 0.28, 0.31, 0.23, 0.26, 0.26, 0.33, 0.28, 0.22, 0.24, 0.26, 0.39, 0.25, 0.24, 0.21, 0.25, 0.25, 0.34, 0.28, 0.29, 0.18, 0.15, 0.24, 0.25, 0.21, 0.31, 0.24, 0.22, 0.25, 0.23, 0.26, 0.24, 0.22, 0.21, 0.3, 0.32, 0.17, 0.23, 0.29, 0.31, 0.21, 0.22, 0.26, 0.21, 0.28, 0.21, 0.26, 0.2, 0.29, 0.23, 0.27, 0.32, 0.26, 0.19, 0.3, 0.21, 0.21, 0.24, 0.21, 0.28, 0.24, 0.26, 0.19, 0.24, 0.26, 0.25, 0.19, 0.25, 0.21, 0.23, 0.25, 0.18, 0.25, 0.21, 0.23, 0.24, 0.27, 0.23, 0.25, 0.24, 0.29, 0.28, 0.23, 0.27, 0.31, 0.21, 0.23, 0.3, 0.19, 0.2, 0.24, 0.24, 0.33, 0.29, 0.27, 0.25, 0.19, 0.3, 0.32, 0.27, 0.26, 0.34, 0.33, 0.21, 0.32, 0.29, 0.27, 0.23, 0.22, 0.24, 0.26, 0.28, 0.32, 0.26, 0.18, 0.23, 0.2, 0.3, 0.31, 0.31, 0.25, 0.27, 0.31, 0.21, 0.17, 0.19, 0.32, 0.18, 0.23, 0.26, 0.29, 0.25, 0.24, 0.36, 0.24, 0.3, 0.25, 0.3, 0.38, 0.27, 0.26, 0.25, 0.21, 0.25, 0.35, 0.27, 0.23, 0.21, 0.2, 0.21, 0.25, 0.23, 0.32, 0.23, 0.2, 0.29, 0.21, 0.23, 0.18, 0.29, 0.24, 0.26, 0.22, 0.31, 0.25, 0.26, 0.26, 0.23, 0.25, 0.21, 0.15, 0.24, 0.21, 0.29, 0.22, 0.17, 0.36, 0.21, 0.25, 0.19, 0.21, 0.18, 0.24, 0.19, 0.25, 0.25, 0.25, 0.21, 0.29, 0.2, 0.27, 0.22, 0.22, 0.35, 0.29, 0.29, 0.22, 0.24, 0.26, 0.26, 0.23, 0.22, 0.23, 0.28, 0.19, 0.24, 0.24, 0.28, 0.26, 0.22, 0.21, 0.27, 0.29, 0.3, 0.3, 0.25, 0.36, 0.17, 0.23, 0.26, 0.31, 0.25, 0.23, 0.25, 0.27, 0.26, 0.23, 0.16, 0.22, 0.23, 0.32, 0.24, 0.22, 0.23, 0.18, 0.26, 0.24, 0.29, 0.25, 0.25, 0.22, 0.29, 0.22, 0.29, 0.23, 0.2, 0.23, 0.2, 0.3, 0.35, 0.19, 0.25, 0.26, 0.3, 0.22, 0.27, 0.26, 0.26, 0.27, 0.29, 0.18, 0.26, 0.17, 0.31, 0.3, 0.22, 0.22, 0.27, 0.21, 0.23, 0.23, 0.31, 0.24, 0.26, 0.24, 0.27, 0.26, 0.23, 0.31, 0.26, 0.23, 0.25, 0.18, 0.23, 0.28, 0.3, 0.35, 0.19, 0.17, 0.26, 0.19, 0.32, 0.25, 0.32, 0.27, 0.23, 0.25, 0.2, 0.23, 0.25, 0.16, 0.28, 0.28, 0.34, 0.21, 0.24, 0.29, 0.2, 0.23, 0.2, 0.22, 0.32, 0.31, 0.26, 0.26, 0.25, 0.19, 0.31, 0.21 \\(\\hat{\\theta_{2}}\\) = 0.55, 0.54, 0.6, 0.63, 0.54, 0.59, 0.58, 0.62, 0.59, 0.6, 0.58, 0.61, 0.57, 0.59, 0.51, 0.55, 0.62, 0.56, 0.61, 0.54, 0.6, 0.54, 0.57, 0.55, 0.55, 0.66, 0.62, 0.57, 0.6, 0.6, 0.55, 0.58, 0.66, 0.59, 0.56, 0.6, 0.6, 0.58, 0.56, 0.56, 0.54, 0.56, 0.55, 0.6, 0.61, 0.63, 0.58, 0.59, 0.6, 0.59, 0.58, 0.61, 0.53, 0.6, 0.59, 0.6, 0.6, 0.63, 0.63, 0.54, 0.56, 0.54, 0.57, 0.56, 0.59, 0.54, 0.57, 0.62, 0.57, 0.59, 0.61, 0.59, 0.49, 0.62, 0.61, 0.5, 0.5, 0.57, 0.53, 0.61, 0.56, 0.6, 0.56, 0.64, 0.56, 0.61, 0.58, 0.57, 0.6, 0.61, 0.64, 0.57, 0.52, 0.57, 0.6, 0.67, 0.58, 0.55, 0.61, 0.58, 0.61, 0.58, 0.55, 0.59, 0.63, 0.62, 0.61, 0.67, 0.56, 0.54, 0.57, 0.59, 0.56, 0.6, 0.63, 0.62, 0.57, 0.63, 0.55, 0.54, 0.62, 0.61, 0.56, 0.59, 0.55, 0.57, 0.58, 0.66, 0.67, 0.57, 0.57, 0.56, 0.6, 0.65, 0.59, 0.59, 0.59, 0.59, 0.53, 0.6, 0.54, 0.59, 0.57, 0.6, 0.63, 0.68, 0.58, 0.54, 0.53, 0.6, 0.65, 0.57, 0.52, 0.59, 0.56, 0.66, 0.5, 0.57, 0.62, 0.61, 0.64, 0.6, 0.63, 0.63, 0.63, 0.59, 0.56, 0.56, 0.55, 0.61, 0.58, 0.63, 0.53, 0.61, 0.62, 0.57, 0.64, 0.59, 0.57, 0.58, 0.58, 0.53, 0.57, 0.64, 0.58, 0.57, 0.61, 0.62, 0.61, 0.58, 0.6, 0.55, 0.54, 0.62, 0.58, 0.55, 0.6, 0.52, 0.62, 0.56, 0.54, 0.59, 0.67, 0.59, 0.65, 0.63, 0.61, 0.56, 0.55, 0.58, 0.56, 0.54, 0.56, 0.58, 0.59, 0.54, 0.6, 0.49, 0.68, 0.51, 0.58, 0.6, 0.59, 0.58, 0.58, 0.63, 0.57, 0.56, 0.51, 0.58, 0.56, 0.62, 0.51, 0.61, 0.55, 0.53, 0.53, 0.56, 0.57, 0.63, 0.6, 0.51, 0.55, 0.64, 0.63, 0.52, 0.61, 0.56, 0.6, 0.63, 0.58, 0.57, 0.59, 0.58, 0.55, 0.55, 0.6, 0.49, 0.51, 0.6, 0.64, 0.62, 0.61, 0.57, 0.54, 0.6, 0.64, 0.59, 0.6, 0.52, 0.58, 0.58, 0.6, 0.59, 0.62, 0.63, 0.6, 0.64, 0.65, 0.58, 0.55, 0.6, 0.63, 0.56, 0.57, 0.61, 0.57, 0.52, 0.61, 0.59, 0.56, 0.56, 0.63, 0.55, 0.62, 0.56, 0.59, 0.61, 0.54, 0.54, 0.56, 0.57, 0.59, 0.6, 0.54, 0.58, 0.63, 0.58, 0.55, 0.58, 0.59, 0.51, 0.57, 0.55, 0.54, 0.54, 0.68, 0.62, 0.6, 0.61, 0.64, 0.56, 0.54, 0.55, 0.55, 0.62, 0.54, 0.59, 0.59, 0.59, 0.62, 0.6, 0.65, 0.57, 0.64, 0.57, 0.57, 0.64, 0.55, 0.54, 0.61, 0.53, 0.54, 0.56, 0.55, 0.57, 0.62, 0.58, 0.59, 0.62, 0.65, 0.58, 0.59, 0.67, 0.56, 0.56, 0.59, 0.56, 0.62, 0.61, 0.55, 0.55, 0.61, 0.56, 0.62, 0.54, 0.54, 0.6, 0.65, 0.57, 0.54, 0.55, 0.61, 0.6, 0.6, 0.6, 0.61, 0.63, 0.53, 0.6, 0.61, 0.6, 0.59, 0.53, 0.61, 0.63, 0.58, 0.59, 0.62, 0.59, 0.56, 0.62, 0.63, 0.62, 0.59, 0.65, 0.62, 0.58, 0.59, 0.64, 0.6, 0.64, 0.55, 0.58, 0.62, 0.63, 0.54, 0.64, 0.61, 0.64, 0.59, 0.52, 0.57, 0.58, 0.59, 0.61, 0.61, 0.57, 0.56, 0.61, 0.59, 0.64, 0.58, 0.58, 0.6, 0.51, 0.55, 0.6, 0.64, 0.55, 0.6, 0.56, 0.63, 0.62, 0.6, 0.57, 0.61, 0.61, 0.53, 0.59, 0.57, 0.53, 0.6, 0.62, 0.57, 0.53, 0.59, 0.6, 0.58, 0.57, 0.56, 0.54, 0.6, 0.58, 0.61, 0.58, 0.62, 0.58, 0.61, 0.61, 0.57, 0.63, 0.57, 0.52, 0.58, 0.6, 0.56, 0.55, 0.6, 0.6, 0.54, 0.54, 0.59, 0.58, 0.54, 0.57, 0.62, 0.6, 0.56, 0.56, 0.65, 0.59, 0.6, 0.58, 0.62, 0.55, 0.61, 0.52, 0.59, 0.64, 0.59, 0.58, 0.63, 0.59, 0.58, 0.64, 0.67, 0.66, 0.62, 0.56, 0.6, 0.53, 0.57, 0.65, 0.59, 0.53, 0.55, 0.63, 0.6, 0.57, 0.56, 0.56, 0.58, 0.58, 0.57, 0.6, 0.6, 0.58, 0.58, 0.57, 0.6, 0.59, 0.57, 0.59, 0.56, 0.54, 0.55, 0.6, 0.59, 0.55, 0.6, 0.57, 0.61, 0.6, 0.58, 0.54, 0.58, 0.64, 0.55, 0.62, 0.63, 0.6, 0.55, 0.53, 0.6, 0.53, 0.57, 0.6, 0.61, 0.61, 0.61, 0.6, 0.63, 0.51, 0.53, 0.57, 0.55, 0.6, 0.61, 0.66, 0.62, 0.61, 0.56, 0.54, 0.63, 0.58, 0.59, 0.53, 0.58, 0.63, 0.58, 0.48, 0.58, 0.62, 0.55, 0.51, 0.61, 0.58, 0.58, 0.61, 0.57, 0.57, 0.55, 0.57, 0.52, 0.51, 0.53, 0.53, 0.61, 0.63, 0.66, 0.55, 0.6, 0.6, 0.57, 0.55, 0.53, 0.58, 0.56, 0.59, 0.6, 0.58, 0.54, 0.54, 0.62, 0.56, 0.62, 0.55, 0.6, 0.55, 0.53, 0.66, 0.58, 0.59, 0.65, 0.57, 0.6, 0.58, 0.6, 0.56, 0.46, 0.58, 0.57, 0.55, 0.56, 0.6, 0.64, 0.55, 0.56, 0.61, 0.63, 0.62, 0.56, 0.55, 0.54, 0.58, 0.56, 0.52, 0.59, 0.6, 0.57, 0.6, 0.54, 0.56, 0.55, 0.55, 0.56, 0.54, 0.54, 0.6, 0.61, 0.61, 0.55, 0.56, 0.62, 0.58, 0.62, 0.61, 0.55, 0.65, 0.57, 0.52, 0.56, 0.6, 0.57, 0.57, 0.6, 0.61, 0.64, 0.55, 0.62, 0.59, 0.59, 0.58, 0.59, 0.59, 0.61, 0.54, 0.6, 0.64, 0.58, 0.52, 0.56, 0.51, 0.56, 0.56, 0.56, 0.58, 0.54, 0.54, 0.54, 0.54, 0.56, 0.55, 0.53, 0.55, 0.61, 0.62, 0.61, 0.65, 0.63, 0.58, 0.54, 0.56, 0.65, 0.53, 0.56, 0.57, 0.57, 0.61, 0.55, 0.56, 0.52, 0.55, 0.6, 0.59, 0.59, 0.52, 0.58, 0.57, 0.64, 0.63, 0.56, 0.53, 0.54, 0.62, 0.62, 0.64, 0.64, 0.62, 0.58, 0.53, 0.62, 0.57, 0.58, 0.6, 0.57, 0.6, 0.65, 0.56, 0.62, 0.59, 0.54, 0.55, 0.6, 0.61, 0.6, 0.62, 0.54, 0.57, 0.61, 0.59, 0.61, 0.6, 0.6, 0.66, 0.55, 0.59, 0.54, 0.62, 0.52, 0.56, 0.54, 0.57, 0.57, 0.59, 0.59, 0.56, 0.58, 0.6, 0.59, 0.57, 0.58, 0.55, 0.62, 0.55, 0.58, 0.65, 0.58, 0.56, 0.59, 0.6, 0.58, 0.58, 0.62, 0.57, 0.55, 0.55, 0.57, 0.6, 0.52, 0.65, 0.57, 0.54, 0.63, 0.56, 0.6, 0.63, 0.55, 0.61, 0.54, 0.61, 0.59, 0.64, 0.54, 0.55, 0.58, 0.58, 0.55, 0.63, 0.58, 0.59, 0.63, 0.57, 0.59, 0.53, 0.62, 0.62, 0.54, 0.62, 0.6, 0.62, 0.6, 0.64, 0.6, 0.55, 0.57, 0.58, 0.54, 0.57, 0.58, 0.57, 0.57, 0.57, 0.61, 0.6, 0.59, 0.64, 0.53, 0.56, 0.62, 0.61, 0.59, 0.58, 0.54, 0.57, 0.62, 0.6, 0.53, 0.58, 0.62, 0.56, 0.54, 0.56, 0.62, 0.55, 0.61, 0.58, 0.65, 0.6, 0.59, 0.58, 0.63, 0.59, 0.6, 0.58, 0.59, 0.58, 0.54, 0.61, 0.59, 0.6, 0.57, 0.57, 0.65, 0.59, 0.56, 0.64, 0.57, 0.57, 0.56, 0.61, 0.56, 0.63, 0.63, 0.6, 0.6, 0.52, 0.5, 0.5, 0.57, 0.6, 0.61, 0.57, 0.54, 0.52, 0.53, 0.57, 0.62, 0.56, 0.58, 0.62, 0.56, 0.64, 0.58, 0.55, 0.54, 0.58, 0.58, 0.64, 0.64, 0.62, 0.58, 0.57, 0.56, 0.56, 0.58, 0.65, 0.65, 0.64, 0.67, 0.55, 0.52, 0.53, 0.6, 0.6, 0.54, 0.57, 0.58, 0.62, 0.59, 0.54, 0.59, 0.62, 0.57, 0.58, 0.56, 0.59, 0.58, 0.52, 0.57, 0.61, 0.56, 0.59, 0.57, 0.58, 0.62, 0.58, 0.6, 0.6, 0.63, 0.61, 0.59, 0.57, 0.57, 0.62, 0.63, 0.63, 0.54, 0.57, 0.56, 0.59, 0.59, 0.56, 0.63, 0.56, 0.62, 0.63, 0.57, 0.55, 0.56, 0.56, 0.59, 0.58, 0.6, 0.62, 0.57, 0.63, 0.6, 0.6, 0.58, 0.56, 0.55, 0.65, 0.62, 0.62, 0.62, 0.61, 0.6, 0.61, 0.58, 0.55, 0.56, 0.6, 0.54, 0.57, 0.55, 0.5, 0.56, 0.55, 0.58, 0.55, 0.61, 0.62, 0.58, 0.54, 0.6, 0.56, 0.53, 0.59, 0.59, 0.56, 0.6, 0.56, 0.56, 0.52, 0.61, 0.61, 0.55, 0.62, 0.62, 0.68, 0.6, 0.63, 0.61, 0.59, 0.54, 0.58, 0.59, 0.54, 0.55, 0.63, 0.61, 0.48, 0.58, 0.54, 0.52, 0.63, 0.6, 0.61, 0.6, 0.6, 0.62, 0.6, 0.57, 0.56, 0.58, 0.58, 0.62, 0.58, 0.59, 0.58, 0.59, 0.64, 0.52, 0.55, 0.6, 0.56, 0.59, 0.61, 0.59, 0.6, 0.63, 0.57, 0.58, 0.6, 0.6, 0.58, 0.57, 0.63, 0.61, 0.52, 0.57, 0.6, 0.55, 0.63, 0.58, 0.54, 0.6, 0.58, 0.58, 0.59, 0.51, 0.55, 0.59, 0.55, 0.56, 0.57, 0.6, 0.57, 0.59, 0.57, 0.56, 0.59, 0.62, 0.6, 0.56, 0.64, 0.56, 0.57, 0.6, 0.61, 0.58, 0.6, 0.55, 0.55, 0.59, 0.66, 0.55, 0.65, 0.58, 0.51, 0.58, 0.56, 0.62, 0.56, 0.61, 0.57, 0.62, 0.58, 0.65, 0.55, 0.66, 0.49, 0.58, 0.58, 0.58, 0.61, 0.68, 0.6, 0.58, 0.56, 0.6, 0.61, 0.59, 0.52, 0.54, 0.6, 0.6, 0.56, 0.55, 0.53, 0.57, 0.55, 0.62, 0.56, 0.55, 0.58, 0.59, 0.56, 0.59, 0.54, 0.62, 0.51, 0.54, 0.62, 0.59, 0.56, 0.57, 0.55, 0.57, 0.61, 0.57, 0.57, 0.53, 0.51, 0.59, 0.59, 0.67, 0.6, 0.63, 0.62, 0.59, 0.61, 0.57, 0.53, 0.56, 0.64, 0.6, 0.63, 0.52, 0.56, 0.49, 0.55, 0.58, 0.55, 0.57, 0.6, 0.53, 0.58, 0.62, 0.58, 0.56, 0.56, 0.61, 0.58, 0.62, 0.59, 0.59, 0.59, 0.63, 0.55, 0.64, 0.57, 0.52, 0.57, 0.53, 0.64, 0.6, 0.59, 0.56, 0.58, 0.58, 0.59, 0.61, 0.63, 0.56, 0.55, 0.58, 0.57, 0.59, 0.59, 0.48, 0.61, 0.59, 0.62, 0.57, 0.63, 0.55, 0.56, 0.6, 0.64, 0.58, 0.53, 0.62, 0.57, 0.52, 0.6, 0.59, 0.65, 0.55, 0.58, 0.56, 0.52, 0.58, 0.58, 0.61, 0.63, 0.53, 0.53, 0.64, 0.57, 0.61, 0.62, 0.62, 0.56, 0.57, 0.57, 0.58, 0.56, 0.57, 0.58, 0.56, 0.63, 0.56, 0.56, 0.61, 0.58, 0.59, 0.56, 0.6, 0.6, 0.61, 0.6, 0.61, 0.67, 0.54, 0.62, 0.53, 0.62, 0.54, 0.56, 0.54, 0.56, 0.64, 0.55, 0.62, 0.56, 0.61, 0.56, 0.53, 0.56, 0.6, 0.6, 0.56, 0.65, 0.56, 0.58, 0.54, 0.59, 0.57, 0.65, 0.53, 0.57, 0.58, 0.6, 0.59, 0.6, 0.57, 0.6, 0.61, 0.65, 0.6, 0.52, 0.57, 0.6, 0.56, 0.56, 0.6, 0.64, 0.53, 0.63, 0.63, 0.56, 0.56, 0.52, 0.56, 0.59, 0.56, 0.59, 0.59, 0.53, 0.54, 0.55, 0.52, 0.63, 0.67, 0.6, 0.56, 0.6, 0.56, 0.6, 0.58, 0.55, 0.62, 0.55, 0.56, 0.63, 0.58, 0.63, 0.62, 0.57, 0.54, 0.66, 0.62, 0.58, 0.61, 0.58, 0.61, 0.56, 0.55, 0.55, 0.53, 0.59, 0.55, 0.58, 0.54, 0.6, 0.56, 0.6, 0.64, 0.55, 0.58, 0.62, 0.58, 0.58, 0.6, 0.63, 0.6, 0.61, 0.62, 0.53, 0.53, 0.63, 0.62, 0.63, 0.59, 0.56, 0.6, 0.58, 0.58, 0.59, 0.59, 0.56, 0.54, 0.56, 0.55, 0.6, 0.58, 0.51, 0.57, 0.62, 0.6, 0.54, 0.58, 0.59, 0.59, 0.52, 0.59, 0.59, 0.58, 0.55, 0.57, 0.6, 0.6, 0.59, 0.55, 0.55, 0.58, 0.6, 0.55, 0.59, 0.52, 0.6, 0.59, 0.54, 0.51, 0.65, 0.54, 0.54, 0.58, 0.65, 0.6, 0.6, 0.63, 0.56, 0.57, 0.56, 0.57, 0.59, 0.6, 0.6, 0.57, 0.59, 0.56, 0.63, 0.6, 0.6, 0.6, 0.61, 0.57, 0.54, 0.58, 0.67, 0.57, 0.64, 0.55, 0.6, 0.59, 0.59, 0.56, 0.58, 0.54, 0.59, 0.57, 0.58, 0.65, 0.63, 0.52, 0.58, 0.51, 0.56, 0.63, 0.58, 0.66, 0.52, 0.61, 0.59, 0.56, 0.53, 0.55, 0.55, 0.59, 0.58, 0.57, 0.56, 0.56, 0.63, 0.56, 0.55, 0.55, 0.51, 0.6, 0.6, 0.55, 0.59, 0.6, 0.54, 0.52, 0.65, 0.52, 0.62, 0.61, 0.6, 0.55, 0.54, 0.62, 0.55, 0.54, 0.65, 0.59, 0.56, 0.58, 0.6, 0.63, 0.54, 0.59, 0.65, 0.62, 0.56, 0.59, 0.59, 0.63, 0.63, 0.61, 0.61, 0.54, 0.6, 0.55, 0.56, 0.55, 0.57, 0.63, 0.6, 0.63, 0.59, 0.65, 0.54, 0.56, 0.62, 0.59, 0.65, 0.61, 0.56, 0.54, 0.6, 0.62, 0.56, 0.57, 0.66, 0.55, 0.62, 0.57, 0.61, 0.69, 0.59, 0.52, 0.59, 0.57, 0.57, 0.59, 0.53, 0.6, 0.63, 0.59, 0.59, 0.58, 0.56, 0.64, 0.61, 0.63, 0.62, 0.58, 0.58, 0.63, 0.6, 0.59, 0.53, 0.51, 0.55, 0.55, 0.55, 0.59, 0.64, 0.57, 0.6, 0.59, 0.61, 0.59, 0.5, 0.52, 0.6, 0.55, 0.57, 0.57, 0.59, 0.61, 0.57, 0.62, 0.58, 0.59, 0.59, 0.55, 0.61, 0.56, 0.59, 0.58, 0.56, 0.61, 0.6, 0.59, 0.62, 0.53, 0.64, 0.55, 0.56, 0.58, 0.6, 0.59, 0.61, 0.56, 0.6, 0.58, 0.53, 0.57, 0.57, 0.58, 0.54, 0.6, 0.58, 0.55, 0.54, 0.53, 0.54, 0.57, 0.6, 0.54, 0.6, 0.63, 0.59, 0.51, 0.54, 0.58, 0.62, 0.62, 0.62, 0.57, 0.56, 0.58, 0.61, 0.63, 0.54, 0.54, 0.59, 0.59, 0.59, 0.53, 0.58, 0.56, 0.6, 0.61, 0.58, 0.51, 0.55, 0.65, 0.61, 0.58, 0.61, 0.56, 0.54, 0.55, 0.65, 0.61, 0.61, 0.6, 0.63, 0.61, 0.54, 0.6, 0.64, 0.62, 0.64, 0.55, 0.56, 0.52, 0.59, 0.61, 0.57, 0.61, 0.58, 0.59, 0.59, 0.53, 0.55, 0.58, 0.57, 0.59, 0.56, 0.58, 0.6, 0.61, 0.58, 0.59, 0.61, 0.62, 0.55, 0.48, 0.59, 0.59, 0.58, 0.57, 0.58, 0.64, 0.53, 0.6, 0.54, 0.57, 0.61, 0.61, 0.57, 0.62, 0.64, 0.58, 0.56, 0.51, 0.62, 0.57, 0.57, 0.58, 0.58, 0.63, 0.6, 0.58, 0.68, 0.59, 0.56, 0.61, 0.55, 0.57, 0.64, 0.55, 0.64, 0.56, 0.58, 0.63, 0.65, 0.54, 0.62, 0.6, 0.56, 0.57, 0.55, 0.52, 0.58, 0.58, 0.6, 0.61, 0.56, 0.52, 0.61, 0.54, 0.57, 0.58, 0.56, 0.6, 0.6, 0.57, 0.58, 0.64, 0.55, 0.59, 0.58, 0.58, 0.61, 0.54, 0.66, 0.63, 0.58, 0.51, 0.55, 0.53, 0.55, 0.58, 0.57, 0.59, 0.6, 0.58, 0.59, 0.59, 0.57, 0.57, 0.58, 0.58, 0.61, 0.58, 0.59, 0.57, 0.57, 0.55, 0.6, 0.6, 0.6, 0.6, 0.62, 0.56, 0.62, 0.66, 0.6, 0.59, 0.54, 0.6, 0.63, 0.57, 0.58, 0.56, 0.58, 0.6, 0.53, 0.61, 0.59, 0.56, 0.58, 0.6, 0.6, 0.57, 0.59, 0.63, 0.64, 0.57, 0.63, 0.57, 0.54, 0.57, 0.6, 0.56, 0.61, 0.62, 0.62, 0.58, 0.6, 0.59, 0.58, 0.57, 0.64, 0.62, 0.53, 0.57, 0.53, 0.56, 0.62, 0.54, 0.6, 0.66, 0.59, 0.58, 0.59, 0.63, 0.62, 0.57, 0.55, 0.54, 0.59, 0.59, 0.53, 0.52, 0.65, 0.63, 0.55, 0.55, 0.56, 0.6, 0.63, 0.53, 0.59, 0.59, 0.54, 0.6, 0.63, 0.62, 0.57, 0.64, 0.58, 0.53, 0.59, 0.61, 0.56, 0.61, 0.53, 0.59, 0.57, 0.62, 0.6, 0.65, 0.58, 0.53, 0.59, 0.49, 0.58, 0.6, 0.6, 0.6, 0.55, 0.58, 0.57, 0.61, 0.62, 0.6, 0.53, 0.58, 0.56, 0.62, 0.6, 0.52, 0.57, 0.61, 0.5, 0.62, 0.56, 0.59, 0.62, 0.63, 0.57, 0.57, 0.57, 0.62, 0.59, 0.62, 0.63, 0.61, 0.54, 0.55, 0.58, 0.58, 0.51, 0.56, 0.53, 0.59, 0.6, 0.55, 0.6, 0.53, 0.59, 0.54, 0.56, 0.58, 0.58, 0.57, 0.5, 0.6, 0.6, 0.58, 0.55, 0.7, 0.54, 0.6, 0.59, 0.6, 0.48, 0.62, 0.56, 0.49, 0.57, 0.54, 0.58, 0.6, 0.56, 0.57, 0.57, 0.51, 0.57, 0.55, 0.6, 0.6, 0.55, 0.59, 0.58, 0.58, 0.64, 0.62, 0.62, 0.58, 0.55, 0.58, 0.63, 0.57, 0.56, 0.6, 0.6, 0.56, 0.62, 0.56, 0.55, 0.6, 0.6, 0.62, 0.53, 0.66, 0.62, 0.56, 0.56 n = 100.48 "],
["multinomial-distribution.html", "3 Multinomial Distribution 3.1 Comparison of Dice vs. Words 3.2 How Multinomial and Bernoulli Relate 3.3 Conjugate Prior: Dirichlet 3.4 Gibbs Sampling - Multinomial &amp; Dirichlet", " 3 Multinomial Distribution This chapter focuses on the multinomial distribution which we will use to model the probability of words in a document. Similar to the previous chapter, we will cover the conjugate prior for the multinomial distributoin, the Dirichlet distribution. 3.1 Comparison of Dice vs. Words The premise of Chapter 2 was to lay a foundation for understanding the multinomial distribution. We are now familiar with the binomial distribution which is actually a special case of the multinomial distribution where the number of possible outcomes is 2. A multinomial distribution can have 2 or more outcomes and therefore is normally shown through examples using a 6-sided die. Instead of using a die with numbers on each side, let’s label the sides with the following words: “Latent” “Dirichlet” “Allocation” “has” “many” “peices” Figure 3.1: Dice for Word Selection In the example above, we assume it is a fair die which would result in equal probabilities of ‘rolling’ any of the 6 unique words. Therefore each word has a probability of being randomly sampled from the die of 1/6. Below is an empirical example where we take each word and assign it to a single side of a fair die. The experiment, a single roll of the die, is repeated 10,000 times. We can see each word comes up at roughly the same frequency. # draw 1000 samples from multinomial distribution outcomes &lt;- replicate(10000, which(rmultinom(1,1,rep(1/6,6))==1)) words &lt;- unlist(strsplit(&quot;Latent Dirichlet Allocation has many peices&quot;, &#39; &#39;)) ds &lt;- data.frame(id = outcomes, word = sapply(outcomes, function(x)words[x])) ggplot(ds, aes(x= word)) + geom_histogram(stat=&#39;count&#39;,color=&#39;#1A384A&#39;, fill=&#39;#7A99AC&#39;) + scale_x_discrete(labels = words) + theme_minimal() Figure 3.2: Sampling from Multinomial with Equal Parameters Quick Note: Let’s think of how this will be used in the case of LDA. If I wanted to generate a document based on a model, I could use a multinomial distribution to determine what words would be in the document. If I knew the probability of a word I could use the example above to draw a new word with each sample. Obviously some words occur much more often than others, so the ‘fair die’ example wouldn’t work for generation of document. In later sections we will build on this concept, but its good to start thinking about how this extends to applications in language. 3.2 How Multinomial and Bernoulli Relate The probability mass function for the multinomial distribution is shown in Equation (3.1): \\[ \\begin{equation} f(x)=\\dfrac{n!}{x_1!x_2!\\cdots x_k!}\\theta_1^{x_1} \\theta_2^{x_2} \\cdots \\theta_k^{x_k} \\tag{3.1} \\end{equation} \\] k - number of sides on the die n - number of times the die will be rolled The multinomial representation of the distributions we’ve already discussed, binomial and bernoulli, would use the following parameters: k sided die rolled n times n = 1, k = 2 is bernoulli distribution: coin has 2 sides and we are only concerned with single experiments n &gt; 1 , k = 2 is binomial distribution: coin has 2 sides, but we are concerned with the probability of the outcome of a series of flips. To help illustrate the relationship between Bernoulli and Multinomial distributions we need to recall the bernoulli probability mass function shown in Equation (3.2). \\[ \\begin{equation} f(x)=P(X=x)=\\theta^{x}(1-\\theta)^{1-x}, \\hspace{1cm} x = \\{0,1\\} \\tag{3.2} \\end{equation} \\] Let’s swap in some different terms. We can replace the first term, \\(\\theta\\), with \\(\\theta_{1}\\) and the second term, \\((1-\\theta)\\) as \\(\\theta_{2}\\). This can be thought of as \\(\\theta_{heads}\\) and \\(\\theta_{tails}\\) for our coin flip example. \\[ \\begin{equation} f(x)={\\theta_{1}}^{x_{1}}{\\theta_{2}}^{x_{2}} \\\\ \\tag{3.3} \\end{equation} \\] You can see this looks a bit more like the multinomial PDF,Equation (3.2), but the factorial terms are not present. In this case k=2 and n=1 and x’s can only have a value of 1 or zero. The values of a bernoulli distribution are plugged into the multinomial PDF in Equation (3.4). Factorial of n in the numerator is always 1 since it is a single trial, i.e. n=1, and the denominator is also always 1 since our only possible outcomes are 0 and 1 (heads or tails). Once we simplify, we see the relationship that was stated previously: the bernoulli distribution is a special case of the multinomial distribution. \\[ \\begin{equation} \\begin{aligned} f(x)&amp;=\\dfrac{n!}{x_1!x_2!\\cdots x_k!}\\theta_1^{x_1} \\theta_2^{x_2} \\cdots \\theta_k^{x_k} \\\\ f(x)&amp;=\\dfrac{1}{1}\\theta_1^{x_1} \\theta_2^{x_2} \\\\ f(x)&amp;={\\theta_{1}}^{x_{1}}{\\theta_{2}}^{x_{2}} \\end{aligned} \\tag{3.4} \\end{equation} \\] 3.3 Conjugate Prior: Dirichlet The conjugate prior for the multinomial distribution is the Dirichlet distribution. Similar to the beta distribution, Dirichlet can be thought of as a distribution of distributions. Also note that the beta distribution is the special case of a Dirichlet distribution where the number of possible outcome is 2. This is similar to the relationship between the binomial and multinomial distributions. The probability distribution function for the Dirichlet distribution is shown in Equation (3.5). \\[ \\begin{equation} Dir(\\overrightarrow{\\theta}|\\overrightarrow{\\alpha})= { {\\Gamma {\\bigl (}\\sum _{i=1}^{K}\\alpha _{i}{\\bigr )}} \\over{\\prod _{i=1}^{K}\\Gamma (\\alpha _{i})} } \\prod _{i=1}^{K}\\theta_{i}^{\\alpha _{i}-1} \\tag{3.5} \\end{equation} \\] Equation (3.5) is often written using the Beta function in place of the first term as seen below: \\[ Dir(\\overrightarrow{\\theta}|\\overrightarrow{\\alpha})= { 1 \\over B(\\alpha) } \\prod _{i=1}^{K}\\theta_{i}^{\\alpha _{i}-1} \\] Where: \\[ {1\\over B(\\alpha)}={ {\\Gamma {\\bigl (}\\sum _{i=1}^{K}\\alpha _{i}{\\bigr )}} \\over{\\prod _{i=1}^{K}\\Gamma (\\alpha _{i})} } \\] The Dirichlet distribution is an extension of the beta distribution for k categories, similar to the relationship between multinomial and bernoulli distributions. To get a better sense of what the distributions look like, let’s visualize a few examples at k=3. In both of the box plots below 10,000 random samples were drawn from a Dirichlet distribution where k=3 and \\(\\alpha\\) is the same for each k in the given plot. The first plot shows the distribution of values drawn when \\(\\alpha\\) = 100. alpha &lt;- c(100,100,100) trials &lt;- 10000 x &lt;- rdirichlet(trials, alpha) colnames(x) &lt;- c(&#39;theta_1&#39;, &#39;theta_2&#39;, &#39;theta_3&#39;) ds &lt;- cbind(as.tibble(x), trial = 1:trials) %&gt;% gather(theta, word, -trial) ggplot(ds, aes(color = theta, fill = theta, x = theta, y = word)) + geom_boxplot(alpha = 0.3) + theme_minimal() + labs(y=&#39;\\U03B8&#39;, x = &#39;&#39;, title = paste0(&quot;\\U03B1 = &quot;,unique(alpha)) ) + scale_x_discrete(labels = c(expression(&quot;\\U03B1&quot;[1]), expression(&quot;\\U03B1&quot;[2]), expression(&quot;\\U03B1&quot;[3]))) + scale_fill_discrete(guide = FALSE) + scale_color_discrete(guide = FALSE)+ scale_y_continuous(limits = c(0,1)) Figure 3.3: Sampling from Dirichlet: α=100 Below the process is repeated, but this time the \\(\\alpha\\) values are set to 1 for each category. We can see the range of distribution of values sampled with the higher \\(\\alpha\\) value is much narrower than the distribution of values sampled using \\(\\alpha\\) values of 1. This is the same pattern we saw with the beta distribution; as the shape parameters increased the distribution became more dense and the shape of the distribution narrowed. alpha &lt;- c(1,1,1) x &lt;- rdirichlet(trials, alpha) colnames(x) &lt;- c(&#39;theta_1&#39;, &#39;theta_2&#39;, &#39;theta_3&#39;) ds &lt;- cbind(as.tibble(x), trial = 1:trials) %&gt;% gather(theta, word, -trial) ggplot(ds, aes(color = theta, fill = theta, x = theta, y = word)) + geom_boxplot(alpha = 0.3) + theme_minimal() + labs(y=&#39;\\U03B8&#39;, x = &#39;&#39;, title = paste0(&quot;\\U03B1 = &quot;,unique(alpha)) ) + scale_x_discrete(labels = c(expression(&quot;\\U03B1&quot;[1]), expression(&quot;\\U03B1&quot;[2]), expression(&quot;\\U03B1&quot;[3]))) + scale_fill_discrete(guide = FALSE) + scale_color_discrete(guide = FALSE) + scale_y_continuous(limits = c(0,1)) Figure 3.4: Sampling from Dirichlet - θ=1 So what happens when the \\(\\alpha\\) values are not the same, i.e. the distribution is asymmetrical? In the histogram below, we see the distribution of values sampled from the dirichlet distribution for each category. The distribution of samples for each category (\\(\\alpha_{i}\\) value), are approximately centered at the ratio of the \\(\\alpha_{i}\\) value to the sum of all \\(\\alpha\\) values. This is similar to the shift of the beta distribution when using hyperparameters that were unequal (see Figure 2.3). alpha &lt;- c(10,50,20) alpha_prop &lt;- alpha/sum(alpha) x &lt;- rdirichlet(trials, alpha) colnames(x) &lt;- c(&#39;theta_1&#39;, &#39;theta_2&#39;, &#39;theta_3&#39;) ds &lt;- cbind(as.tibble(x), trial = 1:trials) %&gt;% gather(theta, word, -trial) ggplot(ds, aes(color = theta, fill=theta, x = word)) + geom_histogram(position=&#39;identity&#39;, alpha = 0.1) + # geom_line(stat=&#39;density&#39;) + theme_minimal() + labs(x = &quot;\\U03B8&quot;, y = &quot;Count&quot;) + scale_color_discrete(label = alpha, name = &quot;\\U03B1&quot; ) + scale_fill_discrete(label = alpha, name = &quot;\\U03B1&quot; ) Figure 3.5: Sampling from Dirichlet: θ=[10,50,20] 3.4 Gibbs Sampling - Multinomial &amp; Dirichlet Prior to getting into an example of Gibbs sampling as it applies to inferring the parameters of a multinomial distribution, let’s first describe a model which generates words for a single document. As you can imagine this would be modeled as a multinomial distribution with parameters \\(\\overrightarrow{\\theta} = \\theta_{1}, \\theta_{2}, ... \\theta_{n}\\) for words 1 to n. The model would be capable of generating a bag of words representation of a document. The term ‘bag of words’ refers to words in no particular order, i.e. the document we would be generating would not have structured sentences, but would contain all the components of the document. The model will be used to generate a document using a limited vocabulary, only 3 distinct words: 📘,📕,📗. First we are going to create a seed document which I will refer to as an ideal document. The document is used as a basis of our \\(\\alpha\\)’s for our prior described by the Dirichlet distribution. To create this document we first define the mixture of words in the document: 📘 : 10% 📕 : 10% 📗 : 80% To clarify, this means the document will contain 80% blue books, 10% green books, and 10% red books: # use letters function as your vocabulary v &lt;- c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;) nwords &lt;- 10 doc_theta &lt;- c(.1, .1, .8) document&lt;-rep(v, doc_theta*nwords) books &lt;- tibble(label = c(&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;), code = c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;)) cat(sapply(document, function(x) books$code[which(books$label == x)])) ## 📕 📗 📘 📘 📘 📘 📘 📘 📘 📘 Do you recall the beta/bernoulli example? The way we informed our prior was using some prior information we had, i.e. the number of heads and tails previously obtained from flipping the two coins. We will use the document above as the basis of our \\(\\alpha\\) paramters for the Dirichlet distribution, i.e. our prior for the multinomial. In more general language, we want to generate documents similar to our ‘ideal’ document. So let’s generate a new document using the word counts from our ideal document as our \\(\\alpha\\) values for the dirichlet prior. Then we use the \\(\\theta\\) values generated by the dirichlet prior as the parameters for a multinomial distribution to generate the next term in the document. words &lt;- document word_counts &lt;- table(words) alphas &lt;- word_counts new_doc &lt;- rep(&#39;&#39;, nwords) for(i in 1:nwords){ set.seed(i) p = rdirichlet(1,alphas) set.seed(i) new_doc[i] &lt;- names(word_counts)[which(rmultinom(1, 1, p) == 1)] } cat(&#39;\\n&#39;, sapply(new_doc, function(x) books$code[which(books$label == x)])) ## ## 📘 📘 📘 📘 📘 📘 📕 📘 📘 📘 It’s not quite the same as the original, but that should be expected. This is a model that generates documents probabalistically based on some prior information. So let’s make a few more documents and see how this changes. word_counts &lt;- table(words) alphas &lt;- word_counts nwords &lt;- 10 ndocs &lt;- 5 word_encodings &lt;- tibble(label = c(&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;), code = c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;), word_props = c(.1, .1, .8)) thetas &lt;- rdirichlet(ndocs*nwords, alphas) print(head(thetas)) ## [,1] [,2] [,3] ## [1,] 0.6259047 0.33535589 0.038739455 ## [2,] 0.8641536 0.07229474 0.063551631 ## [3,] 0.8292271 0.02713615 0.143636766 ## [4,] 0.9399257 0.05494752 0.005126745 ## [5,] 0.7641922 0.19525569 0.040552159 ## [6,] 0.9015058 0.02198210 0.076512137 selected_words &lt;- apply(thetas, 1, function(x) which(rmultinom(1,1,x)==1)) ds &lt;- tibble(doc_id = rep(1:ndocs, each = nwords), word = word_encodings$label[selected_words], word_uni = word_encodings$code[selected_words]) ds %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(word_uni, collapse = &#39; &#39;) ) %&gt;% kable(col.names = c(&#39;Document&#39;, &#39;Words&#39;)) Document Words 1 📘 📕 📘 📘 📕 📘 📕 📕 📘 📘 2 📘 📘 📘 📘 📘 📘 📘 📕 📘 📘 3 📘 📘 📘 📘 📗 📘 📘 📘 📕 📘 4 📘 📘 📗 📕 📘 📘 📘 📘 📘 📘 5 📕 📘 📗 📘 📕 📘 📘 📘 📘 📘 As we can see each document composition is similar, but the word counts and order are different each time. This is to be expected as we are using a model with some degree of randomness to generate our documents. So now onto inferernce …. The process above is known as a generative model. We created documents using a model with a given set of parameters. Inference is going to take this general concept and look at it from a different angle. Instead of generating documents with our model we are going to take a series of pre-existing documents and infer what model created them. We are going to make the assumption that the structure of the model is the same as the generative example, i.e. all documents are generated based on the same word mixture ratios: 📘 : 10% 📕 : 10% 📗 : 80% Let’s use the 5 documents we previously generated as our basis and infer the parameters used to generate them via Gibbs sampling. alphas &lt;- rep(1,nrow(books)) n &lt;- table(ds$word) niters = 2000 burnin = 500 thetas = matrix(0, nrow = (niters-burnin), ncol=nrow(books), dimnames = list(NULL, c(names(n)))) for (i in 1:niters){ theta = rdirichlet(1,n+alphas) if (i &gt;= burnin){ thetas[(i-burnin), ] = theta } } df &lt;- as.tibble(thetas) %&gt;% gather(word, theta) ggplot(df, aes(y=theta, x = word, fill=word)) + geom_violin() + scale_fill_manual(values=c(&#39;#0057e7&#39;, &#39;#008744&#39;,&#39;#d62d20&#39;)) Figure 3.6: Gibbs Sampling with 5 Documents 3.4.1 Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc) Below is a general overview of how inferrence can be carried out using Gibbs sampling. Recall a conjugate priors has the same form as the posterior distribution. In equation (3.6) we start with the proportional solution, i.e. no evidence term, for estimating a posterior through sampling. We need the likelihood, which is derived from the multinomial distribution, and the prior, which is derived from the dirichlet distribution. Once we plug in the prior and likelihood and simplify, we find that we are left with a Dirichlet PDF with the input parameters of \\(\\overrightarrow{\\alpha} + \\overrightarrow{n}\\) where n are the observed word counts. \\[ \\begin{equation} \\begin{aligned} p(\\theta|D) &amp;\\propto p(D|\\theta)p(\\theta)\\\\ &amp;\\propto \\prod _{i=1}^{K}\\theta^{n(k)} { {\\Gamma {\\bigl (}\\sum _{i=1}^{K}\\alpha _{i}{\\bigr )}} \\over{\\prod _{i=1}^{K}\\Gamma (\\alpha _{i})} } \\prod _{i=1}^{K}\\theta_{i}^{\\alpha _{i}-1} \\\\ &amp;\\propto{ {\\Gamma {\\bigl (}\\sum _{i=1}^{K}\\alpha _{i}{\\bigr )}} \\over{\\prod _{i=1}^{K}\\Gamma (\\alpha _{i})} }\\prod _{i=1}^{K}\\theta_{i}^{\\alpha _{i}+n_{k}-1} \\\\ &amp;\\propto Dir(\\overrightarrow{\\alpha} + \\overrightarrow{n}) \\end{aligned} \\tag{3.6} \\end{equation} \\] We can see our mixture estimates are significantly different from the real model used to generate the documents in Figure 3.6. So why is this? One of the issues here is that our sample are documents with only 10 words. Therefore an average document has 1 📘, 1 📕, and 8 📗, but it is not unusual to see a slight variation which causes mixture shifts of 10% or more. Let’s try the same example, but this time instead of only generating 5 documents we will genertate 500 and use this as our sample to infer the word mixtures from. word_counts &lt;- table(words) alphas &lt;- word_counts nwords &lt;- 10 ndocs &lt;- 500 word_encodings &lt;- tibble(label = c(&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;), code = c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;), word_props = c(.1, .1, .8)) thetas &lt;- rdirichlet(ndocs*nwords, alphas) print(head(thetas)) ## [,1] [,2] [,3] ## [1,] 0.5154606 0.40401130 0.08052810 ## [2,] 0.8526076 0.12103847 0.02635393 ## [3,] 0.7801023 0.14641427 0.07348343 ## [4,] 0.8444930 0.06278609 0.09272094 ## [5,] 0.7224804 0.11597181 0.16154776 ## [6,] 0.8639552 0.01028122 0.12576353 selected_words &lt;- apply(thetas, 1, function(x) which(rmultinom(1,1,x)==1)) ds &lt;- tibble(doc_id = rep(1:ndocs, each = nwords), word = word_encodings$label[selected_words], word_uni = word_encodings$code[selected_words]) alphas &lt;- rep(1,nrow(books)) n &lt;- table(ds$word) head(n) ## ## blue green red ## 4008 507 485 niters = 2000 burnin = 500 thetas = matrix(0, nrow = (niters-burnin), ncol=nrow(books), dimnames = list(NULL, c(names(n)))) for (i in 1:niters){ theta = rdirichlet(1,n+alphas) if (i &gt;= burnin){ thetas[(i-burnin), ] = theta } } df &lt;- as.tibble(thetas) %&gt;% gather(word, theta) ggplot(df, aes(y=theta, x = word, fill=word)) + geom_violin() + scale_fill_manual(values=c(&#39;#0057e7&#39;, &#39;#008744&#39;,&#39;#d62d20&#39;)) Figure 3.7: Gibbs Sampling with 500 Documents In Figure 3.7 we can see the distribution of estimates are more dense and centered at approximately the true values used to generate the sample documents. "],
["word-representations.html", "4 Word Representations 4.1 Bag of Words 4.2 Word Counts 4.3 Plug and Play LDA", " 4 Word Representations This chapter offers a general overview of how documents and words are viewed and processed during topic modeling. 4.1 Bag of Words LDA processes documents as a ‘bag of words’. Bag of words means you view the frequency of the words in a document with no regard for the order the words appeared in. Figure 4.1: Bag of Words Representation Obviously there is going to be some information lost in this process, but our goal with topic modeling is to be able to view the ‘big picture’ from a large number of documents. Alternate way of thinking about it: I have a vocabulary of 100k words used across 1 million documents, then I use LDA to look 500 topics. I just narrowed down my feature space from 100k features to 500 features (dimensionality reduction). Possible examples: Scrabble in reverse 4.2 Word Counts For the purposes of LDA we need a way of summarizing our documents. We need to keep count of the following: Topic Term Matrix (variable name: n_topic_term_count, dimensions: Topic x Vocabulary): In the case of LDA, a single unique word can be assigned a different topic for each instance of the word (or generated from more than 1 topic in the case of a generative model.) Document Topic Matrix (variable name: n_doc_topic_count, dimensions: Document x Topics): This is the total number of words assigned to each topic in each document. Topic sum vector (variable name: n_topic_sum, dimensions: 1 x Topics): The topic sum vector keeps count of the total number of words in the entire corpus (all docs together) assigned to each topic. This is just a column sum of the document topic matrix. Note: My example code uses a very small vocabulary and therefore the row/column assignment for the matrices above has very little effect on computational efficiency/speed. If you are writing code to do LDA, you may want to swap around the row/columns to maximize efficiency, i.e. some languages are much faster row-wise than column-wise. The structures above are necessary for performing inference tasks utilizing LDA. We will go over how these structures are initialized, updated, and utilized in Chapter 6. 4.3 Plug and Play LDA Considering most people are not going to be writing LDA code from scratch, it is worth mentioning that when using most software packages for LDA the only structure you will need to build is a term-document matrix. This is a matrix that has a single row for each term in the vocabulary and a single column for each document. This is a common input across packages such as Gensim (Python), Scikit-learn (Python), and TopicModels (R) (some are doc-term, some are term doc, so double check the package docs). Mallet uses something similar, but the user only needs to make a document file. Nearly all of the packages mentioned also have additional helper functions to convert lists of documents to the required input format. "],
["lda-as-a-generative-model.html", "5 LDA as a Generative Model 5.1 General Terminology 5.2 Generative Model", " 5 LDA as a Generative Model This chapter is going to focus on LDA as a generative model. I’m going to build on the unigram generation example from the last chapter and with each new example a new variable will be added until we work our way up to LDA. After getting a grasp of LDA as a generative model in this chapter, the following chapter will focus on working backwards to answer the following question: “If I have a bunch of documents, how do I infer topic information (word distributions, topic mixtures) from them?” 5.1 General Terminology Let’s get the ugly part out of the way, the parameters and variables that are going to be used in the model. alpha (\\(\\overrightarrow{\\alpha}\\)) : In order to determine the value of \\(\\theta\\), the topic distirbution of the document, we sample from a dirichlet distribution using \\(\\overrightarrow{\\alpha}\\) as the input parameter. What does this mean? The \\(\\overrightarrow{\\alpha}\\) values are our prior information about the topic mixtures for that document. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the total number of words from each topic across all documents as the \\(\\overrightarrow{\\beta}\\) values. beta (\\(\\overrightarrow{\\beta}\\)) : In order to determine the value of \\(\\phi\\), the word distirbution of a given topic, we sample from a dirichlet distribution using \\(\\overrightarrow{\\beta}\\) as the input parameter. What does this mean? The \\(\\overrightarrow{\\beta}\\) values are our prior information about the word distribution in a topic. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the number of times each word was used for a given topic as the \\(\\overrightarrow{\\beta}\\) values. theta (\\(\\theta\\)) : Is the topic proportion of a given document. More importantly it will be used as the parameter for the multinomial distribution used to identify the topic of the next word. To clarify, the selected topic’s word distribution will then be used to select a word w. phi (\\(\\phi\\)) : Is the word distribution of each topic, i.e. the probability of each word in the vocabulary being generated if a given topic, z (z ranges from 1 to k), is selected. xi (\\(\\xi\\)) : In the case of a variable lenght document, the document length is determined by sampling from a Poisson distribution with an average length of \\(\\xi\\) k : Topic index z : Topic selected for the next word to be generated. w : Generated Word d : Current Document Outside of the variables above all the distributions should be familiar from the previous chapter. 5.1.1 Selecting Parameters The intent of this section is not aimed at delving into different methods of parameter estimation for \\(\\alpha\\) and \\(\\beta\\), but to give a general understanding of how those values effect your model. For ease of understanding I will also stick with an assumption of symmetry, i.e. all values in \\(\\overrightarrow{\\alpha}\\) are equal to one another and all values in \\(\\overrightarrow{\\beta}\\) are equal to one another. Symmetry can be thought of as each topic having equal probability in each document for \\(\\alpha\\) and each word having an equal probability in \\(\\beta\\). In previous sections we have outlined how the \\(alpha\\) parameters effect a Dirichlet distribution, but now it is time to connect the dots to how this effects our documents. 5.2 Generative Model LDA is know as a generative model. What is a generative model? Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space (Bishop 2006). This means we can create documents with a mixture of topics and a mixture of words based on thosed topics. Let’s start off with a simple example of generating unigrams. 5.2.1 Generating Documents 5.2.1.1 Topic Word Mixtures, Document Topic Mixtures, and Document Length Static Building on the document generating model in chapter two, let’s try to create documents that have words drawn from more than one topic. To clarify the contraints of the model will be: set number of topics (2) constant topic distributions in each document constant word distribution in each topic Known values: 2 topics : word distributions of each topic below \\(\\phi_{1}\\) = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ] \\(\\phi_{2}\\) = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ] All Documents have same topic distribution: \\(\\theta = [ topic a = 0.5, topic b = 0.5 ]\\) All Documents contain 10 words Generative Model Pseudocode For d = 1 to D where D is the number of documents For w = 1 to W where W is the number of words in document d Select the topic for word w \\(z_{i}\\) ~ Multinomial(\\(\\theta_{d}\\)) Select word based on topic z’s word distribution \\(w_{i}\\) ~ Multinomial(\\(\\phi^{(z_{i})}\\)) k &lt;- 2 # number of topics M &lt;- 10 # let&#39;s create 10 documents vocab &lt;- c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;) alphas &lt;- rep(1,k) # topic document dirichlet parameters phi &lt;- matrix(c(0.1, 0, 0.9, 0.4, 0.4, 0.2), nrow = k, ncol = length(vocab), byrow = TRUE) theta &lt;- c(0.5, 0.5) N &lt;- 10 #words in each document ds &lt;-tibble(doc_id = rep(0,N*M), word = rep(&#39;&#39;, N*M), topic = rep(0, N*M) ) row_index &lt;- 1 for(m in 1:M){ for(n in 1:N){ # sample topic index , i.e. select topic topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- vocab[which(rmultinom(1,1,phi[topic, ])==1)] ds[row_index,] &lt;- c(m,new_word, topic) row_index &lt;- row_index + 1 } } ds %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(word, collapse = &#39; &#39;) ) %&gt;% kable() doc_id tokens 1 📘 📕 📗 📘 📘 📘 📗 📘 📗 📗 10 📗 📘 📕 📘 📘 📗 📕 📕 📗 📗 2 📗 📗 📘 📗 📗 📗 📗 📗 📘 📗 3 📕 📗 📗 📗 📗 📕 📗 📕 📕 📗 4 📗 📘 📗 📗 📗 📗 📕 📗 📘 📗 5 📕 📗 📗 📘 📗 📘 📘 📗 📗 📗 6 📗 📘 📗 📘 📗 📕 📕 📗 📗 📗 7 📘 📗 📗 📗 📗 📗 📕 📕 📗 📕 8 📗 📗 📗 📗 📗 📗 📘 📕 📗 📕 9 📗 📘 📗 📗 📗 📗 📘 📘 📘 📕 5.2.1.2 Topic Word Mixtures &amp; Document Topic Mixtures Static, Document Length Varying This next example is going to be very similar, but it now allows for varying document length. The length of each document is determined by a Poisson distribution with an average document length of 10. Known values: 2 topics : word distributions of each topic below \\(\\phi_{1}\\) = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ] \\(\\phi_{2}\\) = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ] All Documents have same topic distribution: \\(\\theta = [ topic a = 0.5, topic b = 0.5 ]\\) Generative Model Pseudocode For d = 1 to D where D is the number of documents Determine length of document \\(W ~ Poisson(\\xi)\\) For w = 1 to W where W is the number of words in document d Select the topic for word w \\(z_{i}\\) ~ Multinomial(\\(\\theta_{d}\\)) Select word based on topic z’s word distribution \\(w_{i}\\) ~ Multinomial(\\(\\phi^{(z_{i})}\\)) k &lt;- 2 # number of topics M &lt;- 10 # let&#39;s create 10 documents vocab &lt;- c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;) alphas &lt;- rep(1,k) # topic document dirichlet parameters phi &lt;- matrix(c(0.1, 0, 0.9, 0.4, 0.4, 0.2), nrow = k, ncol = length(vocab), byrow = TRUE) theta &lt;- c(0.5, 0.5) xi &lt;- 10 # average document length N &lt;- rpois(M, xi) #words in each document ds &lt;-tibble(doc_id = rep(0,sum(N)), word = rep(&#39;&#39;, sum(N)), topic = rep(0, sum(N)) ) row_index &lt;- 1 for(m in 1:M){ for(n in 1:N[m]){ # sample topic index , i.e. select topic topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- vocab[which(rmultinom(1,1,phi[topic, ])==1)] ds[row_index,] &lt;- c(m,new_word, topic) row_index &lt;- row_index + 1 } } ds %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(word, collapse = &#39; &#39;) ) %&gt;% kable() doc_id tokens 1 📗 📘 📗 📘 📗 📗 📗 📕 📕 📗 📗 📘 10 📕 📕 📘 📗 📘 📗 📗 📗 📕 📗 2 📗 📗 📗 📗 📗 📕 📗 📘 📗 📕 📗 📘 📗 3 📕 📗 📗 📕 📘 📘 📗 📘 📕 📘 📕 📘 📕 📘 📘 📗 4 📗 📕 📕 📗 📘 📗 📘 📘 📘 📕 📗 📘 📗 📕 📘 📕 5 📗 📗 📗 📗 📗 📘 📕 📗 📗 📘 📕 📗 📗 📘 📗 6 📗 📘 📘 📗 📗 📗 📗 📘 📗 📗 📗 📗 📕 📗 7 📘 📘 📘 📗 📕 📗 📘 📘 📗 📘 📘 📗 📕 📘 📘 📗 📗 📕 📗 📗 📘 📗 8 📘 📗 📗 📕 📕 📗 📗 📕 📗 📗 📗 📕 📗 📗 📗 📕 📕 📕 📘 9 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 5.2.1.3 Topic Word Mixtures Static, Varying Document Topic Distributions and Document Length So this time we will introduce documents with different topic distributions and length.The word distributions for each topic are still fixed. Known values: 2 topics : word distributions of each topic below \\(\\phi_{1}\\) = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ] \\(\\phi_{2}\\) = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ] Generative Model Pseudocode For d = 1 to D where number of documents is D + Sample parameters for document topic distribution + \\(\\theta_{d}\\) ~ Dirichlet(\\(\\alpha\\)) + For w = 1 to W where W is the number of words in document d + Select the topic for word w + \\(z_{i}\\) ~ Multinomial(\\(\\theta_{d}\\)) + Select word based on topic z’s word distribution + \\(w_{i}\\) ~ Multinomial(\\(\\phi^{(z_{i})}\\)) k &lt;- 2 # number of topics M &lt;- 10 # let&#39;s create 10 documents vocab &lt;- c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;) alphas &lt;- rep(1,k) # topic document dirichlet parameters phi &lt;- matrix(c(0.1, 0, 0.9, 0.4, 0.4, 0.2), nrow = k, ncol = length(vocab), byrow = TRUE) xi &lt;- 10 # average document length N &lt;- rpois(M, xi) #words in each document ds &lt;-tibble(doc_id = rep(0,sum(N)), word = rep(&#39;&#39;, sum(N)), topic = rep(0, sum(N)), theta_a = rep(0, sum(N)), theta_b = rep(0, sum(N)) ) row_index &lt;- 1 for(m in 1:M){ theta &lt;- rdirichlet(1, alphas) for(n in 1:N[m]){ # sample topic index , i.e. select topic topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- vocab[which(rmultinom(1,1,phi[topic, ])==1)] ds[row_index,] &lt;- c(m,new_word, topic,theta) row_index &lt;- row_index + 1 } } ds %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(word, collapse = &#39; &#39;), topic_a = round(as.numeric(unique(theta_a)), 2), topic_b = round(as.numeric(unique(theta_b)), 2) ) %&gt;% kable() doc_id tokens topic_a topic_b 1 📕 📕 📗 📕 📗 📘 📗 📗 📗 📗 📗 0.08 0.92 10 📘 📕 📘 📘 📘 📘 📘 📘 📗 📘 0.11 0.89 2 📘 📗 📗 📗 📘 📕 📗 📕 📘 0.39 0.61 3 📗 📗 📗 📗 📗 📗 📘 📗 📘 📗 📗 0.43 0.57 4 📗 📘 📕 📕 📘 📗 📗 📕 📘 📕 📗 0.40 0.60 5 📗 📘 📘 📗 0.82 0.18 6 📘 📗 📘 📕 📗 📕 📘 📘 📕 📗 📗 📕 📗 📘 📕 0.11 0.89 7 📗 📗 📗 📗 📗 📘 📗 0.66 0.34 8 📘 📘 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 0.68 0.32 9 📕 📘 📕 📘 📗 📘 📗 📕 📘 📗 📘 📕 0.01 0.99 5.2.2 LDA Generative Model We are finally at the full generative model for LDA. The word distributions for each topic vary based on a dirichlet distribtion, as do the topic distribution for each document, and the document length is drawn from a Poisson distribution. Generative Model Pseudocode For k = 1 to K where K is the total number of topics Sample parameters for word distribution of each topic \\(\\phi^{(k)}\\) ~ Dirichlet(\\(\\beta\\)) For d = 1 to D where number of documents is D Sample parameters for document topic distribution \\(\\theta_{d}\\) ~ Dirichlet(\\(\\alpha\\)) For w = 1 to W where W is the number of words in document d Select the topic for word w \\(z_{i}\\) ~ Multinomial(\\(\\theta_{d}\\)) Select word based on topic z’s word distribution \\(w_{i}\\) ~ Multinomial(\\(\\phi^{(z_{i})}\\)) k &lt;- 2 # number of topics M &lt;- 10 # let&#39;s create 10 documents vocab &lt;- c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;) alphas &lt;- rep(1,k) # topic document dirichlet parameters betas &lt;- rep(1,length(vocab)) # dirichlet parameters for topic word distributions phi &lt;- rdirichlet(k, betas) xi &lt;- 10 # average document length N &lt;- rpois(M, xi) #words in each document ds &lt;-tibble(doc_id = rep(0,sum(N)), word = rep(&#39;&#39;, sum(N)), topic = rep(0, sum(N)), theta_a = rep(0, sum(N)), theta_b = rep(0, sum(N)) ) row_index &lt;- 1 for(m in 1:M){ theta &lt;- rdirichlet(1, alphas) for(n in 1:N[m]){ # sample topic index , i.e. select topic topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- vocab[which(rmultinom(1,1,phi[topic, ])==1)] ds[row_index,] &lt;- c(m,new_word, topic,theta) row_index &lt;- row_index + 1 } } ds %&gt;% group_by(doc_id, topic) %&gt;% summarise( tokens = paste(word, collapse = &#39; &#39;), topic_a = round(as.numeric(unique(theta_a)), 2), topic_b = round(as.numeric(unique(theta_b)), 2) ) %&gt;% kable() doc_id topic tokens topic_a topic_b 1 1 📕 📘 📘 📕 📗 📘 📘 📕 📕 📗 📕 0.75 0.25 1 2 📕 📕 📘 0.75 0.25 10 2 📘 📘 📘 📘 📘 📘 0.08 0.92 2 1 📕 📕 📗 📕 📕 📗 📘 0.56 0.44 2 2 📕 📘 📗 0.56 0.44 3 2 📘 📗 📘 📗 📘 📘 📕 📕 📘 0.01 0.99 4 1 📕 0.13 0.87 4 2 📕 📘 📗 📗 📕 📕 📘 0.13 0.87 5 1 📗 📗 📗 📕 📘 📘 📕 📘 📕 📗 📘 📗 📘 📘 📗 0.87 0.13 5 2 📘 📕 0.87 0.13 6 1 📕 0.26 0.74 6 2 📘 📘 📕 📗 📘 📘 📕 📘 0.26 0.74 7 1 📘 📕 0.13 0.87 7 2 📘 📘 📗 📕 📘 📘 📗 0.13 0.87 8 1 📗 📘 📕 📕 📕 📗 📕 0.60 0.40 8 2 📕 📘 📘 📘 📕 0.60 0.40 9 1 📕 📘 📘 📗 📗 📕 📘 📘 📘 📘 📕 📗 0.77 0.23 9 2 📘 📗 0.77 0.23 (NOTE: Spread the table so topics are side by side for each doc) The LDA generative process for each document is shown below(Darling 2011): \\[ \\begin{equation} p(w,z,\\theta,\\phi|\\alpha, B) = p(\\phi|B)p(\\theta|\\alpha)p(z|\\theta)p(w|\\phi_{z}) \\tag{5.1} \\end{equation} \\] You may be like me and have a hard time seeing how we get to the equation above and what it even means. If we look back at the pseudo code for the LDA model it is a bit easier to see how we got here. We start by giving a probability of a topic for each word in the vocabulary, \\(\\phi\\). This value is drawn randomly from a dirichlet distribution with the parameter \\(\\beta\\) giving us our first term \\(p(\\phi|\\beta)\\). The next step is generating documents which starts by calculating the topic mixture of the document, \\(\\theta_{d}\\) generated from a dirichlet distribution with the parameter \\(\\alpha\\). This is our second term \\(p(\\theta|\\alpha)\\). You can see the following two terms also follow this trend. The topic, z, of the next word is drawn from a multinomial distribuiton with the parameter \\(\\theta\\). Once we know z, we use the distribution of words in topic z, \\(\\phi_{z}\\), to determine the word that is generated. "],
["lda-inference.html", "6 LDA Inference 6.1 General Overview 6.2 Mathematical Derivations for Inference 6.3 Animal Farm - Code Example", " 6 LDA Inference 6.1 General Overview We have talked about LDA as a generative model, but now it is time to flip the problem around. What if I have a bunch of documents and I want to infer topics? To figure out a problem like this we are first going to assume the documents were generated using a generative model similar to the ones we created in the previous section. Now it is time to flip the problem around. What happens when I have a bunch of documents and I want to know what topics are present in each document and what words are present (most probable) in each topic? To answer the previous questions, we first need to be able to attain the answer for Equation (6.1). (NOTE: The derivation for LDA inference via Gibbs Sampling is taken from (Darling 2011), (Heinrich 2008) and (Steyvers and Griffiths 2007).) \\[ \\begin{equation} p(\\theta, \\phi, z|w, \\alpha, \\beta) = {p(\\theta, \\phi, z, w|\\alpha, \\beta) \\over p(w|\\alpha, \\beta)} \\tag{6.1} \\end{equation} \\] Equation (6.1) says the following: The probability of the document topic distribution, the word distribution of each topic, and the topic labels of all words (in all documents) and the hyperparameters \\(\\alpha\\) and \\(\\beta\\). In particular we are interested in estimating the probability of topic (z) for a given word (w) (and our prior assumptions, i.e. hyperparameters) for all words and topics. From this we can infer \\(\\phi\\) and \\(\\theta\\). (NOTE - COME BACK TO THIS AS THIS NEEDS TO BE MORE CLEAR) Quick notes: * Equation (6.1) is based on the following statistical property \\[ \\begin{equation} p(A, B | C) = {p(A,B,C) \\over p(C)} \\tag{6.2} \\end{equation} \\] All the variables used in this section were outlined at the beginning of Chapter 5 if you need a refresher. Let’s take a step from the math and just map out ‘things we know’ versus the ‘things we don’t know’ in regards to an inference problem: Known Parameters Documents (d in D): We have a set number of documents we want to identify the topic strucutres in. Words (w in W): We have a collection of words and word counts for each document which we place in a document word matrix. Vocabulary (W): The unique list of words across all documents. This means our document word matrix has one row for each document and one column for each word in our vocabulary. Hyperparemeters: \\(\\overrightarrow{\\alpha}\\): Our prior assumption about the topic distribution of our documents. This book will only use symmetric \\(\\alpha\\) values, in other words we assume all topics are equally as probably in any given document (similar to the naive assumption of a fair die). We will be supplying the \\(\\alpha\\) value for inference. Higher \\(\\overrightarrow{\\alpha}\\) - We assume documents will have a similar and close to uniform distribution of topics. Lower \\(\\overrightarrow{\\alpha}\\) - We assume document topic distributions vary more drastically. \\(\\overrightarrow{\\beta}\\): Our prior assumption about the word distribution of each topic. Higher \\(\\overrightarrow{\\beta}\\): Word distributions in each topic are closer to uniform, i.e. each word is equally as likely in each topic. Lower \\(\\overrightarrow{\\beta}\\): Word distributions vary more from topic to topic. And on to the parts we don’t know…. ** Unknown (Latent) Parameters ** Number of Topics (k): Disclaimer - We need to specify the number of topics we assume are present in the documents. However we don’t know the real number of topics in the corpus. For the purposes of this book I’m going to skip how to estimate the number of topics. If you are interested in learning more about how to estimate the number of topics present in a document see REFERENCE LIST. Document Topic Mixture (\\(\\theta\\)): We need to determine the topic distribution in each document. Word Distribution of Each Topic (\\(\\phi\\)): We need to know the distribution of words in each topic. Obviously some words are going to occur very often in a topic while others may have zero probability of occurring in a topic. Word topic assignment (z): This is actually the main thing we need to infer. To be clear, if we know the topic assignment of every word in every document, then we can derive the document topic mixture, \\(\\theta\\), and the word distribution, \\(\\phi\\), of each topic. 6.2 Mathematical Derivations for Inference Back to the math… The derivation connecting equation (6.1) to the actual Gibbs sampling solution to determine z for each word in each document, \\(\\overrightarrow{\\theta}\\), and \\(\\overrightarrow{\\phi}\\) is very complicated and I’m going to gloss over a few steps. For complete derivations see (Heinrich 2008) and (Carpenter 2010). As stated previously, the main goal of inference in LDA is to determine the topic of each word, \\(z_{i}\\) (topic of word i), in each document. \\[ \\begin{equation} p(z_{i}|z_{\\neg i}, \\alpha, \\beta, w) \\tag{6.3} \\end{equation} \\] Notice that we are interested in identifying the topic of the current word, \\(z_{i}\\), based on the topic assignments of all other words (not including the current word i), which is signified as \\(z_{\\neg i}\\). \\[ \\begin{equation} \\begin{aligned} p(z_{i}|z_{\\neg i}, \\alpha, \\beta, w) &amp;= {p(z_{i},z_{\\neg i}, w, | \\alpha, \\beta) \\over p(z_{\\neg i},w | \\alpha, \\beta)}\\\\ &amp;\\propto p(z_{i}, z_{\\neg i}, w | \\alpha, \\beta)\\\\ &amp;\\propto p(z,w|\\alpha, \\beta) \\end{aligned} \\tag{6.4} \\end{equation} \\] You may notice \\(p(z,w|\\alpha, \\beta)\\) looks very similar to the definition of the generative process of LDA from the previous chapter (equation (5.1)). The only difference is the absence of \\(\\theta\\) and \\(\\phi\\). This means we can swap in equation (5.1) and integrate out \\(\\theta\\) and \\(\\phi\\). \\[ \\begin{equation} \\begin{aligned} p(w,z|\\alpha, \\beta) &amp;= \\int \\int p(z, w, \\theta, \\phi|\\alpha, \\beta)d\\theta d\\phi\\\\ &amp;= \\int \\int p(\\phi|\\beta)p(\\theta|\\alpha)p(z|\\theta)p(w|\\phi_{z})d\\theta d\\phi \\\\ &amp;= \\int p(z|\\theta)p(\\theta|\\alpha)d \\theta \\int p(w|\\phi_{z})p(\\phi|\\beta)d\\phi \\end{aligned} \\tag{6.5} \\end{equation} \\] As with the previous Gibbs sampling examples in this book we are going to expand equation (6.4), plug in our conjugate priors, and get to a point where we can use a Gibbs sampler to estimate our solution. Below we continue to solve for the first term of equation (6.5) utilizing the conjugate prior relationship between the multinomial and Dirichlet distribution. The result is a Dirichlet distribution with the parameters comprised of the sum of the number of words assigned to each topic and the alpha value for each topic in the current document d. \\[ \\begin{equation} \\begin{aligned} \\int p(z|\\theta)p(\\theta|\\alpha)d \\theta &amp;= \\int \\prod_{i}{\\theta_{d_{i},z_{i}}{1\\over B(\\alpha)}}\\prod_{k}\\theta_{d,k}^{\\alpha k}\\theta_{d} \\\\ &amp;={1\\over B(\\alpha)} \\int \\prod_{k}\\theta_{d,k}^{n_{d,k} + \\alpha k} \\\\ &amp;={B(n_{d,.} + \\alpha) \\over B(\\alpha)} \\end{aligned} \\tag{6.6} \\end{equation} \\] Similarly we can expand the second term of Equation (6.5) and we find a solution with a similar form. The result is a Dirichlet distribution with the parameter comprised of the sum of the number of words assigned to each topic across all documents and the alpha value for that topic. \\[ \\begin{equation} \\begin{aligned} \\int p(w|\\phi_{z})p(\\phi|\\beta)d\\phi &amp;= \\int \\prod_{d}\\prod_{i}\\phi_{z_{d,i},w_{d,i}} \\prod_{k}{1 \\over B(\\beta)}\\prod_{w}\\phi^{B_{w}}_{k,w}d\\phi_{k}\\\\ &amp;= \\prod_{k}{1\\over B(\\beta)} \\int \\prod_{w}\\phi_{k,w}^{B_{w} + n_{k,w}}d\\phi_{k}\\\\ &amp;=\\prod_{k}{B(n_{k,.} + \\beta) \\over B(\\beta)} \\end{aligned} \\tag{6.7} \\end{equation} \\] This leaves us with the following: \\[ \\begin{equation} \\begin{aligned} p(w,z|\\alpha, \\beta) &amp;= \\prod_{d}{B(n_{d,.} + \\alpha) \\over B(\\alpha)} \\prod_{k}{B(n_{k,.} + \\beta) \\over B(\\beta)} \\end{aligned} \\tag{6.8} \\end{equation} \\] The equation necessary for Gibbs sampling can be derived by utilizing (6.8). This is accomplished via the chain rule and the definition of conditional probability. Quick Note: The chain rule is outlined in Equation (6.9) \\[ \\begin{equation} p(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C) \\tag{6.9} \\end{equation} \\] The conditional probability property utilized is shown in (6.10) \\[ \\begin{equation} P(B|A) = {P(A,B) \\over P(A)} \\tag{6.10} \\end{equation} \\] CHANGE THE -i’s TO THE NEGATION SIGN FOR CONSISTENCY \\[ \\begin{equation} \\begin{aligned} p(z_{i}|z_{\\neg i}, w) &amp;= {p(w,z)\\over {p(w,z_{\\neg i})}} = {p(z)\\over p(z_{\\neg i})}{p(w|z)\\over p(w_{\\neg i}|z_{\\neg i})p(w_{i})}\\\\ \\\\ &amp;\\propto \\prod_{d}{B(n_{d,.} + \\alpha) \\over B(n_{d,\\neg i}\\alpha)} \\prod_{k}{B(n_{k,.} + \\beta) \\over B(n_{k,\\neg i} + \\beta)}\\\\ \\\\ &amp;\\propto {\\Gamma(n_{d,k} + \\alpha_{k}) \\Gamma(\\sum_{k=1}^{K} n_{d,\\neg i}^{k} + \\alpha_{k}) \\over \\Gamma(n_{d,\\neg i}^{k} + \\alpha_{k}) \\Gamma(\\sum_{k=1}^{K} n_{d,k}+ \\alpha_{k})} {\\Gamma(n_{k,w} + \\beta_{w}) \\Gamma(\\sum_{w=1}^{W} n_{k,\\neg i}^{w} + \\beta_{w}) \\over \\Gamma(n_{k,\\neg i}^{w} + \\beta_{w}) \\Gamma(\\sum_{w=1}^{W} n_{k,w}+ \\beta_{w})}\\\\ \\\\ &amp;\\propto (n_{d,\\neg i}^{k} + \\alpha_{k}) {n_{k,\\neg i}^{w} + \\beta_{w} \\over \\sum_{w} n_{k,\\neg i}^{w} + \\beta_{w}} \\end{aligned} \\tag{6.11} \\end{equation} \\] We will now use Equation (6.11) in the example below to complete the LDA Inference task on a random sample of documents. To calculate our word distributions in each topic we will use Equation (6.12). \\[ \\begin{equation} \\phi_{k,w} = { n^{(w)}_{k} + \\beta_{w} \\over \\sum_{w=1}^{W} n^{(w)}_{k} + \\beta_{w}} \\tag{6.12} \\end{equation} \\] The topic distribution in each document is calcuated using Equation (6.13). \\[ \\begin{equation} \\theta_{d,k} = {n^{(k)}_{d} + \\alpha_{k} \\over \\sum_{k=1}^{K}n_{d}^{k} + \\alpha_{k}} \\tag{6.13} \\end{equation} \\] What if I don’t want to generate docuements. What if my goal is to infer what topics are present in each document and what words belong to each topic? This is were LDA for inference comes into play. Before going through any derivations of how we infer the document topic distributions and the word distributions of each topic, I want to go over the process of inference more generally. The General Idea of the Inference Process Initialization: Randomly select a topic for each word in each document from a multinomial distribution. Gibbs Sampling: For i iterations For document d in documents: For each word in document d: assign a topic to the current word based on probability of the topic given the topic of all other words (except the current word) as shown in Equation (6.11) 6.3 Animal Farm - Code Example Now let’s revisit the animal example from the first section of the book and break down what we see. This time we will also be taking a look at the code used to generate the example documents as well as the inference code. rm(list = ls()) library(MCMCpack) library(tidyverse) library(Rcpp) library(knitr) library(kableExtra) library(lsa) get_topic &lt;- function(k){ which(rmultinom(1,size = 1,rep(1/k,k))[,1] == 1) } get_word &lt;- function(theta, phi){ topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- which(rmultinom(1,1,phi[topic, ])==1) return(c(new_word, topic)) } cppFunction( &#39;List gibbsLda( NumericVector topic, NumericVector doc_id, NumericVector word, NumericMatrix n_doc_topic_count,NumericMatrix n_topic_term_count, NumericVector n_topic_sum, NumericVector n_doc_word_count){ int alpha = 1; int beta = 1; int cs_topic,cs_doc, cs_word, new_topic; int n_topics = max(topic)+1; int vocab_length = n_topic_term_count.ncol(); double p_sum = 0,num_doc, denom_doc, denom_term, num_term; NumericVector p_new(n_topics); IntegerVector topic_sample(n_topics); for (int iter = 0; iter &lt; 100; iter++){ for (int j = 0; j &lt; word.size(); ++j){ // change values outside of function to prevent confusion cs_topic = topic[j]; cs_doc = doc_id[j]; cs_word = word[j]; // decrement counts n_doc_topic_count(cs_doc,cs_topic) = n_doc_topic_count(cs_doc,cs_topic) - 1; n_topic_term_count(cs_topic , cs_word) = n_topic_term_count(cs_topic , cs_word) - 1; n_topic_sum[cs_topic] = n_topic_sum[cs_topic] -1; // get probability for each topic, select topic with highest prob for(int tpc = 0; tpc &lt; n_topics; tpc++){ // word cs_word topic tpc + beta num_term = n_topic_term_count(tpc, cs_word) + beta; // sum of all word counts w/ topic tpc + vocab length*beta denom_term = n_topic_sum[tpc] + vocab_length*beta; // count of topic tpc in cs_doc + alpha num_doc = n_doc_topic_count(cs_doc,tpc) + alpha; // total word count in cs_doc + n_topics*alpha denom_doc = n_doc_word_count[cs_doc] + n_topics*alpha; p_new[tpc] = (num_term/denom_term) * (num_doc/denom_doc); } // normalize the posteriors p_sum = std::accumulate(p_new.begin(), p_new.end(), 0.0); for(int tpc = 0; tpc &lt; n_topics; tpc++){ p_new[tpc] = p_new[tpc]/p_sum; } // sample new topic based on the posterior distribution R::rmultinom(1, p_new.begin(), n_topics, topic_sample.begin()); for(int tpc = 0; tpc &lt; n_topics; tpc++){ if(topic_sample[tpc]==1){ new_topic = tpc; } } // print(new_topic) // update counts n_doc_topic_count(cs_doc,new_topic) = n_doc_topic_count(cs_doc,new_topic) + 1; n_topic_term_count(new_topic , cs_word) = n_topic_term_count(new_topic , cs_word) + 1; n_topic_sum[new_topic] = n_topic_sum[new_topic] + 1; // update current_state topic[j] = new_topic; } } return List::create( n_topic_term_count, n_doc_topic_count); } &#39;) # 3 topics - land sea &amp; air # birds and amphibious have cross over # fish - sea 100 # land animals - 100 land beta &lt;- 1 k &lt;- 3 # number of topics M &lt;- 100 # let&#39;s create 10 documents alphas &lt;- rep(1,k) # topic document dirichlet parameters xi &lt;- 100 # average document length N &lt;- rpois(M, xi) #words in each document # whale1, whale2, FISH1, FISH2,OCTO sea_animals &lt;- c(&#39;\\U1F40B&#39;, &#39;\\U1F433&#39;,&#39;\\U1F41F&#39;, &#39;\\U1F420&#39;, &#39;\\U1F419&#39;) # crab, alligator, TURTLE,SNAKE amphibious &lt;- c(&#39;\\U1F980&#39;, &#39;\\U1F40A&#39;, &#39;\\U1F422&#39;, &#39;\\U1F40D&#39;) # CHICKEN, TURKEY, DUCK, PENGUIN birds &lt;- c(&#39;\\U1F413&#39;,&#39;\\U1F983&#39;,&#39;\\U1F426&#39;,&#39;\\U1F427&#39;) # SQUIRREL, ELEPHANT, COW, RAM, CAMEL land_animals&lt;- c(&#39;\\U1F43F&#39;,&#39;\\U1F418&#39;,&#39;\\U1F402&#39;,&#39;\\U1F411&#39;,&#39;\\U1F42A&#39;) vocab &lt;- c(sea_animals, amphibious, birds, land_animals) # equal probability 1/18 # 0 - animals that are not possible # 1 - for shared # 4 - non-shared shared &lt;- 2 non_shared &lt;- 4 not_present &lt;- 0 land_phi &lt;- c(rep(not_present, length(sea_animals)), rep(shared, length(amphibious)), rep(non_shared, 2), # turkey and chicken can&#39;t fly rep(shared, 2), # regular bird and pengiun rep(non_shared, length(land_animals))) land_phi &lt;- land_phi/sum(land_phi) sea_phi &lt;- c(rep(non_shared, length(sea_animals)), rep(shared, length(amphibious)), rep(not_present, 2), # turkey and chicken can&#39;t fly rep(shared, 2), # regular bird and pengiun rep(not_present, length(land_animals))) sea_phi &lt;- sea_phi/sum(sea_phi) air_phi &lt;- c(rep(not_present, length(sea_animals)), rep(not_present, length(amphibious)), rep(not_present, 2), # turkey and chicken can&#39;t fly non_shared, # regular bird not_present, # penguins can&#39;t fly rep(not_present, length(land_animals))) air_phi &lt;- air_phi/sum(air_phi) # calculate topic word distributions phi &lt;- matrix(c(land_phi, sea_phi, air_phi), nrow = k, ncol = length(vocab), byrow = TRUE, dimnames = list(c(&#39;land&#39;, &#39;sea&#39;, &#39;air&#39;))) theta_samples &lt;- rdirichlet(M, alphas) thetas &lt;- theta_samples[rep(1:nrow(theta_samples), times = N), ] new_words &lt;- t(apply(thetas, 1, function(x) get_word(x,phi))) ds &lt;-tibble(doc_id = rep(1:length(N), times = N), word = new_words[,1], topic = new_words[,2], theta_a = thetas[,1], theta_b = thetas[,2], theta_c = thetas[,3] ) ds %&gt;% filter(doc_id &lt; 3) %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(vocab[word], collapse = &#39; &#39;) ) %&gt;% kable(col.names = c(&#39;Document&#39;, &#39;Animals&#39;), caption =&quot;Animals at the First Two Locations&quot;) Table 6.1: Animals at the First Two Locations Document Animals 1 🐑 🐋 🐂 🐓 🐑 🦃 🦃 🐪 🦃 🐓 🐑 🐂 🐋 🐪 🐢 🐂 🐑 🐓 🐍 🐑 🐟 🐍 🐙 🐿 🐪 🐘 🐑 🐙 🦃 🐘 🐑 🐂 🐂 🐊 🐂 🐘 🐿 🦃 🦃 🐧 🐪 🐍 🐿 🐘 🐍 🐂 🐙 🐦 🐂 🐟 🐍 🐠 🐑 🦃 🐓 🦃 🐂 🐘 🐠 🐓 🐢 🐘 🦃 🐙 🐘 🐍 🐢 🐑 🐍 🐘 🐦 🐧 🐙 🐦 🐂 🐋 🐘 🐠 🐓 🐿 🐘 🐿 🦀 🐓 🐿 🐊 🐓 🦀 🐂 🐪 🐓 🐠 🐪 🐦 🐙 🐠 🐘 🦃 🐘 🐑 🐊 🐑 🐳 2 🐠 🐟 🐋 🐋 🐢 🐳 🦀 🐦 🐘 🐦 🐓 🐦 🐦 🦃 🐦 🐦 🐂 🦀 🐢 🦀 🐊 🐿 🦃 🐳 🐦 🐠 🐪 🐍 🐠 🐪 🐧 🐂 🐿 🐋 🐦 🐦 🐦 🐿 🐋 🐿 🐠 🐦 🐳 🐂 🐦 🐙 🐟 🐑 🐪 🐪 🦀 🐑 🐊 🐦 🐦 🐿 🦃 🐙 🐿 🦃 🐓 🐊 🐦 🐢 🐙 🐊 🦀 🐙 🐍 🐓 🐍 🐳 🐋 🐙 🐧 🐢 🐋 🐘 🐑 🐍 🐑 🐳 🐦 🐠 🐂 🐧 🐙 🐍 The habitat (topic) distributions for the first couple of documents: Table 6.2: Distribution of Habitats in the First Two Locations Document Land Sea Air 1 0.8168897 0.1688578 0.0142525 2 0.3860697 0.4442854 0.1696449 With the help of LDA we can go through all of our documents and estimate the topic/word distributions and the topic/document distributions. This is our estimated values and our resulting values: ######### Inference ############### current_state &lt;- ds %&gt;% dplyr::select(doc_id, word, topic) current_state$topic &lt;- NA t &lt;- length(unique(current_state$word)) # n_doc_topic_count n_doc_topic_count &lt;- matrix(0, nrow = M, ncol = k) # document_topic_sum n_doc_topic_sum &lt;- rep(0,M) # topic_term_count n_topic_term_count &lt;- matrix(0, nrow = k, ncol = t) # colnames(n_topic_term_count) &lt;- unique(current_state$word) # topic_term_sum n_topic_sum &lt;- rep(0,k) p &lt;- rep(0, k) # initialize topics current_state$topic &lt;- replicate(nrow(current_state),get_topic(k)) # get word, topic, and document counts (used during inference process) n_doc_topic_count &lt;- current_state %&gt;% group_by(doc_id, topic) %&gt;% summarise( count = n() ) %&gt;% spread(key = topic, value = count) %&gt;% as.matrix() n_topic_sum &lt;- current_state %&gt;% group_by(topic) %&gt;% summarise( count = n() ) %&gt;% select(count) %&gt;% as.matrix() %&gt;% as.vector() n_topic_term_count &lt;- current_state %&gt;% group_by(topic, word) %&gt;% summarise( count = n() ) %&gt;% spread(word, count) %&gt;% as.matrix() # minus 1 in, add 1 out lda_counts &lt;- gibbsLda( current_state$topic-1 , current_state$doc_id-1, current_state$word-1, n_doc_topic_count[,-1], n_topic_term_count[,-1], n_topic_sum, N) # calculate estimates for phi and theta # phi - row apply to lda_counts[[1]] # rewrite this function and normalize by row so that they sum to 1 phi_est &lt;- apply(lda_counts[[1]], 1, function(x) (x + beta)/(sum(x)+length(vocab)*beta) ) rownames(phi_est) &lt;- vocab colnames(phi) &lt;- vocab theta_est &lt;- apply(lda_counts[[2]],2, function(x)(x+alphas[1])/(sum(x) + k*alphas[1])) theta_est &lt;- t(apply(theta_est, 1, function(x) x/sum(x))) # rewrite this function and normalize by row so that they sum to 1 phi_est &lt;- apply(lda_counts[[1]], 1, function(x) (x + beta)/(sum(x)+length(vocab)*beta) ) rownames(phi_est) &lt;- vocab colnames(phi) &lt;- vocab theta_est &lt;- apply(lda_counts[[2]],2, function(x)(x+alphas[1])/(sum(x) + k*alphas[1])) theta_est &lt;- t(apply(theta_est, 1, function(x) x/sum(x))) colnames(theta_samples) &lt;- c(&#39;land&#39;, &#39;sea&#39;, &#39;air&#39;) vector_angles &lt;- cosine(cbind(theta_samples,theta_est))[4:6, 1:3] estimated_topic_names &lt;- apply(vector_angles, 1, function(x)colnames(vector_angles)[which.max(x)]) phi_table &lt;- as.tibble(t(round(phi,2))[,estimated_topic_names]) phi_table &lt;- cbind(phi_table, as.tibble(round(phi_est, 2))) # names(theta_table)[4:6] &lt;- paste0(estimated_topic_names, &#39; estimated&#39;) # theta_table &lt;- theta_table[, c(4,1,5,2,6,3)] names(phi_table)[4:6] &lt;- paste0(estimated_topic_names, &#39; estimated&#39;) phi_table &lt;- phi_table[, c(4,1,5,2,6,3)] row.names(phi_table) &lt;- colnames(phi) kable(round(phi_table, 2), caption = &#39;True and Estimated Word Distribution for Each Topic&#39;) Table 6.3: True and Estimated Word Distribution for Each Topic air estimated air land estimated land sea estimated sea 🐋 0.00 0 0.00 0.00 0.12 0.12 🐳 0.00 0 0.02 0.00 0.12 0.12 🐟 0.00 0 0.01 0.00 0.12 0.12 🐠 0.00 0 0.01 0.00 0.12 0.12 🐙 0.00 0 0.01 0.00 0.13 0.12 🦀 0.00 0 0.05 0.05 0.06 0.06 🐊 0.01 0 0.04 0.05 0.06 0.06 🐢 0.00 0 0.05 0.05 0.07 0.06 🐍 0.01 0 0.06 0.05 0.05 0.06 🐓 0.01 0 0.09 0.10 0.00 0.00 🦃 0.00 0 0.11 0.10 0.00 0.00 🐦 0.92 1 0.01 0.05 0.08 0.06 🐧 0.01 0 0.05 0.05 0.06 0.06 🐿 0.01 0 0.09 0.10 0.01 0.00 🐘 0.00 0 0.10 0.10 0.00 0.00 🐂 0.00 0 0.11 0.10 0.00 0.00 🐑 0.00 0 0.10 0.10 0.00 0.00 🐪 0.00 0 0.11 0.10 0.00 0.00 The document topic mixture estimates are shown below for the first 5 documents: Table 6.4: The Estimated Topic Distributions for the First 5 Documents Location air estimated air land estimated land sea estimated sea 1 0.04 0.01 0.90 0.82 0.06 0.17 2 0.19 0.17 0.48 0.39 0.33 0.44 3 0.31 0.40 0.13 0.18 0.56 0.42 4 0.82 0.64 0.17 0.33 0.01 0.03 5 0.31 0.29 0.31 0.41 0.39 0.30 6 0.35 0.32 0.11 0.15 0.54 0.53 "],
["references.html", "References", " References "]
]
