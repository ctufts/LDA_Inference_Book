[
["index.html", "LDA Tutorial Preface", " LDA Tutorial Chris Tufts Preface The purpose of this book is to provide a step by step guide of LDA utilizing Gibbs Sampling. It is heavily inspired by the Gregor Heinrich’s Parameter Estimation for Text Analysis(Heinrich 2008). "],
["background.html", "Background", " Background I’ve often found that resources covering LDA are either hard to follow, do to a heavy reliance on calculus derivations, or the resources are extremely high level so you get the idea of what LDA accomplishes, but you never fully grasp how it works. The book focuses on LDA for inference via Gibbs Sampling. To aid in understanding both LDA and Gibbs sampling all probability distributions used in LDA will be reviewed along with a variety of different approaches for parameter estimation. Following the introduction of these components, LDA will be presented as a generative model. This will lay the groundwork for understanding how LDA can be used for inference of topics in a corpus. "],
["layout-of-book.html", "Layout of Book", " Layout of Book What is LDA? - High level overview Parameter Estimation Methods (general overview using Bernoulli distribution) - ML, MAP, Bayesian Inference, Gibbs Sampling Multinomial Distribution - Explains the relationship of Bernoulli to Multinomial, then goes into Gibbs Sampling Word/Document Structures - Bag of Words, Word Document Matrix LDA - A Generative Model LDA - Inference "],
["what-is-lda.html", "1 What is LDA? 1.1 Animal Generator 1.2 Inference", " 1 What is LDA? Latent Dirichlet Allocation (LDA) is a generative probablistic model for collections of discrete data developed by Blei, Ng, and Jordan. (Blei, Ng, and Jordan 2003) One of the most common uses of LDA is for modeling collections of text. The general idea is that each document is generated from a mixture of topics and each of those topics is a mixture of words. In regards to the model name, you can think of it as follows: Latent: Topic structures in a document are ‘latent’ meaning they are hidden structures in the text. Dirichlet: The Dirichlet distribution determines the mixture proportions of the topics in the documents and the words in each topic. Allocation: Allocation of words to a given topic. So to review: we have latent structures in a corpus (topics), taking into account Dirichlet priors for the word and topic distributions, to allocate words to a given topic and topics to a given document. Throughout this book I will work through all of the building blocks which make LDA possible, but to help get an understanding of what LDA is and why it is useful, I will offer a quick example first. 1.1 Animal Generator The majority of this book is about words, topics, and documents, but lets start with something a bit different: animals and where they live. One of the ways you can classify animals is by where they spend the majority of their time - land, air, sea. Obviously there are some animals that only dwell in one place; a cow only lives on land and a fish only lives in the sea. However, there are other animals, such as some birds, that split their time between land, sea, and air. You are probably asking yourself ‘where is he going with this?’. We can think of land, air, and sea as topics that contain a distribution of animals. In this case we can equate animals with words. For example, on land I am much more likely to see a cow than a whale, but in the sea it would be the reverse. If I quantify these probabilities into a distribution over all the animals (words) for each type of habitat (land,sea, air - topics) I can use them to generate sets of animals (words) to populate a given location (document) which may contain a mix of land, sea, and air (topics). So let’s move on to generating a specific location. We know that different locations will vary in terms of which habitats are present. For example, a beach contains land, sea, and air, but some areas inland may only contain air and land like a desert. We can define the mixture of these types of habitats in each location. For example, a beach is 1/3 land, 1/3 sea, and 1/3 air. We can think of the beach as a single document. To review: a given location (document) contains a mixture of land, air, and sea (topics) and each of those contain different mixtures of animals (words). Let’s work through some examples using our animals and habitats. The examples provided in this chapter are oversimplified so that we can get a general idea how LDA works. The rest of the book will handle all the nuts and bolts of the model, but for now let’s try and get a handle on how this works. We’ll start by generating a beach location with 1/3 land animals, 1/3 sea animals, and 1/3 air animals. Below you can see our collection of animals and their probability in each topic. Note that some animals have zero probabilities in a given topic, i.e. a cow is never in the ocean, where some have higher probabilities than others; a crab is in the sea sometimes, but a fish is always in the sea. You may notice that there is only 1 animal in the air category. There are several birds, but only 1 of them is cabable of flight in our vocabulary. (NOTE: These are the probability of a word given the topic and therefore the probabilities of each habitat (column) sum to 1.) Table 1.1: Animal Distributions in Each Habitat vocab land sea air 🐋 0.00 0.12 0 🐳 0.00 0.12 0 🐟 0.00 0.12 0 🐠 0.00 0.12 0 🐙 0.00 0.12 0 🦀 0.05 0.06 0 🐊 0.05 0.06 0 🐢 0.05 0.06 0 🐍 0.05 0.06 0 🐓 0.10 0.00 0 🦃 0.10 0.00 0 🐦 0.05 0.06 1 🐧 0.05 0.06 0 🐿 0.10 0.00 0 🐘 0.10 0.00 0 🐂 0.10 0.00 0 🐑 0.10 0.00 0 🐪 0.10 0.00 0 To generate a beach (document) based off the description we would use those probabilities in a straightforward manner: words_per_topic &lt;- 3 equal_doc &lt;- c(vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$land, replace = T)], vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$sea, replace = T)], vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$air, replace = T)]) cat(equal_doc) ## 🦃 🐂 🐧 🐳 🐠 🐟 🐦 🐦 🐦 NOTE: In the above example the topic mixtures are static and equal, so each habitat (topic) contributes 3 animals to the beach. Ok, now let’s make an ocean setting. In the case of the ocean we only have sea and air present, so our topic distribution in the document would be %50 sea, %50 air, and %0 land. words_per_topic &lt;- 3 ocean_doc &lt;- c(vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$sea, replace = T)], vocab[sample.int(length(vocab),words_per_topic, prob=phi_ds$air, replace = T)]) cat(ocean_doc) ## 🐟 🐙 🐍 🐦 🐦 🐦 NOTE: In the example above only the air and land contribute to the ocean location. Therefore they both contribute an equal number of animals to the location. 1.1.1 Generating the Mixtures It is important to note the examples above use static word and topic mixtures that were predetermined, but these mixtures could just as easily be created by sampling from a Dirichlet distribution. This is an important distinction to make as it is the foundation of how we can use LDA to infer topic structures in our documents. The Dirichlet distribution and it’s role in LDA is discussed in detail in the coming chapters. 1.2 Inference We have seen that we can generate collections of animals that are representative of the given location. What if we have thousands of locations and we want to know the mixture of land, air, and sea that are present? And what if we had no idea where each animal spends its time? LDA allows us to infer both of these peices of information. Similar to the locations (documents) generated above, I will create 100 random documents with varying length and various habitat mixtures. Table 1.2: Animals at the First Two Locations Document Animals 1 🐦 🐦 🐘 🐦 🐦 🐦 🐦 🐦 🐦 🐢 🐦 🐓 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐧 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐿 🐂 🐦 🐦 🐦 🐠 🐦 🐦 🐦 🐦 🐦 🐠 🐳 🐦 🐊 🐦 🐳 🐦 🐠 🐦 🐿 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐠 🐋 🐦 🐦 🐦 🐦 🐦 🐦 🐢 🐍 🐿 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐋 🐦 🐦 🐢 🐦 🐦 🐦 🐦 🐦 🐦 🦀 🦃 🐿 🐦 🐙 🐦 🐦 🐦 🐦 🐦 🐦 🐦 🐦 2 🐟 🐂 🐿 🐦 🐦 🐦 🐢 🐦 🐦 🐦 🐦 🐿 🐋 🐂 🐦 🐪 🐠 🐦 🐦 🐟 🦃 🐍 🐦 🐦 🐧 🐦 🐂 🐠 🐠 🐋 🐦 🐦 🐳 🐋 🐦 🐦 🐦 🐦 🐿 🐦 🐦 🐦 🐦 🐦 🐦 🐧 🐦 🐦 🐓 🐙 🐦 🐦 🦀 🐠 🐢 🦃 🐳 🐍 🐑 🐦 🐦 🐿 🐊 🐦 🐦 🐦 🐍 🐦 🐙 🐦 🐠 🐟 🐦 🐳 🐳 🐳 🐧 🐦 🐦 🐪 🐟 🐳 🐦 🐦 🐙 🐦 🐦 🐦 The topic word distributions shown in Table 1.1 were used to generate our sample documents. The true habitat (topic) mixtures used to generate the first couple of documents are shown in Table 1.3: Table 1.3: Distribution of Habitats in the First Two Locations Document Land Sea Air 1 0.0677780 0.1446454 0.7875767 2 0.2292364 0.3021864 0.4685772 With the help of LDA we can go through all of our documents and estimate the topic/word distributions and the topic/document distributions. The estimated topic word distributions are shown in Table 1.4 and can be compared to the true distributions in Table 1.5. Table 1.4: Estimated word distribution for each topic air sea land 🐋 0.00 0.13 0.01 🐳 0.00 0.13 0.00 🐟 0.00 0.12 0.00 🐠 0.00 0.13 0.00 🐙 0.01 0.12 0.00 🦀 0.00 0.07 0.05 🐊 0.00 0.07 0.05 🐢 0.01 0.05 0.05 🐍 0.01 0.06 0.05 🐓 0.00 0.00 0.11 🦃 0.00 0.00 0.11 🐦 0.94 0.04 0.01 🐧 0.00 0.06 0.06 🐿 0.00 0.01 0.11 🐘 0.00 0.00 0.11 🐂 0.01 0.01 0.09 🐑 0.00 0.00 0.10 🐪 0.00 0.00 0.10 Table 1.5: The word distribution for each topic used to build the documents air sea land 🐋 0 0.12 0.00 🐳 0 0.12 0.00 🐟 0 0.12 0.00 🐠 0 0.12 0.00 🐙 0 0.12 0.00 🦀 0 0.06 0.05 🐊 0 0.06 0.05 🐢 0 0.06 0.05 🐍 0 0.06 0.05 🐓 0 0.00 0.10 🦃 0 0.00 0.10 🐦 1 0.06 0.05 🐧 0 0.06 0.05 🐿 0 0.00 0.10 🐘 0 0.00 0.10 🐂 0 0.00 0.10 🐑 0 0.00 0.10 🐪 0 0.00 0.10 The document topic mixture estimates are shown below for the first 5 documents: Table 1.6: The Estimated Topic Distributions for the First 5 Documents air sea land 0.75 0.14 0.11 0.47 0.36 0.17 0.15 0.06 0.79 0.57 0.11 0.32 0.07 0.61 0.32 0.24 0.14 0.62 Here are our real mixtures for comparison: Table 1.7: The Real Topic Distributions for the First 5 Documents air sea land 0.79 0.14 0.07 0.47 0.30 0.23 0.22 0.07 0.71 0.66 0.10 0.23 0.06 0.64 0.30 0.21 0.14 0.65 "],
["parameter-estimation.html", "2 Parameter Estimation 2.1 Distributions 2.2 Inference: The Building Blocks 2.3 Maximum Likelihood 2.4 Maximum a Posteriori (MAP) 2.5 Bayesian Inference", " 2 Parameter Estimation LDA is a generative probabilistic model, so to understand exactly how this works we need to understand the underlying probability distibutions. In this chapter we will focus on the Bernoulli distribution and the Beta distribution. Both of these distributions are very closely related to (and also special cases of) the multinomial and Dirichlet distributions utilized by LDA, but they are a bit easier to comprehend. Once we have made our way through Bernoulli and beta, the following chapter will go into detail about multinomial and Dirichlet distributions and how all these peices are connected. Throughout the chapter I’m going to build off of a simple example - a single coin flip. Let’s begin. 2.1 Distributions 2.1.1 Bernoulli When you flip a coin you get either heads or tails as an outcome (barring the possibility it lands on it’s side). This single coin flip is an example of a Bernoulli trial and we can use the Bernoulli distribution to calculate the probability of either outcome. Any single trial with two possible outcomes can be modeled as a Bernoulli trial: team wins/loses, pitch is a strike/ball, coin comes up heads or tails, etc. 2.1.1.1 Bernoulli: A Special Case of the Binomial Distribution You will often see Bernoulli distribution mentioned as a special case of the Binomial distribution. The binomial model consists of n Bernoulli trials, where each trial is independent and the probability of success does not change between trials.(Kerns 2010). The Bernoulli distribution is the case of a single trial or n=1. NOTE: I will use the term success interchangably with the term heads when describing bernoulli distribution. In reality success could be tails if you choose to define it that way. To clarify, if I want to calculate the probability of getting heads on a single coin flip I will use a bernoulli distribution. However, if I want to know the probability of getting 2 heads (or more) in a row, this is where the binomial distribution comes in. For our purposes we are only concerned about the outcome of a single coin flip and will therefore stick to the Bernoulli distribution. Figure 2.1: Bernoulli and Binomial Distributions 2.1.1.2 Bernoulli - Distribution Notation The probability mass function of the bernoulli distribution is shown in Equation (2.1). \\[ \\begin{equation} f_{x}(x)=P(X=x)=\\theta^{x}(1-\\theta)^{1-x}, \\hspace{1cm} x = \\{0,1\\} \\tag{2.1} \\end{equation} \\] The only parameter of the bernoulli distribution is \\(\\theta\\), which defines the probability of success during a bernoulli trial. The value of x is 0 for a failure and 1 for a success. In a practical example you can think of this as 0 for tails and 1 for heads during a coin flip. In Equation (2.2) the value of \\(\\theta\\) is set to 0.7. We can see the probability of getting a success is 0.7, while the probability of failure is 0.3. \\[ \\begin{equation} \\begin{aligned} P(X=1)&amp;=\\theta^{1}(1-\\theta)^{1-1}, \\hspace{1cm} \\theta=0.7 \\\\ P(X=1)&amp;=0.7*1=0.7 \\\\\\\\ P(X=0)&amp;=0.7^{0}(1-0.7)^{1-0}\\\\ P(X=0)&amp;=0.3 \\end{aligned} \\tag{2.2} \\end{equation} \\] 2.1.2 Beta Distribution The beta distribution can be thought of as a probability distribution of distributions(Robinson 2014). We know the bernoulli distribution has one parameter, \\(\\theta\\). We can use the beta distribution to determine the probability of a specific value of \\(\\theta\\) based on prior information, in our case previous coin flips. If you flip a coin 2 times resulting in 1 heads and 1 tails how sure are you that the coin is fair? Probably not all that sure, right? But what if you flipped the coin 200 times and it resulted in 100 heads and 100 tails? You would be much more confident that the coin is fair. This is the basis of the beta distribution. The beta distribution has 2 shape parameters, \\(\\alpha\\) and \\(\\beta\\). These can be though of as the results from the coin flips we just talked about. Below the probability density for different values of \\(\\theta\\) is displayed based on different values of \\(\\alpha\\) and \\(\\beta\\). In general, the higher the value of \\(\\alpha\\) and \\(\\beta\\) the narrower the density curve is. This makes sense with our thought example above, the more information (coin flip results) we have, the more confident we are in our coin’s bias (i.e. is it fair, head heavy, etc.). library(tidyverse) a &lt;- c(1, 10, 100) b &lt;- c(1, 10, 100) params &lt;- cbind(a,b) ds &lt;- NULL n &lt;- seq(0,1,0.01) for(i in 1:nrow(params)){ ds &lt;- rbind(data.frame(x = n, y = dbeta(n, params[i,1], params[i,2]), parameters = paste0(&quot;\\U03B1 = &quot;,params[i,1], &quot;, \\U03B2 = &quot;, params[i,2])), ds) } ggplot(ds, aes(x = x, y = y, color=parameters)) + geom_line() + labs(x = &#39;\\U03B8&#39;, y = &#39;Probability Density&#39;) + scale_color_discrete(name=NULL) + theme_minimal() Figure 2.2: Beta Distribution What about the cases where \\(\\alpha\\) and \\(\\beta\\) are not equal or close to equal? Well in those cases you would probably assume a bit of skew in the distribution, i.e. your coin may be biased toward head or tails as shown below. a &lt;- c(8, 2) b &lt;- c(2, 8) params &lt;- cbind(a,b) ds &lt;- NULL n &lt;- seq(0,1,0.01) for(i in 1:nrow(params)){ ds &lt;- rbind(data.frame(x = n, y = dbeta(n, params[i,1], params[i,2]), parameters = paste0(&quot;\\U03B1 = &quot;,params[i,1], &quot;, \\U03B2 = &quot;, params[i,2])), ds) } ggplot(ds, aes(x = x, y = y, color=parameters)) + geom_line() + labs(x = &#39;\\U03B8&#39;, y = &#39;Probability Density&#39;) + scale_color_manual(name=NULL, values = c(&quot;#7A99AC&quot;, &quot;#E4002B&quot;)) + theme_minimal() Figure 2.3: Beta Distribution - Skewed The probability distribution function for the beta distribution can be found in Equation (2.3). \\[ \\begin{equation} f(\\theta;\\alpha,\\beta) ={{\\theta^{(\\alpha-1)}(1-\\theta)^{(\\beta-1)}}\\over B(\\alpha,\\beta)} \\tag{2.3} \\end{equation} \\] Quick Note: The Beta function, B is the ratio of the product of the Gamma function, \\(\\Gamma\\), of each parameter divided by the Gamma function of the sum of the parameters. The Beta function is not the same as the beta distribution. The Beta function is shown below along with the Gamma function, which is used in the Beta function. \\[ \\begin{equation} \\beta(a,b) = {\\Gamma(a)\\Gamma(b) \\over{\\Gamma(a+b)}} \\tag{2.4} \\end{equation} \\] The Gamma function is the factorial of the parameter minus 1. \\[ \\begin{equation} \\Gamma(a) = (a-1)! \\tag{2.5} \\end{equation} \\] 2.2 Inference: The Building Blocks Equation (2.6) is a fundamental to understanding parameter estimation and inference. \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} = {\\overbrace{p(D|\\theta)}^{likelihood} \\overbrace{p(\\theta)}^{prior} \\over \\underbrace{p(D)}_{evidence}} \\tag{2.6} \\end{equation} \\] The 4 components are: Prior: The probability of the parameter(s). Defines our prior beliefs of the parameter. Do we believe to a good degree of certainty that the coin is fair? Maybe take a step back and ask yourself ‘do I trust the manufacturer of this coin?’. If this manufacturer has always had great quality (i.e. fair coins) then you would have some confidence that your case is no different. Posterior: The probability of the parameter given the evidence. The only way to know this value is to already have the evidence. However we can estimate the posterior in various ways. Think of it this way, given 100 coin flips with 47 heads and 53 tails what is the probability that theta is 0.5 (coin is fair)? Likelihood: The probability of the evidence given the parameter. Given that we know the coin is fair (theta = 0.5) what is the probability of having 47 heads out of 100 flips? Evidence: The probability of all possible outcomes. Probability of 1/100 heads, 2/100 heads, …, 100/100 heads. Conditioning your brain for LDA : We are starting with a coin flip, but the eventual goal is to link this back to words appearing in a document. Try to keep in mind that we think of a word similar to the outcome of a coin: a word exists in the document (heads!) or a word doesn’t exist in the document (tails!). 2.3 Maximum Likelihood The simplest method of parameter estimation is the maximum likelihood method. Effectively we calculate the parameter that maximizes the likelihood. \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} = {\\overbrace{\\bf \\Large p(D|\\theta)}^{\\bf \\Large LIKELIHOOD} \\overbrace{p(\\theta)}^{prior} \\over \\underbrace{p(D)}_{evidence}} \\tag{2.7} \\end{equation} \\] Let’s first discuss what the likelihood is. The likelihood can be described as the probability of getting observed data given a specified value of the parameter, \\(\\theta\\). For example, let’s say I’ve flipped a coin 10 times and got 5 heads, 5 tails. Assuming the coin is fair, \\(\\theta\\) equals 0.5, what is the likelihood of observing 5 heads and 5 tails. To calculate the likelihood of a parameter given a single outcome we would use the probability mass function: \\[ \\begin{equation} P(X=x)=\\theta^{x}(1-\\theta)^{1-x}, \\hspace{1cm} x = \\{0,1\\} \\tag{2.8} \\end{equation} \\] Where an outcome of heads equal 1 and tails is 0. Now let’s say we have carried out the 10 flips as mentioned previously: \\[ \\begin{equation} \\begin{aligned} P(X_{1}=x_{1},X_{2}=x_{2},...,X_{10}=x_{10}) &amp;= \\prod\\limits_{n=1}^{10} \\theta^{x}(1-\\theta)^{1-x}\\\\ L(\\theta) &amp;= \\prod\\limits_{n=1}^{10} \\theta^{x}(1-\\theta)^{1-x} \\end{aligned} \\tag{2.9} \\end{equation} \\] What is shown in Equation (2.9) is the joint probability mass function. Each coin flip is independent so we calculate the product of the PMF’s for each trial. This is known as the likelihood function - the likelihood of a value of \\(\\theta\\) given our observed data. Back to the maximum likelihood…. Our goal is to find the value of \\(\\theta\\) which maximizes the likelihood of the observed data. To derive the maximum likelihood we start by taking the log of the likelihood, \\(\\mathcal{L}\\). \\[ \\begin{equation} \\begin{aligned} \\mathcal{L} &amp;= log \\prod\\limits_{n=1}^N \\theta^{x}(1-\\theta)^{1-x} \\\\\\\\ &amp;= \\sum\\limits_{n=1}^N log(\\theta^{x}(1-\\theta)^{1-x}) \\\\\\\\ &amp;= n^{(1)}log(\\theta) + n^{(0)}log(1-\\theta) \\end{aligned} \\tag{2.10} \\end{equation} \\] Differentiate with respect to \\(\\theta\\): \\[ \\begin{equation} {d\\mathcal{L} \\over d\\theta} = {n^{(1)}\\over \\theta} - {n^{(0)}\\over 1-\\theta} \\tag{2.11} \\end{equation} \\] Set it equal to zero and solve: \\[ \\begin{equation} \\begin{aligned} {n^{(1)}\\over \\theta} - {n^{(0)}\\over 1-\\theta} &amp;= 0 \\\\ \\\\ {n^{(1)}\\over\\theta} &amp;= {n^{(0)}\\over 1-\\theta} \\\\ \\\\ {n^{1} - \\theta n^{1}} &amp;= {\\theta n^{0}} \\\\ \\\\ n^{(1)} &amp;= \\theta(n^{(1)} + n^{0}) \\\\ \\\\ \\theta &amp;= {n^{(1)} \\over N} \\end{aligned} \\tag{2.12} \\end{equation} \\] The value of \\(\\theta\\) that maximizes the likelihood is the number of heads over the number of flips. heads = 1:10 flips = 10 ds &lt;- data.frame(heads, flips = rep(flips, length(heads))) ds$theta &lt;- ds$heads/ds$flips ggplot(ds, aes(x = heads, y = theta)) + geom_point(color =&#39;#1520c1&#39;, size = 3) + geom_linerange(aes(x=heads, y=NULL, ymax=theta, ymin=0)) + scale_x_continuous(breaks = seq(0,10,2), labels = seq(0,10,2)) + labs(y=&#39;\\U03B8&#39;, x=&quot;Number of Heads&quot;, title =&quot;ML Parameter Estimation: 10 Bernoulli Trials&quot;) + theme(plot.title = element_text(hjust = 0.5)) + theme_minimal() Figure 2.4: Bernoulli Maximum Likelihood 2.4 Maximum a Posteriori (MAP) MAP is similar to the method of maximum likelihood estimation, but it also let’s us include information about our prior beliefs. Unlike ML estimation, MAP estimates parameters by trying to maximize the posterior. \\[ \\begin{equation} \\theta_{MAP}=\\underset{\\theta}{\\operatorname{argmax}} P(\\theta|X) \\tag{2.13} \\end{equation} \\] \\[ \\begin{equation} \\require{enclose} \\theta_{MAP}=\\underset{\\theta}{\\operatorname{argmax}}{\\overbrace{p(D|\\theta)}^{likelihood} \\overbrace{p(\\theta)}^{prior} \\over \\underbrace{\\enclose{horizontalstrike}{p(D)}}_{evidence}} \\tag{2.14} \\end{equation} \\] The evidence term is dropped during the calculation of \\(\\theta_{MAP}\\) since it is not a function of \\(\\theta\\) and our only concern is maximizing the posterior based on \\(\\theta\\). Similar to calculating the Likelhood, the first step is to apply the log function to the remaining terms. \\[ \\begin{equation} \\begin{aligned} \\theta_{MAP} &amp;=\\underset{\\theta}{\\operatorname{argmax}}p(D|\\theta)p(\\theta) \\\\\\\\ &amp;= \\mathcal{L}(\\theta|D) + log(p(\\theta)) \\end{aligned} \\tag{2.15} \\end{equation} \\] We have already derived the log likelihood during our derivation of the maximum likelihood, so let’s now focus on the prior. The prior for the Bernoulli distribution is the Beta distribution and can be used to describe \\(p(\\theta)\\). The probability distribution function, PDF, for the Beta distribution is shown in Equation @ref\\tag{2.16}. \\[ \\begin{equation} p(\\theta|\\alpha,\\beta) = {\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\over{B(\\alpha, \\beta)}} \\tag{2.16} \\end{equation} \\] Plugging in the PDF of the beta distribution for the prior: $$ \\[\\begin{equation} \\begin{aligned} \\theta_{MAP}&amp;= \\mathcal{L}(\\theta|D) + log(p(\\theta)) \\\\\\\\ \\theta_{MAP}&amp;= n^{(1)}log(\\theta) + n^{(0)}log(1-\\theta) + log({\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\over{B(\\alpha, \\beta)}}) \\\\\\\\ \\theta_{MAP}&amp;= n^{(1)}log(\\theta) + n^{(0)}log(1-\\theta) + \\\\ &amp;\\quad log({\\theta^{\\alpha-1}) + log((1-\\theta)^{\\beta-1}) - log({B(\\alpha, \\beta)}}) \\\\\\\\ {d \\over d\\theta} \\mathcal{L}(\\theta|D) + log(p(\\theta)) &amp;= {n^{(1)}\\over \\theta} - {n^{(0)}\\over 1-\\theta} + {\\alpha - 1\\over\\theta} - {\\beta - 1 \\over 1-\\theta} \\\\\\\\ 0 &amp;= {n^{(1)}\\over \\theta} - {n^{(0)}\\over 1-\\theta} + {\\alpha - 1\\over\\theta} - {\\beta - 1 \\over 1-\\theta} \\\\\\\\ \\theta_{MAP}&amp;= {{n^{(1)} + \\alpha -1} \\over {n^{(1)} + n^{0} + \\alpha + \\beta - 2}} \\end{aligned} \\tag{2.17} \\end{equation}\\] $$ Now that we know how to calculate the parameter \\(\\theta\\) that maximizes the posterior, lets take a look at how choices of different priors and different number of observed trials effects our outcome. In the figure below we see MAP estimation of \\(\\theta\\) with a relatively uninformed prior and a small number of observed experiments (n=20). Uninformed means we are going to make a weak assumption about the prior. In more general terms, this means that we don’t have a strong intuition that our coin is fair or unfair. The Beta distribution has \\(\\alpha\\) and \\(\\beta\\) parameters of 2 resulting in the blue density curve shown below. NOTE: The terms weak and uninformed are often used interchangeably when referencing priors. The same goes for the terms strong and informed. # description: # ml will yeild the expected average and is not effected by the prior # map uses a weak assumption - uniform density for all values of theta # this results in a theta_map value very similar to the ml value n &lt;- 20 heads &lt;- 14 tails &lt;- 6 # ml ml_theta &lt;- heads/n # map B &lt;- 2 alpha &lt;- 2 map_theta &lt;- (heads + alpha - 1)/(heads + tails + alpha + B -2) possible_theta &lt;- seq(0,1,0.01) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) ggplot(beta_ds, aes(x = theta, y = density)) + geom_point(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.1, y=0.5, label= paste(&quot;\\U03B8[MAP]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;) + theme_minimal() Figure 2.5: MAP: Small number of experiments and uninformed prior In the next example we have the same number of samples, but we assume a much stronger prior. In particular, we assume the coin is most likely fair by selecting \\(\\alpha\\) and \\(\\beta\\) parameters of 100. These parameters represent a Beta distribution that is centered at 0.5 and is very dense around that point. We can see the resuting MAP estimate is much closer to 0.5, which is the value of \\(\\theta\\) where the Beta distribution is most dense. # description: # the strong assumption of a &#39;fair&#39; coin prior reduces the width of the # distribution, i.e. much higher probability density near theta (p) of 0.5 # This forces the MAP theta value to stay much closer to the prior due to the # small amount of observed evidence n &lt;- 20 heads &lt;- 14 tails &lt;- 6 # ml ml_theta &lt;- heads/n # map B &lt;- 100 alpha &lt;- 100 map_theta &lt;- (heads + alpha - 1)/(heads + tails + alpha + B -2) possible_theta &lt;- seq(0,1,0.001) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) ggplot(beta_ds, aes(x = theta, y = density)) + geom_line(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.1, y=11, label= paste(&quot;\\U03B8[MAP]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;, y = &#39;Density&#39;) + theme_minimal() Figure 2.6: MAP: Small number of experiments and informed prior What happens when we have a much larger number of observed experiments and an uninformed prior? In general, the MAP estimate gives us a value that is very close to a likelihood, heads divide by number of trials. # description # high number of observed samples (evidence) # weak prior - uniform # ml and map are close to one another - close... as expected n &lt;- 1000 heads &lt;- 723 tails &lt;- n-heads # ml ml_theta &lt;- heads/n # map B &lt;- 2 alpha &lt;- 2 map_theta &lt;- (heads + alpha - 1)/(heads + tails + alpha + B -2) possible_theta &lt;- seq(0,1,0.001) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) ggplot(beta_ds, aes(x = theta, y = density)) + geom_line(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.1, y=1.2, label= paste(&quot;\\U03B8[MAP]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;, y = &#39;Density&#39;) + theme_minimal() Figure 2.7: MAP: Large number of experiments and uninformed prior Now let’s assume a much stronger prior that the coin is fair while using the same number of experiments. Notice that when we use a larger number of experiments it overpowers the strong prior and gives us a very similar MAP estimate in comparison to the example with the uninformed prior and the same number of experiments. n &lt;- 1000 heads &lt;- 723 tails &lt;- n-heads # ml ml_theta &lt;- heads/n # map B &lt;- 100 alpha &lt;- 100 possible_theta &lt;- seq(0,1,0.001) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) map_theta &lt;- (heads + alpha - 1)/(heads + tails + alpha + B -2) ggplot(beta_ds, aes(x = theta, y = density)) + geom_line(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.1, y=8.2, label= paste(&quot;\\U03B8[MAP]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;, y = &#39;Density&#39;) + theme_minimal() Figure 2.8: MAP: Large number of experiments and informed prior In summary: The stronger your prior assumptions are, the more observations you will need to overcome an incorrect prior. Stronger prior - MAP estimate moves toward most dense area of prior distribution Weaker prior - MAP looks more like a maximum likelihood 2.5 Bayesian Inference Another option we have for estimating parameters is to estimate the posterior of the distribution via Bayesian inference. In MAP estimation we included prior assumptions as part of our calculation. We are going to do the same via Bayesian Inference however instead of a point estimate of \\(\\theta\\), we will be calculating the posterior distribution over all possible values of \\(\\theta\\). From this we can take the expected value of \\(\\theta\\) as our estimated parameter. In the case of the coin flip, Bayesian inference can be solved analytically. We have reviewed the likelihood and the prior, but when estimating the posterior we need to include the evidence term. This is somewhat tricky. \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} = { p(D|\\theta) p(\\theta) \\over \\underbrace{p(D)}_{evidence}} \\tag{2.18} \\end{equation} \\] \\[ \\begin{equation} p(D) = \\int_{\\theta}p(D|\\theta)p(\\theta)d\\theta \\tag{2.19} \\end{equation} \\] \\[ \\begin{equation} {p(\\theta|z,N)} = {\\overbrace{\\theta^z(1-\\theta)^{(N-z)}}^{likelihood} \\overbrace{{\\theta^{(a-1)}(1-\\theta)^{(b-1)}}\\over \\beta(a,b)}^{prior}\\over{\\underbrace{\\int_{0}^1 \\theta^z(1-\\theta)^{(N-z)}{{\\theta^{(a-1)}(1-\\theta)^{(b-1)}}\\over \\beta(a,b)}d\\theta}_{Evidence}}} \\tag{2.20} \\end{equation} \\] Note that the evidence term is actually a constant value since it is an integral over all values of \\(\\theta\\). The evidence term is used as a scaling factor to ensure our probabilities sum to 1. We will plug in a generic placeholder of our evidence value since we have established that it is a constant. \\[ \\begin{equation} \\begin{aligned} p(\\theta|z, N) &amp;= {\\theta^{z}(1-\\theta)^{(N-z)}{{\\theta^{(a-1)}(1-\\theta)^{(b-1)}}\\over \\beta(a,b)}\\over{C}} \\\\\\\\ &amp;= {\\theta^{(z+a-1)}(1-\\theta)^{(N-z+b-1)} \\over \\beta(z+a, N-z+b)}\\\\\\\\ &amp;= Beta(z+a, N-z+b) \\end{aligned} \\tag{2.21} \\end{equation} \\] Notice what happens in the second step of equation 18. I combine the Beta function term and the constant evidence term. The Beta function in this equation is also a constant value. Therefore when we combine the likelihood and prior we alter the Beta function’s parameters and drop the constant for the evidence term. This leaves us with a Beta distribution for our posterior that includse our observed experiments and our prior’s hyperparameters. ????????????? Possibly rephrase the above to specify that we are preserving the scaling of the posterior… ?????????????? To estimate \\(\\theta\\), we calculate the expected value of a Beta distribution as shown in Equation 19. \\[ \\begin{equation} E[\\theta]={\\alpha \\over \\alpha + \\beta} \\tag{2.22} \\end{equation} \\] Plugging in our parameters as per the derivation in Equation 18: \\[ \\begin{equation} \\begin{aligned} E[\\theta] &amp;= {z+a \\over {z+a + n-z+b}}\\\\\\\\ &amp;={z+a \\over {a+N+b}}\\\\\\\\ \\end{aligned} \\tag{2.23} \\end{equation} \\] n &lt;- 20 heads &lt;- 14 tails &lt;- n-heads # ml ml_theta &lt;- heads/n # map B &lt;- 2 alpha &lt;- 2 possible_theta &lt;- seq(0,1,0.001) beta_ds &lt;- data.frame(theta = possible_theta, density = dbeta(possible_theta, alpha,B)) map_theta &lt;- (heads + alpha)/(alpha + n + B) ggplot(beta_ds, aes(x = theta, y = density)) + geom_line(color=&#39;#7A99AC&#39;) + geom_vline(xintercept=map_theta, color = &#39;#ba0223&#39;) + annotate(&quot;text&quot;, x = map_theta + 0.08, y=1.4, label= paste(&quot;\\U03B8[BI]==&quot;, round(map_theta,2)), parse=T)+ labs(x=&#39;\\U03B8&#39;, y = &#39;Density&#39;) + theme_minimal() Figure 2.9: Bayesian Inference: Analytical Solution (n=20) Keep in mind Bayesian inference is effected by the selection of a prior and the number of observations in the same way MAP estimation is. 2.5.1 The Issue of Intractability The coin flip example solved via Bayesian inference was capable of being solved analytically. However in many cases of Bayesian inference this is not possible due to the intractability of solving for the marginal likelihood (evidence term). We do have other options for solutions such as Gibbs sampling, expectation-maximization, and Metropolis-Hastings methods. This book will only focus on Gibbs Sampling, but be aware other types of solvers are used for Bayesian inference, and in particular LDA. 2.5.2 A Tale of Two MC’s Gibbs Sampling is a Markov Chain Monte Carlo (MCMC) technique for parameter estimation. Let’s break down the two MC’s. A markov chain is a process where the next state is determined by the current state. More importantly it does not rely on any information prior to the current state. ????? 1. This needs a diagram and possibly a general example. 2. LETS MAKE SURE TO REITERATE THIS AFTER SHOWING THE GENERAL GIBBS SAMPLING MATH AND DIAGRAM ???? The other MC, Monte Carlo, is a technique used to solve a variety of problems by repeated random sampling. A common example used to showcase Monte Carlo methods is the estimation of the value of \\(\\pi\\). All we need is some chalk and a bucket of rice. I draw a circle on the ground. I then draw a square along the circumference of the square. Now for the bucket of rice, aka our random number generator. I stand over the square and uniformly pour rice over the area of the square. Now comes the monotonous part, counting the rice. First I count the number of pieces of rice inside the circle. Next I tally the number of pieces of rice remaining. From here I can infer the value of \\(\\pi\\) as follows: \\[ \\begin{equation} \\begin{aligned} {Rice_{circle}\\over Rice_{circle + square}} &amp;= {\\pi r^{2} \\over (2r)^{2}}\\\\\\\\ \\pi &amp;\\approx {4Rice_{circle}\\over Rice_{circle + square}} \\end{aligned} \\tag{2.24} \\end{equation} \\] 2.5.3 Conjugate Priors Before wading into the deeper water that is Gibbs sampling I need to touch on the concept of conjugate priors. The general premise of conjugate priors is that the prior has the same form as the posterior distribution. Ok, but what does that mean for us? In a pragmatic sense, it means we need to look for a pattern when solving for our posterior. PATTERN MATCHING ????????? Double check the ‘observed data’ statement here ??????????????? Let’s take the coin flip example. We want to estimate the posterior, the probability of \\(\\theta\\) given the the results of all experiments (this includes experiments that we haven’t yet completed). Obviously we don’t have data which does not yet exist (i.e. all experiments) so we will need to use the likelihood and the prior to estimate the posterior. 2.5.3.1 Bernoulli &amp; Beta In previous example we have used the Beta distribution to calculate the prior. What I did not point out is that the Beta distribution is the conjugate prior of the Bernoulli distribution. We start with our base relationship between posterior, likelihood, prior, and posterior. \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} = {\\overbrace{p(D|\\theta)}^{likelihood} \\overbrace{p(\\theta)}^{prior} \\over \\underbrace{p(D)}_{evidence}} \\tag{2.25} \\end{equation} \\] When estimating the posterior we drop out the evidence term. To reiterate, we can drop out the evidence term because it is constant and is used as a normalizing factor for scaling our probabilities to so that they sum to 1. We will see in the upcoming section that Gibbs sampling accounts for the scaling issue and allows us to infer probabilities at the correct scale. \\[ \\begin{equation} \\underbrace{p(\\theta|D)}_{posterior} \\propto {\\overbrace{p(D|\\theta)}^{likelihood} \\overbrace{p(\\theta)}^{prior}} \\tag{2.26} \\end{equation} \\] To estimate the posterior of the bernoulli distribution we plug in our likelihood and prior (Beta distribution) equations. \\[ \\begin{equation} p(\\theta|z,N) \\propto \\overbrace{\\theta^z(1-\\theta)^{(N-z)}}^{likelihood} \\overbrace{{\\theta^{(a-1)}(1-\\theta)^{(b-1)}}\\over \\beta(a,b)}^{prior} \\tag{2.27} \\end{equation} \\] After combining terms we end up with Equation (2.28) which looks very similar to @(bernBIFinal). This is a very simple example where an analytical solution for the posterior is possible, so the equations are effectively the same with the exeption of the \\(\\propto\\) in place of the \\(=\\). The \\(\\propto\\) is used because we are going to base our posterior off of random samples from the Beta distribution with the parameters shown in (2.28), unlike the BI example where the solution was found analytically. \\[ \\begin{equation} \\begin{aligned} p(\\theta|z,N) &amp;\\propto {\\overbrace{\\theta^{(a + z -1)}(1-\\theta)^{(N-z+b-1)}}^{Same \\hspace{1 mm} Pattern \\hspace{1 mm} as \\hspace{1 mm} Prior}\\over{\\beta(a,b)}}\\\\ p(\\theta|z,N) &amp;\\propto Beta(a+z, N-z+b) \\end{aligned} \\tag{2.28} \\end{equation} \\] 2.5.4 Gibbs Sampling Gibbs sampling is a Markov Chain Monte Carlo technique that can be used for estimating parameters by walking through a given parameter space. A generalized way to think of Gibbs sampling is estimating a given parameter based on what we currently know about all other parameters. You may be thinking ‘What other parameters?’. Gibbs sampling is only really applicable when you are trying to estimate multiple parameters. The coin flip example will now be expanded so that we have more than one parameter to estimate. We are going to compare the bias of two coins to see if the there is any discernable difference between the two coins. Mathematical representation of what happens in Gibbs Sampling: \\[ \\begin{equation} \\begin{aligned} &amp;p(\\theta_{1}^{i+1}) \\sim p(\\theta_{1}^{i}|\\theta_{2}^{i}, \\theta_{3}^{i},..., \\theta{n}^{i}) \\\\ &amp;p(\\theta_{2}^{i+1}) \\sim p(\\theta_{2}^{i}|\\theta_{1}^{i+1}, \\theta_{3}^{i},..., \\theta{n}^{i}) \\\\ &amp;p(\\theta_{3}^{i+1}) \\sim p(\\theta_{3}^{i}|\\theta_{1}^{i+1}, \\theta_{2}^{i+1},..., \\theta{n}^{i}) \\\\ &amp;................................ \\\\ &amp;p(\\theta_{n}^{i+1}) \\sim p(\\theta_{n}^{i}|\\theta_{1}^{i+1}, \\theta_{2}^{i+1},..., \\theta_{n-1}^{i+1}) \\\\ \\end{aligned} \\tag{2.29} \\end{equation} \\] In Equation (2.29) we see the next estimate, i+1, of \\(\\theta_{1}\\) is based on all other current parameter values. When estimating \\(\\theta_{2}\\) the i+1 value of \\(\\theta_{1}\\) is used along with all of the current (i) parameter values. The is continues for all parameter values. After the the nth parameter is estimated, the process starts all over again until a specific stopping criteria is met. Without the math it looks like this: FIGURE OF 3 CIRCLES CROSSED OUT/OPEN/COLORED - BEING ESTIMATED, CURRENT STEP, PAST STEP 2.5.5 Bias of Two Coins Below is an example to compare two coins. We use a fairly weak prior by setting \\(\\alpha\\) and \\(\\beta\\) to 2 for both coins. Coin 1 has 11 heads out of 14 flips while coin 2 has 7 heads out of 14 flips. a = 2 b = 2 z1 = 11 N1 = 14 z2 = 7 N2 = 14 theta = rep(0.5,2) niters = 10000 burnin = 500 thetas = matrix(0, nrow = (niters-burnin), ncol=2) for (i in 1:niters){ theta1 = rbeta(n = 1, shape1 = a + z1, shape2 = b + N1 - z1) # get value theta2| all other vars theta2 = rbeta(n = 1, shape1 = a + z2, shape2 =b + N2 - z2) if (i &gt;= burnin){ thetas[(i-burnin), ] = c(theta1, theta2) } } ds &lt;- data.frame(theta1 = thetas[,1], theta2= thetas[,2]) ggplot(ds, aes(x=theta1)) + geom_histogram(aes(y=..density..),color=&#39;#1A384A&#39;, fill=&#39;#7A99AC&#39;) + labs(title = expression(theta[1]~Estimate), x=expression(theta[1]), y = &#39;Density&#39;) + geom_vline(xintercept = mean(ds$theta1), color=&#39;#b7091a&#39;) + theme_minimal() Figure 2.10: Bias of Two Coins: Theta 1 ggplot(ds, aes(x=theta2)) + geom_histogram(aes(y=..density..),color=&#39;#1A384A&#39;, fill=&#39;#7A99AC&#39;) + labs(title = expression(theta[2]~Estimate), x=expression(theta[2]), y = &#39;Density&#39;) + geom_vline(xintercept = mean(ds$theta2), color=&#39;#b7091a&#39;) + theme_minimal() Figure 2.11: Bias of Two Coins - Theta 2 In the Figures above we can see the distribution of samples drawn from the Beta distribution using the prior parameters and the observed data we have. To compare the coins we look at the 95% confidence interval of the mean…???? 2.5.6 Change Point Example IMAGE PLACE HOLDER - MAKE A DRAWING OF FLIPPING TWO COINS ON A TIME SCALE What if the problem we are solving is more complicated. Let’s say I flip a coin repeatedly, but at some point I switch to another coin with a different bias (\\(\\theta\\)). I want to detect the point in time when coin 1 was swapped out for coin 2. We can use Gibbs sampling to solve this problem however now is a good time to highlight the general structure for using your conjugate priors and how to get to the equations required for the sampling process. The general process for derivation of our sampling distribution, as outlined in (Yildirim 2012), is as follows: Derive the full joint density. Derive the posterior conditionals for each of the random variables in the model. Simulate samples from the posterior joint distribution based on the posterior conditionals. In the previous example we only needed to get a posterior of a single variable (technically there were two variables, but both have the same posterior). In this case we have 3 variables that we need to estimate: Coin bias for coin 1: \\(\\theta_{1}\\) Coin bias for coin 2: \\(\\theta_{2}\\) The point in time, i.e. on which flip, the coin was swapped from coin 1 to coin 2: n We have three variables, but how do we describe the actual process we are modeling? In Equation (2.30) the distrutions for each variable are displayed. The coinflips, x, are drawn from a bernoulli distribution, but are dependent on the coin being flipped and the bias that coin has. The change point, n, NEED TO FINISH THIS EXPLANATION \\[ \\begin{equation} \\begin{aligned} x &amp;\\sim \\begin{cases} Bern(x_{i};\\theta_{1}) \\quad 1 \\le i \\le n \\\\ Bern(x_{i};\\theta_{1}) \\quad n \\lt i \\lt N \\end{cases} \\\\ n &amp;\\sim Uniform(1...N) \\\\ \\theta_{i} &amp;\\sim Beta(\\theta_{i}, a,b) \\end{aligned} \\tag{2.30} \\end{equation} \\] 2.5.6.1 Derivation of Joint Distribution \\[ \\begin{equation} p(\\theta_{1}, \\theta_{2}, n| x_{1:n}) \\propto \\overbrace{p(x_{1:N}|\\theta_{1}) p(x_{n+1:N}|\\theta_{2})}^{Likelihoods} \\overbrace{p(\\theta_{1})p(\\theta_{2})p(n)}^{Priors} \\tag{2.31} \\end{equation} \\] Our goal here is to get one equation to estimate the posterior of each of our variables, aka the posterior conditionals. The easiest way to accomplish this task is to identify the terms of the joint distribution that contain the variable you want the posterior conditional for. Let’s start by breaking (2.31) a bit further. $$ \\[\\begin{equation} \\begin{aligned} p(\\theta_{1}, \\theta_{2}, n| x_{1:n}) &amp;\\propto (\\prod_{1}^{n}p(x_{i}|\\theta_{1})) (\\prod_{n+1}^{N}p(x_{i}|\\theta_{2})) p(\\theta_{1})p(\\theta_{2})p(n)\\\\ &amp;\\propto [\\theta_{1}^{z_{1}}(1-\\theta_{1})^{n-z_{1}}] [\\theta_{2}^{z_{2}}(1-\\theta_{2})^{N-(n+1)-z_{2}}] p(\\theta_{1})p(\\theta_{2})p(n)\\\\ \\\\ &amp;\\propto [\\theta_{1}^{z_{1}}(1-\\theta_{1})^{n-z_{1}}] [\\theta_{2}^{z_{2}}(1-\\theta_{2})^{N-(n+1)-z_{2}}] {{\\theta_{1}^{(a_{1}-1)}(1-\\theta_{1})^{(b_{1}-1)}}\\over \\beta(a_{1},b_{1})} {{\\theta_{2}^{(a_{2}-1)}(1-\\theta_{2})^{(b_{2}-1)}}\\over \\beta(a_{2},b_{2})} {1\\over N}\\\\ \\\\ \\end{aligned} \\tag{2.32} \\end{equation}\\] $$ Let’s start by solving for n’s posterior conditional. In Equation (2.32) we see that only the likelihood terms and the priors for the \\(\\theta\\)’s contain n. Using these terms we can solve for the posterior conditional. While we are at it we will also take the log of the conditional posterior as is good practice to prevent issues such as underflow. \\[ \\begin{equation} \\begin{aligned} p(n| x_{1:n}, \\theta_{1}, \\theta_{2}) &amp;\\propto [\\theta_{1}^{z_{1}}(1-\\theta_{1})^{n-z_{1}}] [\\theta_{2}^{z_{2}}(1-\\theta_{2})^{N-(n+1)-z_{2}}]\\\\ log(p(n| x_{1:n}, \\theta_{1}, \\theta_{2})) &amp;\\propto log([\\theta_{1}^{z_{1}}(1-\\theta_{1})^{n-z_{1}}]) + log([\\theta_{2}^{z_{2}}(1-\\theta_{2})^{N-(n+1)-z_{2}}]) \\end{aligned} \\end{equation} \\] To get the posterior conditionals for the \\(\\theta\\) values we will need to utilize the conjugate prior relationship between the likelihoods and the priors. First we will collapse the priors and likelihoods for the \\(\\theta\\) values. \\[ \\begin{equation} \\begin{aligned} p(\\theta_{1}, \\theta_{2}, n| x_{1:n}) &amp;\\propto [\\theta_{1}^{(z_{1}+a_{1}-1)}(1-\\theta_{1})^{(n-z_{1}+b_{1}-1)}] [\\theta_{2}^{(z_{2}+a_{2}-1)}(1-\\theta_{2})^{(N-n-1-z_{2}+b_{2}-1)}]({1 \\over N})\\\\ &amp;\\propto Beta(a_{1}+z_{1}, n-z_{1}+b_{1}) Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})({1\\over N}) \\end{aligned} \\end{equation} \\] Now we can solve for each of the \\(\\theta\\)’s posterior conditionals. \\[ \\begin{equation} \\begin{aligned} p(\\theta_{1}| x_{1:n},\\theta_{2}, n) &amp;\\propto Beta(a_{1}+z_{1}, n-z_{1}+b_{1})\\\\ log(p(\\theta_{1}| x_{1:n},\\theta_{2}, n)) &amp;\\propto log(Beta(a_{1}+z_{1}, n-z_{1}+b_{1})) \\end{aligned} \\end{equation} \\] \\[ \\begin{equation} \\begin{aligned} p(\\theta_{2}| x_{1:n},\\theta_{1}, n) &amp;\\propto Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})\\\\ log(p(\\theta_{2}| x_{1:n},\\theta_{1}, n)) &amp;\\propto log(Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})) \\end{aligned} \\end{equation} \\] Now let’s put our derived posteriors to work and Gibb’s sample our change point and coin biases. real_thetas &lt;- c(0.2, 0.6) N &lt;- 300 a = 2 b = 3 change_point &lt;- 100 x &lt;- c(rbinom(1:change_point, 1, real_thetas[1]),rbinom((change_point+1):N, 1, real_thetas[2])) ## Initialize all parameters # n ~ uniform n &lt;- round(N*runif(1)) # theta1 ~ beta(a,b) theta1 &lt;- rbeta(1, a, b) # theta2 ~ beta(a,b) theta2 &lt;- rbeta(1, a, b) niters = 10000 burnin = 2000 # sigma = np.diag([0.2,0.2]) # thetas = np.zeros((niters-burnin,2), np.float) params = matrix(0, nrow = (niters-burnin), ncol=3) for (i in 1:niters){ z1 &lt;- sum(x[1:n]) if(n == N){ z2 &lt;- 0 }else{ z2 &lt;- sum(x[(n+1):N]) } theta1 = rbeta(n = 1, shape1 = a + z1, shape2 = b + n - z1) # get value theta2| all other vars theta2 = rbeta(n = 1, shape1 = a + z2, shape2 =N-n-1-z2+b) ## 2 things: 1 - should I be summing all the values over these? # No - the product is being calculated due to the sum - should be fine n_multi &lt;- rep(0, N) for(steps in 1:N){ if(steps==N || theta2 == 1){ n_multi[steps] &lt;- log(theta1^sum(x[1:steps]) * (1-theta1)^(steps-sum(x[1:steps]))) }else{ n_multi[steps] &lt;- log(theta1^sum(x[1:steps]) * (1-theta1)^(steps-sum(x[1:steps]))) + log(theta2^sum(x[(steps + 1):N]) * (1-theta2)^(N-steps-1-sum(x[(steps+1):N]))) } } n_multi &lt;- exp(n_multi - max(n_multi)) # what about for n? n &lt;- which(rmultinom(1, 1, n_multi/sum(n_multi))[,1] ==1) if (i &gt;= burnin){ params[(i-burnin), ] = c(theta1,theta2, n) } } ds &lt;- data.frame(x = x, theta = c(rep(real_thetas[1],N-change_point), rep(real_thetas[2],change_point)), sample_index = seq(1:length(x))) params_df &lt;- as.data.frame(params) names(params_df) &lt;- c(&#39;theta1&#39;, &#39;theta2&#39;, &#39;change_point&#39;) ggplot(params_df, aes(x = change_point)) + geom_histogram(fill=&quot;#7A99AC&quot;, color =&#39;#1A384A&#39;) + theme_minimal() + geom_vline(xintercept = mean(params_df$change_point), color=&#39;#b7091a&#39;) + labs(title = &#39;Change Point Estimate&#39;, x=&#39;Change Point&#39;, y = &#39;Density&#39;) Figure 2.12: Estimated Change Point ggplot(params_df, aes(x = theta1)) + geom_histogram(fill=&quot;#7A99AC&quot;, color =&#39;#1A384A&#39; ) + theme_minimal() + geom_vline(xintercept = mean(params_df$theta1), color=&#39;#b7091a&#39;) + labs(title = expression(theta[1]~Estimate), x=expression(theta[1]), y = &#39;Density&#39;) Figure 2.13: Estimated Theta 1 ggplot(params_df, aes(x = theta2)) + geom_histogram(fill=&quot;#7A99AC&quot;, color =&#39;#1A384A&#39; ) + theme_minimal() + geom_vline(xintercept = mean(params_df$theta2), color=&#39;#b7091a&#39;) + labs(title = expression(theta[2]~Estimate), x=expression(theta[2]), y = &#39;Density&#39;) Figure 2.14: Estimated Theta 2 "],
["multinomial-distribution.html", "3 Multinomial Distribution 3.1 Comparison of Dice vs. Words 3.2 Relationship to Bernoulli 3.3 Conjugate Prior: Dirichlet 3.4 Gibbs Sampling - Multinomial &amp; Dirichlet", " 3 Multinomial Distribution 3.1 Comparison of Dice vs. Words The binomial distribution is a special case of the multinomial distribution where the number of possible outcomes is 2. A multinomial distribution can have 2 or more outcomes and therefore is normally shown through examples using a 6-sided die. Instead of using a die with numbers on each side, let’s label the sides with the following words: “Latent” “Dirichlet” “Allocation” “has” “many” “peices” —————image placeholder———————– Maybe draw a die with those words on it and consider it a ‘document’ ——————-placeholder end——————– In the example above we would assume it is a fair die which would result in equal probabilities of ‘rolling’ any of the 6 unique words. Therefore each word has 1/6 chances of being randomly sampled from the die. Below is an empirical example where we take each word and assign it to a single side of a fair die. The experiment, a single roll of the die, is repeated 10,000 times. We can see each word comes up at roughly the same frequency. # draw 1000 samples from multinomial distribution outcomes &lt;- replicate(10000, which(rmultinom(1,1,rep(1/6,6))==1)) words &lt;- unlist(strsplit(&quot;Latent Dirichlet Allocation has many peices&quot;, &#39; &#39;)) ds &lt;- data.frame(id = outcomes, word = sapply(outcomes, function(x)words[x])) ggplot(ds, aes(x= word)) + geom_histogram(stat=&#39;count&#39;,color=&#39;#1A384A&#39;, fill=&#39;#7A99AC&#39;) + scale_x_discrete(labels = words) + theme_minimal() Figure 3.1: Sampling from Multinomial with Equal Parameters Side Note: Let&#39;s think of how this will be used in the case of LDA. If I wanted to generate a document based on a model, I could use a multinomial distribution to determine what words would be in the document. If I knew the probability of a word I could use the example above to draw a new word with each sample. Obviously some words occur much more often than others, so the &#39;fair die&#39; example wouldn&#39;t work for generation of document. In later sections we will build on this concept, but its a good idea to start thinking about how this extends to language. As stated before, the binomial distribution is a special case of the multinomial distribution. The probability mass function for the multinomial distribution is shown in (3.1): \\[ \\begin{equation} f(x)=\\dfrac{n!}{x_1!x_2!\\cdots x_k!}\\theta_1^{x_1} \\theta_2^{x_2} \\cdots \\theta_k^{x_k} \\tag{3.1} \\end{equation} \\] k - number of sides on the die n - number of times the die will be rolled Therefore the multinomial representation of the distributions we’ve already discussed, binomial and bernoulli, would use the following parameters: k sided die rolled n times n = 1, k = 2 is bernoulli distribution n &gt; 1 , k = 2 is binomial distribution 3.2 Relationship to Bernoulli Time to connect the dots between Bernoulli and Multinomial distributions . Recall the bernoulli probability mass function shown in Equation (3.2). \\[ \\begin{equation} f_{x}(x)=P(X=x)=\\theta^{x}(1-\\theta)^{1-x}, \\hspace{1cm} x = \\{0,1\\} \\tag{3.2} \\end{equation} \\] Let’s swap in some different terms. We can replace the first term, \\(\\theta\\), with \\(\\theta_{1}\\) and the second term, \\((1-\\theta)\\) as \\(\\theta_{2}\\). Then we have: \\[ \\begin{equation} {\\theta_{1}}^{x_{1}}{\\theta_{2}}^{x_{2}} \\tag{3.3} \\end{equation} \\] You can see this looks a bit more like Equation (3.2) if the case were k=2 and n=1. Since n=1, x’s can only have a value of 1 or zero. Therefore the factorial is always 1 for the case of a bernoulli trial. Factorial of n is also 1 since it is a single trial, i.e. n=1, resulting in what we see in equation (3.1). 3.3 Conjugate Prior: Dirichlet The conjugate prior for the multinomial distribution is the Dirichlet distribution. Similar to the beta distribution, Dirichlet can be though of as a distribution of distributions. Also note that the beta distribution is the special case of a Dirichlet distribution where the number of possible outcome is 2 similar to the connection between the binomial and multinomial distributions. The probability distribution function for the Dirichlet distribution is shown in Equation (3.4). \\[ \\begin{equation} Dir(\\overrightarrow{p}|\\overrightarrow{\\alpha})= { {\\Gamma {\\bigl (}\\sum _{i=1}^{K}\\alpha _{i}{\\bigr )}} \\over{\\prod _{i=1}^{K}\\Gamma (\\alpha _{i})} } \\prod _{i=1}^{K}x_{i}^{\\alpha _{i}-1} \\tag{3.4} \\end{equation} \\] THINK THIS CAN BE REMOVED - IS IN BETA A few notes about equation (3.4). The Gamma function is the factorial of the parameter minus 1. \\[ \\begin{equation} \\Gamma (\\alpha _{i}) = (\\alpha_{i}-1)! \\tag{3.5} \\end{equation} \\] The Dirichlet distribution function is often written using the Beta function in place of the first term as seen below: \\[ Dir(\\overrightarrow{p}|\\overrightarrow{\\alpha})= { 1 \\over B(\\alpha) } \\prod _{i=1}^{K}x_{i}^{\\alpha _{i}-1} \\] Where: \\[ {1\\over B(\\alpha)}={ {\\Gamma {\\bigl (}\\sum _{i=1}^{K}\\alpha _{i}{\\bigr )}} \\over{\\prod _{i=1}^{K}\\Gamma (\\alpha _{i})} } \\] The Dirichlet distribution is an extension of the beta distribution for k categories. To get a better sense of what the distributions look like let’s visualize a few examples at k=3, think 3 sided die, with varying alpha values. In both of the box plots below 10,000 random samples were drawn from a Dirichlet distribution where k=3 and \\(\\alpha\\) is the same for each k in the given plot. The first plot shows the distribution of values drawn when \\(\\alpha\\) = 100. alpha &lt;- c(100,100,100) trials &lt;- 10000 x &lt;- rdirichlet(trials, alpha) colnames(x) &lt;- c(&#39;theta_1&#39;, &#39;theta_2&#39;, &#39;theta_3&#39;) ds &lt;- cbind(as.tibble(x), trial = 1:trials) %&gt;% gather(theta, word, -trial) ggplot(ds, aes(color = theta, fill = theta, x = theta, y = word)) + geom_boxplot(alpha = 0.3) + theme_minimal() + labs(y=&#39;\\U03B8&#39;, x = &#39;&#39;, title = paste0(&quot;\\U03B1 = &quot;,unique(alpha)) ) + scale_x_discrete(labels = c(expression(&quot;\\U03B1&quot;[1]), expression(&quot;\\U03B1&quot;[2]), expression(&quot;\\U03B1&quot;[3]))) + scale_fill_discrete(guide = FALSE) + scale_color_discrete(guide = FALSE)+ scale_y_continuous(limits = c(0,1)) Figure 3.2: Sampling from Dirichlet: α=100 Below the process is repeated, but this time the \\(\\alpha\\) values are set to 1 for each category. We can see the range of distribution of values sampled with the higher \\(\\alpha\\) value is much narrower than the distribution of values sampled using \\(\\alpha\\) values of 1. This is the same pattern we saw with the beta distribution, as the shape parameters increased the distribution became more dense and the shape of the distribution narrowed. alpha &lt;- c(1,1,1) x &lt;- rdirichlet(trials, alpha) colnames(x) &lt;- c(&#39;theta_1&#39;, &#39;theta_2&#39;, &#39;theta_3&#39;) ds &lt;- cbind(as.tibble(x), trial = 1:trials) %&gt;% gather(theta, word, -trial) ggplot(ds, aes(color = theta, fill = theta, x = theta, y = word)) + geom_boxplot(alpha = 0.3) + theme_minimal() + labs(y=&#39;\\U03B8&#39;, x = &#39;&#39;, title = paste0(&quot;\\U03B1 = &quot;,unique(alpha)) ) + scale_x_discrete(labels = c(expression(&quot;\\U03B1&quot;[1]), expression(&quot;\\U03B1&quot;[2]), expression(&quot;\\U03B1&quot;[3]))) + scale_fill_discrete(guide = FALSE) + scale_color_discrete(guide = FALSE) + scale_y_continuous(limits = c(0,1)) Figure 3.3: Sampling from Dirichlet - θ=1 So what happens when the \\(\\alpha\\) values are not the same, i.e. the distribution is non-symmetrical? In the histogram below, you can see the distribution of values sampled from the dirichlet distribution for each category. Recall the beta distribution shape skews as the difference between the two parameters grows. You recall we only need to estimate one value, \\(\\theta_{1}\\) generated from the beta distribution with 2 parameters because we can infer \\(\\theta_{2}\\) from this value (\\(\\theta_{2} = 1-\\theta_{1}\\)). alpha &lt;- c(10,50,20) alpha_prop &lt;- alpha/sum(alpha) x &lt;- rdirichlet(trials, alpha) colnames(x) &lt;- c(&#39;theta_1&#39;, &#39;theta_2&#39;, &#39;theta_3&#39;) ds &lt;- cbind(as.tibble(x), trial = 1:trials) %&gt;% gather(theta, word, -trial) ggplot(ds, aes(color = theta, fill=theta, x = word)) + geom_histogram(position=&#39;identity&#39;, alpha = 0.1) + # geom_line(stat=&#39;density&#39;) + theme_minimal() + labs(x = &quot;\\U03B8&quot;, y = &quot;Count&quot;) + scale_color_discrete(label = alpha, name = &quot;\\U03B1&quot; ) + scale_fill_discrete(label = alpha, name = &quot;\\U03B1&quot; ) Figure 3.4: Sampling from Dirichlet - θ=[10,50,20] General notes on Dirichlet: higher values of beta (or whatever parameter name is), the more uniform the probability for each class, the lower the more more likely a specific class is going to be much larger than the rest. Let’s try to rationalize this in the same way we do bernoulli. Bernoulli is 2 possible outcomes, beta is it’s prior. Bernoulli is a special case of multinomial where k = 2, so I would assume that means beta is the special case of dirichlet where the number of shape parameters is 2. Think about this a bit more - in the case of beta distribution we use prior data (or assumption) of coin flips - 5 heads, 5 tails - means a = 5, b = 5. From this we make our distribution and we can randomly sample a valid value of theta (and remember we only use theta for bernoulli because if we have one probability, we can infer the other via subtraction from 1, but in reality there is a p value for heads, and a p value for tails… this makes it a bit eaiser to understand the transfer to multinomial) based on this prior information. So it would be no different if we have 3 different words in a topic, we use the prior info as the parameters for the Dirichlet distribution - 3 red, 2 blue, 1 green which would translate to alpha1 = 3, alpha2 = 2, alpha3 = 1, from there we can build our multidimensional distribution and select the most likely value for our thetas (or the probability of a specific outcome where all those p’s sum to one). [May want to put a plot or two here showing the dirichlet code, but creating the same beta distributions, so this makes more sense] 3.4 Gibbs Sampling - Multinomial &amp; Dirichlet Prior to getting into an example of Gibbs sampling as it applies to inferring the parameters of a multinomial distribution, let’s first describe a model which generates words for a single document. As you can imagine this would be modeled as a multinomial distribution with parameters \\(\\overrightarrow{\\theta} = \\theta_{1}, \\theta_{2}, ... \\theta_{n}\\) for words 1 to n. The model would be capable of generating a bag of words representation of a document. The term ‘bag of words’ refers to words in no particular order, i.e. the document we would be generating would not have structured sentences, but would contain all the components of the document. Let’s start by defining our model. Let’s start with a basic composition for our ideal document. We are going to have a document with only 3 distinct words 📘,📕,📗. Remember a document is just a mixture of words, obviously this is not a fine work of literature and has a very limited vocabulary, but it is meant as a basic example of document composition. First we are going to create a seed document, i.e. the document that will be used as a basis of our \\(\\alpha\\)’s for our prior. In order to do this we need to identify the mixture proportions for each word in the vocabulary. 📘 : 10% 📕 : 10% 📗 : 80% To clarify this means the document will contain 80% blue books, 10% green books, and 10% red books: # use letters function as your vocabulary v &lt;- c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;) nwords &lt;- 10 doc_theta &lt;- c(.1, .1, .8) document&lt;-rep(v, doc_theta*nwords) books &lt;- tibble(label = c(&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;), code = c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;)) cat(sapply(document, function(x) books$code[which(books$label == x)])) ## 📕 📗 📘 📘 📘 📘 📘 📘 📘 📘 So what is the structure of the document generator? Alpha -&gt; Dirichlet -&gt; Multinomial (Maybe here is when you can introduce the terrible idea of block diagrams????, It might be good to introduce them even though you aren’t a fan, for all you know people might even understant them better this way…) Do you recall the beta/bernoulli example? The way we informed our prior was using some prior information we had, i.e. the number of heads and tails previously obtained from flipping the two coins. We will use the document above as the basis of our \\(\\alpha\\) paramters for the Dirichlet distribution, i.e. our prior for the multinomial. In more general language, we want to generate documents similar to our ‘ideal’ document. So let’s generate a new document using the word counts from our ideal document as our \\(\\alpha\\) values for the dirichlet prior. Then we use the \\(\\theta\\) values generated by the dirichlet prior as the parameters for a multinomial distribution to generate the next term in the document. # generate text based only on document #1 # leave this code, I like the for loop for creating a single document, shows the process # in a straight forward manner, the next code chunk shows a faster way, which is good as well words &lt;- document # lenght of new document #rep(1,length(unique(words))) word_counts &lt;- table(words) alphas &lt;- word_counts new_doc &lt;- rep(&#39;&#39;, nwords) for(i in 1:nwords){ set.seed(i) p = rdirichlet(1,alphas) set.seed(i) new_doc[i] &lt;- names(word_counts)[which(rmultinom(1, 1, p) == 1)] } table(new_doc) ## new_doc ## blue red ## 9 1 cat(&#39;\\n&#39;, sapply(new_doc, function(x) books$code[which(books$label == x)])) ## ## 📘 📘 📘 📘 📘 📘 📕 📘 📘 📘 It’s not quite the same as the original, but that should be expected. This is a model that generates documents probabalistically based on some prior information. So let’s make a few more and see how this changes. word_counts &lt;- table(words) alphas &lt;- word_counts nwords &lt;- 10 ndocs &lt;- 5 word_encodings &lt;- tibble(label = c(&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;), code = c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;), word_props = c(.1, .1, .8)) thetas &lt;- rdirichlet(ndocs*nwords, alphas) print(head(thetas)) ## [,1] [,2] [,3] ## [1,] 0.6259047 0.33535589 0.038739455 ## [2,] 0.8641536 0.07229474 0.063551631 ## [3,] 0.8292271 0.02713615 0.143636766 ## [4,] 0.9399257 0.05494752 0.005126745 ## [5,] 0.7641922 0.19525569 0.040552159 ## [6,] 0.9015058 0.02198210 0.076512137 selected_words &lt;- apply(thetas, 1, function(x) which(rmultinom(1,1,x)==1)) ds &lt;- tibble(doc_id = rep(1:ndocs, each = nwords), word = word_encodings$label[selected_words], word_uni = word_encodings$code[selected_words]) ds %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(word_uni, collapse = &#39; &#39;) ) %&gt;% kable(col.names = c(&#39;Document&#39;, &#39;Words&#39;)) Document Words 1 📘 📕 📘 📘 📕 📘 📕 📕 📘 📘 2 📘 📘 📘 📘 📘 📘 📘 📕 📘 📘 3 📘 📘 📘 📘 📗 📘 📘 📘 📕 📘 4 📘 📘 📗 📕 📘 📘 📘 📘 📘 📘 5 📕 📘 📗 📘 📕 📘 📘 📘 📘 📘 As we can see each document composition is similar, but the word counts and order are different each time. This is to be expected (maybe say why? ) So now onto inferernce …. The process above is known as a generative model. We created documents using a model with a given set of parameters. Inference is going to take this general concept and look at it from a different angle. Instead of generating documents with our model we are going to take a series of pre-existing documents and infer what model created them. We are going to make the assumption that the structure of the model is the same as the generative example, i.e. all documents are generated based on the same word mixture ratios. Let’s use the 10 documents we previously generated as our basis and see if we can infer the parameters used to generate them. # counts -&gt; alphas # for each of the 3 thetas iterate 1k times # plot distributions alphas &lt;- rep(1,nrow(books)) # alphas &lt;- 1:6 n &lt;- table(ds$word) head(n) ## ## blue green red ## 38 3 9 niters = 2000 burnin = 500 thetas = matrix(0, nrow = (niters-burnin), ncol=nrow(books), dimnames = list(NULL, c(names(n)))) for (i in 1:niters){ theta = rdirichlet(1,n+alphas) if (i &gt;= burnin){ thetas[(i-burnin), ] = theta } } # hist(thetas[, 1]) # hist(thetas[, 2]) # df &lt;- as.tibble(thetas) %&gt;% gather(word, theta) # map book colors to each segment of plot to avoid bothering with the emoji labels (for now) ggplot(df, aes(y=theta, x = word)) + geom_violin() # apply(thetas, 2, median) # n/sum(n) 3.4.1 Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc) Below is a general overview of how inferrence can be carried out using Gibbs sampling. Recall conjugate priors have the same posterior form as their conjugate distribution. In equation 24 we start with the now familiar proportional solution for estimating a posterior through sampling. We need the likelihood, which is derived from the multinomial distribution, and the prior, which is derived from the dirichlet distribution. Once we plug in the prior and likelihood and simplify, we find that we are left with a Dirichlet PDF with the input parameters of \\(\\overrightarrow{\\alpha} + \\overrightarrow{n}\\) where n are the observed word counts. \\[ \\begin{aligned} p(\\theta|D) &amp;\\propto p(D|\\theta)p(\\theta)\\\\ &amp;\\propto \\prod _{i=1}^{K}\\theta^{n(k)} { {\\Gamma {\\bigl (}\\sum _{i=1}^{K}\\alpha _{i}{\\bigr )}} \\over{\\prod _{i=1}^{K}\\Gamma (\\alpha _{i})} } \\prod _{i=1}^{K}\\theta_{i}^{\\alpha _{i}-1} \\\\ &amp;\\propto{ {\\Gamma {\\bigl (}\\sum _{i=1}^{K}\\alpha _{i}{\\bigr )}} \\over{\\prod _{i=1}^{K}\\Gamma (\\alpha _{i})} }\\prod _{i=1}^{K}\\theta_{i}^{\\alpha _{i}+n_{k}-1} \\\\ &amp;\\propto Dir(\\overrightarrow{\\alpha} + \\overrightarrow{n}) \\end{aligned} \\tag{24} \\] We can see our mixture estimates are significantly different from the real model used to generate the documents. So why is this? One of the issues here is that our sample are documents with only 10 words. Therefore an average document has 8 …, 1 …, and 1…, but it is not unusual to see a slight variation which causes mixture shifts of 10% or more. And with Let’s try the same example but this time instead of only generating 5 documents we will genertate 500 and use this as our sample to draw inference from. word_counts &lt;- table(words) alphas &lt;- word_counts nwords &lt;- 10 ndocs &lt;- 500 word_encodings &lt;- tibble(label = c(&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;), code = c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;), word_props = c(.1, .1, .8)) thetas &lt;- rdirichlet(ndocs*nwords, alphas) print(head(thetas)) ## [,1] [,2] [,3] ## [1,] 0.5154606 0.40401130 0.08052810 ## [2,] 0.8526076 0.12103847 0.02635393 ## [3,] 0.7801023 0.14641427 0.07348343 ## [4,] 0.8444930 0.06278609 0.09272094 ## [5,] 0.7224804 0.11597181 0.16154776 ## [6,] 0.8639552 0.01028122 0.12576353 selected_words &lt;- apply(thetas, 1, function(x) which(rmultinom(1,1,x)==1)) ds &lt;- tibble(doc_id = rep(1:ndocs, each = nwords), word = word_encodings$label[selected_words], word_uni = word_encodings$code[selected_words]) # counts -&gt; alphas # for each of the 3 thetas iterate 1k times # plot distributions alphas &lt;- rep(1,nrow(books)) # alphas &lt;- 1:6 n &lt;- table(ds$word) head(n) ## ## blue green red ## 4008 507 485 niters = 2000 burnin = 500 thetas = matrix(0, nrow = (niters-burnin), ncol=nrow(books), dimnames = list(NULL, c(names(n)))) for (i in 1:niters){ theta = rdirichlet(1,n+alphas) if (i &gt;= burnin){ thetas[(i-burnin), ] = theta } } # hist(thetas[, 1]) # hist(thetas[, 2]) # df &lt;- as.tibble(thetas) %&gt;% gather(word, theta) # map book colors to each segment of plot to avoid bothering with the emoji labels (for now) ggplot(df, aes(y=theta, x = word)) + geom_violin() # apply(thetas, 2, median) n/sum(n) ## ## blue green red ## 0.8016 0.1014 0.0970 "],
["word-embeddings-and-representations.html", "4 Word Embeddings and Representations 4.1 Bag of Words 4.2 Word Embeddings", " 4 Word Embeddings and Representations This chapter offers a general overview of how documents and words are viewed and processed during topic modeling. 4.1 Bag of Words LDA processes documents as a ‘bag of words’. Bag of words means you view the frequency of the words in a document with no regard for the order the words appeared in. Figure 4.1: Bag of Words Representation Obviously there is going to be some information lost in this process, but our goal with topic modeling is to be able to view the ‘big picture’ from a large number of documents. Alternate way of thinking about it: I have a vocabulary of 100k words used across 1 million documents, then I use LDA to look 500 topics. I just narrowed down my feature space from 100k features to 500 features (dimensionality reduction). Possible examples: Scrabble in reverse 4.2 Word Embeddings I will preface this section by warning you there are many differnt types of word embeddings. A word embedding is a way to summarized words in documents based on a frequency or weight. Why would we need this? The primary reason is that we need a way of summarizing text in a manner that we can process easily with a computer (or with math in general). One of the simplest ways to do this is by taking word counts for each individual word in a document. Each row is a unique word in the vocabulary and each column is a document. ——-image———- ——-placeholder——- "],
["lda-as-a-generative-model.html", "5 LDA as a Generative Model 5.1 General Terminology 5.2 Generative Model", " 5 LDA as a Generative Model This chapter is going to focus on LDA as a generative model. I’m going to build on the unigram generation example from the last chapter and with each new example a new variable will be added until we work our way up to LDA. After getting a grasp of LDA as a generative model in this chapter, the following chapter will focus on working backwards to answer the following question: “If I have a bunch of documents, how do I infer topic information (word distributions, topic mixtures) from them?” 5.1 General Terminology Let’s get the ugly part out of the way, the parameters and variables that are going to be used in the model. alpha (\\(\\overrightarrow{\\alpha}\\)) : In order to determine the value of \\(\\theta\\), the topic distirbution of the document, we sample from a dirichlet distribution using \\(\\overrightarrow{\\alpha}\\) as the input parameter. What does this mean? The \\(\\overrightarrow{\\alpha}\\) values are our prior information about the topic mixtures for that document. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the total number of words from each topic across all documents as the \\(\\overrightarrow{\\beta}\\) values. beta (\\(\\overrightarrow{\\beta}\\)) : In order to determine the value of \\(\\phi\\), the word distirbution of a given topic, we sample from a dirichlet distribution using \\(\\overrightarrow{\\beta}\\) as the input parameter. What does this mean? The \\(\\overrightarrow{\\beta}\\) values are our prior information about the word distribution in a topic. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the number of times each word was used for a given topic as the \\(\\overrightarrow{\\beta}\\) values. theta (\\(\\theta\\)) : Is the topic proportion of a given document. More importantly it will be used as the parameter for the multinomial distribution used to identify the topic of the next word. To clarify, the selected topic’s word distribution will then be used to select a word w. phi (\\(\\phi\\)) : Is the word distribution of each topic, i.e. the probability of each word in the vocabulary being generated if a given topic, z (z ranges from 1 to k), is selected. xi (\\(\\xi\\)) : In the case of a variable lenght document, the document length is determined by sampling from a Poisson distribution with an average length of \\(\\xi\\) k : Topic index z : Topic selected for the next word to be generated. Outside of the variables above all the distributions should be familiar from the previous chapter. 5.1.1 Selecting Parameters The intent of this section is not aimed at delving into different methods of parameter estimation for \\(\\alpha\\) and \\(\\beta\\), but to give a general understanding of how those values effect your model. For ease of understanding I will also stick with an assumption of symmetry, i.e. all values in \\(\\overrightarrow{\\alpha}\\) are equal to one another and all values in \\(\\overrightarrow{\\beta}\\) are equal to one another. Symmetry can be thought of as each topic having equal probability in each document for \\(\\alpha\\) and each word having an equal probability in \\(\\beta\\). In previous sections we have outlined how the \\(alpha\\) parameters effect a Dirichlet distribution, but now it is time to connect the dots to how this effects our documents. 5.2 Generative Model LDA is know as a generative model. What is a generative model? Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space (Bishop 2006). This means we can create documents with a mixture of topics and a mixture of words based on thosed topics. Let’s start off with a simple example of generating unigrams. 5.2.1 Generating Documents 5.2.1.1 Topic Word Mixtures, Document Topic Mixtures, and Document Length Static Building on the document generating model in chapter two, let’s try to create documents that have words drawn from more than one topic. To clarify the contraints of the model will be: set number of topics (2) constant topic distributions in each document constant word distribution in each topic Known values: 2 topics : word distributions of each topic below \\(\\phi_{1}\\) = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ] \\(\\phi_{2}\\) = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ] All Documents have same topic distribution: \\(\\theta = [ topic a = 0.5, topic b = 0.5 ]\\) All Documents contain 10 words Generative Model Pseudocode For d = 1 to D where D is the number of documents For w = 1 to W where W is the number of words in document d Select the topic for word w \\(z_{i}\\) ~ Multinomial(\\(\\theta_{d}\\)) Select word based on topic z’s word distribution \\(w_{i}\\) ~ Multinomial(\\(\\phi^{(z_{i})}\\)) k &lt;- 2 # number of topics M &lt;- 10 # let&#39;s create 10 documents vocab &lt;- c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;) alphas &lt;- rep(1,k) # topic document dirichlet parameters phi &lt;- matrix(c(0.1, 0, 0.9, 0.4, 0.4, 0.2), nrow = k, ncol = length(vocab), byrow = TRUE) theta &lt;- c(0.5, 0.5) N &lt;- 10 #words in each document ds &lt;-tibble(doc_id = rep(0,N*M), word = rep(&#39;&#39;, N*M), topic = rep(0, N*M) ) row_index &lt;- 1 for(m in 1:M){ for(n in 1:N){ # sample topic index , i.e. select topic topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- vocab[which(rmultinom(1,1,phi[topic, ])==1)] ds[row_index,] &lt;- c(m,new_word, topic) row_index &lt;- row_index + 1 } } ds %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(word, collapse = &#39; &#39;) ) %&gt;% kable() doc_id tokens 1 📘 📕 📗 📘 📘 📘 📗 📘 📗 📗 10 📗 📘 📕 📘 📘 📗 📕 📕 📗 📗 2 📗 📗 📘 📗 📗 📗 📗 📗 📘 📗 3 📕 📗 📗 📗 📗 📕 📗 📕 📕 📗 4 📗 📘 📗 📗 📗 📗 📕 📗 📘 📗 5 📕 📗 📗 📘 📗 📘 📘 📗 📗 📗 6 📗 📘 📗 📘 📗 📕 📕 📗 📗 📗 7 📘 📗 📗 📗 📗 📗 📕 📕 📗 📕 8 📗 📗 📗 📗 📗 📗 📘 📕 📗 📕 9 📗 📘 📗 📗 📗 📗 📘 📘 📘 📕 5.2.1.2 Topic Word Mixtures &amp; Document Topic Mixtures Static, Document Length Varying This next example is going to be very similar, but it now allows for varying document length. The length of each document is determined by a Poisson distribution with an average document length of 10. Known values: 2 topics : word distributions of each topic below \\(\\phi_{1}\\) = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ] \\(\\phi_{2}\\) = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ] All Documents have same topic distribution: \\(\\theta = [ topic a = 0.5, topic b = 0.5 ]\\) Generative Model Pseudocode For d = 1 to D where D is the number of documents Determine length of document \\(W ~ Poisson(\\xi)\\) For w = 1 to W where W is the number of words in document d Select the topic for word w \\(z_{i}\\) ~ Multinomial(\\(\\theta_{d}\\)) Select word based on topic z’s word distribution \\(w_{i}\\) ~ Multinomial(\\(\\phi^{(z_{i})}\\)) k &lt;- 2 # number of topics M &lt;- 10 # let&#39;s create 10 documents vocab &lt;- c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;) alphas &lt;- rep(1,k) # topic document dirichlet parameters phi &lt;- matrix(c(0.1, 0, 0.9, 0.4, 0.4, 0.2), nrow = k, ncol = length(vocab), byrow = TRUE) theta &lt;- c(0.5, 0.5) xi &lt;- 10 # average document length N &lt;- rpois(M, xi) #words in each document ds &lt;-tibble(doc_id = rep(0,sum(N)), word = rep(&#39;&#39;, sum(N)), topic = rep(0, sum(N)) ) row_index &lt;- 1 for(m in 1:M){ for(n in 1:N[m]){ # sample topic index , i.e. select topic topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- vocab[which(rmultinom(1,1,phi[topic, ])==1)] ds[row_index,] &lt;- c(m,new_word, topic) row_index &lt;- row_index + 1 } } ds %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(word, collapse = &#39; &#39;) ) %&gt;% kable() doc_id tokens 1 📗 📘 📗 📘 📗 📗 📗 📕 📕 📗 📗 📘 10 📕 📕 📘 📗 📘 📗 📗 📗 📕 📗 2 📗 📗 📗 📗 📗 📕 📗 📘 📗 📕 📗 📘 📗 3 📕 📗 📗 📕 📘 📘 📗 📘 📕 📘 📕 📘 📕 📘 📘 📗 4 📗 📕 📕 📗 📘 📗 📘 📘 📘 📕 📗 📘 📗 📕 📘 📕 5 📗 📗 📗 📗 📗 📘 📕 📗 📗 📘 📕 📗 📗 📘 📗 6 📗 📘 📘 📗 📗 📗 📗 📘 📗 📗 📗 📗 📕 📗 7 📘 📘 📘 📗 📕 📗 📘 📘 📗 📘 📘 📗 📕 📘 📘 📗 📗 📕 📗 📗 📘 📗 8 📘 📗 📗 📕 📕 📗 📗 📕 📗 📗 📗 📕 📗 📗 📗 📕 📕 📕 📘 9 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 5.2.1.3 Topic Word Mixtures Static, Varying Document Topic Distributions and Document Length So this time we will introduce documents with different topic distributions and length.The word distributions for each topic are still fixed. Known values: 2 topics : word distributions of each topic below \\(\\phi_{1}\\) = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ] \\(\\phi_{2}\\) = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ] Generative Model Pseudocode For d = 1 to D where number of documents is D + Sample parameters for document topic distribution + \\(\\theta_{d}\\) ~ Dirichlet(\\(\\alpha\\)) + For w = 1 to W where W is the number of words in document d + Select the topic for word w + \\(z_{i}\\) ~ Multinomial(\\(\\theta_{d}\\)) + Select word based on topic z’s word distribution + \\(w_{i}\\) ~ Multinomial(\\(\\phi^{(z_{i})}\\)) k &lt;- 2 # number of topics M &lt;- 10 # let&#39;s create 10 documents vocab &lt;- c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;) alphas &lt;- rep(1,k) # topic document dirichlet parameters phi &lt;- matrix(c(0.1, 0, 0.9, 0.4, 0.4, 0.2), nrow = k, ncol = length(vocab), byrow = TRUE) xi &lt;- 10 # average document length N &lt;- rpois(M, xi) #words in each document ds &lt;-tibble(doc_id = rep(0,sum(N)), word = rep(&#39;&#39;, sum(N)), topic = rep(0, sum(N)), theta_a = rep(0, sum(N)), theta_b = rep(0, sum(N)) ) row_index &lt;- 1 for(m in 1:M){ theta &lt;- rdirichlet(1, alphas) for(n in 1:N[m]){ # sample topic index , i.e. select topic topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- vocab[which(rmultinom(1,1,phi[topic, ])==1)] ds[row_index,] &lt;- c(m,new_word, topic,theta) row_index &lt;- row_index + 1 } } ds %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(word, collapse = &#39; &#39;), topic_a = round(as.numeric(unique(theta_a)), 2), topic_b = round(as.numeric(unique(theta_b)), 2) ) %&gt;% kable() doc_id tokens topic_a topic_b 1 📕 📕 📗 📕 📗 📘 📗 📗 📗 📗 📗 0.08 0.92 10 📘 📕 📘 📘 📘 📘 📘 📘 📗 📘 0.11 0.89 2 📘 📗 📗 📗 📘 📕 📗 📕 📘 0.39 0.61 3 📗 📗 📗 📗 📗 📗 📘 📗 📘 📗 📗 0.43 0.57 4 📗 📘 📕 📕 📘 📗 📗 📕 📘 📕 📗 0.40 0.60 5 📗 📘 📘 📗 0.82 0.18 6 📘 📗 📘 📕 📗 📕 📘 📘 📕 📗 📗 📕 📗 📘 📕 0.11 0.89 7 📗 📗 📗 📗 📗 📘 📗 0.66 0.34 8 📘 📘 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 0.68 0.32 9 📕 📘 📕 📘 📗 📘 📗 📕 📘 📗 📘 📕 0.01 0.99 5.2.2 LDA Generative Model We are finally at the full generative model for LDA. The word distributions for each topic vary based on a dirichlet distribtion, as do the topic distribution for each document, and the document length is drawn from a Poisson distribution. Generative Model Pseudocode For k = 1 to K where K is the total number of topics Sample parameters for word distribution of each topic \\(\\phi^{(k)}\\) ~ Dirichlet(\\(\\beta\\)) For d = 1 to D where number of documents is D Sample parameters for document topic distribution \\(\\theta_{d}\\) ~ Dirichlet(\\(\\alpha\\)) For w = 1 to W where W is the number of words in document d Select the topic for word w \\(z_{i}\\) ~ Multinomial(\\(\\theta_{d}\\)) Select word based on topic z’s word distribution \\(w_{i}\\) ~ Multinomial(\\(\\phi^{(z_{i})}\\)) k &lt;- 2 # number of topics M &lt;- 10 # let&#39;s create 10 documents vocab &lt;- c(&#39;\\U1F4D8&#39;, &#39;\\U1F4D5&#39;, &#39;\\U1F4D7&#39;) alphas &lt;- rep(1,k) # topic document dirichlet parameters betas &lt;- rep(1,length(vocab)) # dirichlet parameters for topic word distributions phi &lt;- rdirichlet(k, betas) xi &lt;- 10 # average document length N &lt;- rpois(M, xi) #words in each document ds &lt;-tibble(doc_id = rep(0,sum(N)), word = rep(&#39;&#39;, sum(N)), topic = rep(0, sum(N)), theta_a = rep(0, sum(N)), theta_b = rep(0, sum(N)) ) row_index &lt;- 1 for(m in 1:M){ theta &lt;- rdirichlet(1, alphas) for(n in 1:N[m]){ # sample topic index , i.e. select topic topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- vocab[which(rmultinom(1,1,phi[topic, ])==1)] ds[row_index,] &lt;- c(m,new_word, topic,theta) row_index &lt;- row_index + 1 } } ds %&gt;% group_by(doc_id, topic) %&gt;% summarise( tokens = paste(word, collapse = &#39; &#39;), topic_a = round(as.numeric(unique(theta_a)), 2), topic_b = round(as.numeric(unique(theta_b)), 2) ) %&gt;% kable() doc_id topic tokens topic_a topic_b 1 1 📕 📘 📘 📕 📗 📘 📘 📕 📕 📗 📕 0.75 0.25 1 2 📕 📕 📘 0.75 0.25 10 2 📘 📘 📘 📘 📘 📘 0.08 0.92 2 1 📕 📕 📗 📕 📕 📗 📘 0.56 0.44 2 2 📕 📘 📗 0.56 0.44 3 2 📘 📗 📘 📗 📘 📘 📕 📕 📘 0.01 0.99 4 1 📕 0.13 0.87 4 2 📕 📘 📗 📗 📕 📕 📘 0.13 0.87 5 1 📗 📗 📗 📕 📘 📘 📕 📘 📕 📗 📘 📗 📘 📘 📗 0.87 0.13 5 2 📘 📕 0.87 0.13 6 1 📕 0.26 0.74 6 2 📘 📘 📕 📗 📘 📘 📕 📘 0.26 0.74 7 1 📘 📕 0.13 0.87 7 2 📘 📘 📗 📕 📘 📘 📗 0.13 0.87 8 1 📗 📘 📕 📕 📕 📗 📕 0.60 0.40 8 2 📕 📘 📘 📘 📕 0.60 0.40 9 1 📕 📘 📘 📗 📗 📕 📘 📘 📘 📘 📕 📗 0.77 0.23 9 2 📘 📗 0.77 0.23 (NOTE: Spread the table so topics are side by side for each doc) The LDA generative process for each document is shown below(Darling 2011): \\[ \\begin{equation} p(w,z,\\theta,\\phi|\\alpha, B) = p(\\phi|B)p(\\theta|\\alpha)p(z|\\theta)p(w|\\phi_{z}) \\tag{5.1} \\end{equation} \\] You may be like me and have a hard time seeing how we get to the equation above and what it even means. If we look back at the pseudo code for the LDA model it is a bit easier to see how we got here. We start by giving a probability of a topic for each word in the vocabulary, \\(\\phi\\). This value is drawn randomly from a dirichlet distribution with the parameter \\(\\beta\\) giving us our first term \\(p(\\phi|\\beta)\\). The next step is generating documents which starts by calculating the topic mixture of the document, \\(\\theta_{d}\\) generated from a dirichlet distribution with the parameter \\(\\alpha\\). This is our second term \\(p(\\theta|\\alpha)\\). You can see the following two terms also follow this trend. The topic, z, of the next word is drawn from a multinomial distribuiton with the parameter \\(\\theta\\). Once we know z, we use the distribution of words in topic z, \\(\\phi_{z}\\), to determine the word that is generated. "],
["lda-inference.html", "6 LDA Inference 6.1 General Overview", " 6 LDA Inference 6.1 General Overview We have talked about LDA as a generative model, but now it is time to flip the problem around. What if I have a bunch of documents and I want to infer topics? To figure out a problem like this we are first going to assume the documents were generated using a generative model similar to the ones we created in the previous section. Now it is time to flip the problem around. What happens when I have a bunch of documents and I want to know what topics are present in each document and what words are present (most probable) in each topic? In terms of math, the values we need to answer the questions above are: \\[ \\begin{equation} p(\\theta, \\phi, z|w, \\alpha, \\beta) = {p(\\theta, \\phi, z, w|\\alpha, \\beta) \\over p(w|\\alpha, \\beta)} \\tag{6.1} \\end{equation} \\] Equation (6.1) says the following: The probability of the topic proportion, the word distribution of each topic, and the topic label given a word (in a document) and the hyperparameters \\(\\alpha\\) and \\(\\beta\\). In a more basic term this means the probability of a specific topic z given the word w (and our prior assumptions, i.e. hyperparameters), once the topic label of a word is known, then we can answer derive \\(\\phi\\) and \\(\\theta\\). Let’s take a step from the math and just map out things we know versus the things we don’t know in our current scenario: Known Parameters Documents: We have a set number of documents we want to identify the topic strucutres in. Words: We have a collection of words and word counts for each document which we place in a document word matrix. Vocabulary: The unique list of words across all documents. Hyperparemeters: \\(\\overrightarrow{\\alpha}\\) \\(\\overrightarrow{\\beta}\\) And on to the parts we don’t know…. ** Unknown (Latent) Parameters ** Number of Topics: We need to determine the number of topics we assume are present in the documents. For the purposes of this book I’m going to skip how to estimate this number properly. If you are interested in learning more about how to estimate the number of topics present in a document see REFERENCE LIST. Document Topic Mixture: We need to determine the topic distribution in each document. Word Distribution of Each Topic: We need to know the distribution of words in each topic. Obviously some words are going to occur very often in a topic while others may have zero probability of occurring in a topic. Word topic assignment: This is actually the main thing we need to infer. To be clear, if we know the topic assignment of every word in every document, then we can derive the document topic mixture, \\(\\theta\\), and the word distribution, \\(\\phi\\), of each topic. Quick note: Equation (6.1) is based on the following statistical property \\[ \\begin{equation} p(A, B | C) = {p(A,B,C) \\over p(C)} \\tag{6.2} \\end{equation} \\] The derivation connecting equation (6.1) to the actual Gibbs sampling solution to determine z for each word in each document, \\(\\overrightarrow{\\theta}\\), and \\(\\overrightarrow{\\phi}\\) is very complicated and I’m just going to gloss over a few steps. For complete derivations see (site heinrich and that other one from darling). As stated previously, the main goal of inference in LDA is to determine the topic of each word, \\(z_{i}\\) (topic of word i), in each document. \\[ \\begin{equation} p(z_{i}|z_{\\neg i}, \\alpha, \\beta, w) \\tag{6.3} \\end{equation} \\] Notice that we are interested in identifying the topic of the current word, \\(z_{i}\\), based on the topic assignments of all other words (not including word i), \\(z_{\\neg i}\\). The following derivations are all from (Darling 2011) \\[ \\begin{equation} \\begin{aligned} p(z_{i}|z_{\\neg i}, \\alpha, \\beta, w) &amp;= {p(z_{i},z_{\\neg i}, w, | \\alpha, \\beta) \\over p(z_{\\neg i},w | \\alpha, \\beta)}\\\\ &amp;\\propto p(z_{i}, z_{\\neg i}, w | \\alpha, \\beta)\\\\ &amp;\\propto p(z,w|\\alpha, \\beta) \\end{aligned} \\tag{6.4} \\end{equation} \\] You may notice \\(p(z,w|\\alpha, \\beta)\\) looks very similar to the definition of the generative process of LDA (cite Darling) from the previous chapter (equation (5.1)). The only difference is the absence of \\(\\theta\\) and \\(\\phi\\). This means we can swap in equation (5.1) and integrate out \\(\\theta\\) and \\(\\phi\\). \\[ \\begin{equation} \\begin{aligned} p(w,z|\\alpha, \\beta) &amp;= \\int \\int p(z, w, \\theta, \\phi|\\alpha, \\beta)d\\theta d\\phi\\\\ &amp;= \\int \\int p(\\phi|\\beta)p(\\theta|\\alpha)p(z|\\theta)p(w|\\phi_{z})d\\theta d\\phi \\\\ &amp;= \\int p(z|\\theta)p(\\theta|\\alpha)d \\theta \\int p(w|\\phi_{z})p(\\phi|\\beta)d\\phi \\end{aligned} \\tag{6.5} \\end{equation} \\] As with previous examples in this book we are now going to expand equation (6.4), plug in our conjugate priors, and get to a point where we can use a gibbs sampler to estimate our solution. ADD IN A NOTE ABOUT GIBBS SAMPLING FOR THE UNITIATED - PAPER - this follows the exact steps laid out in that paper. topic assignment/topic proportion of documents. i - documents, k - topics Below you will see the derivation for the first term of equation (6.5) which to identify the sampling distribution for the topic distribution in each document. The conjugate prior is utilized. The result is a dirichlet distribution with the parameter comprised of the sum of the number of words assigned each topic and the alpha value for that topic. (This needs to be more clear…) \\[ \\begin{equation} \\begin{aligned} \\int p(z|\\theta)p(\\theta|\\alpha)d \\theta &amp;= \\int \\prod_{i}{\\theta_{d_{i},z_{i}}{1\\over B(\\alpha)}}\\prod_{k}\\theta_{d,k}^{\\alpha k}\\theta_{d} \\\\ &amp;={1\\over B(\\alpha)} \\int \\prod_{k}\\theta_{d,k}^{n_{d,k} + \\alpha k} \\\\ &amp;={B(n_{d,.} + \\alpha) \\over B(\\alpha)} \\end{aligned} \\tag{6.6} \\end{equation} \\] Similarly we can expand the second term of Equation (6.5) and we find a solution with a similar form. \\[ \\begin{equation} \\begin{aligned} \\int p(w|\\phi_{z})p(\\phi|\\beta)d\\phi &amp;= \\int \\prod_{d}\\prod_{i}\\phi_{z_{d,i},w_{d,i}} \\prod_{k}{1 \\over B(\\beta)}\\prod_{w}\\phi^{B_{w}}_{k,w}d\\phi_{k}\\\\ &amp;= \\prod_{k}{1\\over B(\\beta)} \\int \\prod_{w}\\phi_{k,w}^{B_{w} + n_{k,w}}d\\phi_{k}\\\\ &amp;=\\prod_{k}{B(n_{k,.} + \\beta) \\over B(\\beta)} \\end{aligned} \\tag{6.7} \\end{equation} \\] This leaves us with the following: \\[ \\begin{equation} \\begin{aligned} p(w,z|\\alpha, \\beta) &amp;= \\prod_{d}{B(n_{d,.} + \\alpha) \\over B(\\alpha)} \\prod_{k}{B(n_{k,.} + \\beta) \\over B(\\beta)} \\end{aligned} \\tag{6.8} \\end{equation} \\] The equation necessary for Gibbs sampling can be derived by utilizing (6.8). This is accomplished via the chain rule. NEED TO ADD IN MORE INFORMATION ON THIS - DEFINITELY HAVE NOTES ABOUT THIS SOMEWHERE. \\[ \\begin{equation} \\begin{aligned} p(z_{i}|z^{(-i)}, w) &amp;= {p(w,z)\\over {p(w,z^{(-i)})}} = {p(z)\\over p(z^{(-i)})}{p(w|z)\\over p(w^{(-i)}|z^{(-i)})p(w_{i})}\\\\ \\\\ &amp;\\propto \\prod_{d}{B(n_{d,.} + \\alpha) \\over B(n_{d,.}^{(-i)}\\alpha)} \\prod_{k}{B(n_{k,.} + \\beta) \\over B(n_{k,.}^{(-i)} + \\beta)}\\\\ \\\\ &amp;\\propto {\\Gamma(n_{d,k} + \\alpha_{k}) \\Gamma(\\sum_{k=1}^{K} n_{d,k}^{(-i)} + \\alpha_{k}) \\over \\Gamma(n_{d,k}^{(-i)} + \\alpha_{k}) \\Gamma(\\sum_{k=1}^{K} n_{d,k}+ \\alpha_{k})} {\\Gamma(n_{k,w} + \\beta_{w}) \\Gamma(\\sum_{w=1}^{W} n_{k,w}^{(-i)} + \\beta_{w}) \\over \\Gamma(n_{k,w}^{(-i)} + \\beta_{w}) \\Gamma(\\sum_{w=1}^{W} n_{k,w}+ \\beta_{w})}\\\\ \\\\ &amp;\\propto (n_{d,k}^{(-i)} + \\alpha_{k}) {n_{k,w}^{(-i)} + \\beta_{w} \\over \\sum_{w&#39;} n_{k,w&#39;}^{(-i)} + \\beta_{w&#39;}} \\end{aligned} \\tag{6.9} \\end{equation} \\] Need to explain the difference between primes/non-primes (w’s) We will now use Equation (6.9) in the example below to complete the LDA Inference task on a random sample of documents. To calculate our word distributions in each topic we will use: (NEED TO SWAP I, J’S WITH K,D,W TO MATCH ABOVE) \\[ \\begin{equation} \\begin{aligned} \\phi_{i}^{j} = {C^{WT}_{ij} + \\beta \\over \\sum_{k=1}^{W} C^{WT}_{kj} + W\\beta} \\end{aligned} (\\#eq:phiEstimation) \\end{equation} \\] The topic distribution in each document is calcuated using: \\[ \\begin{equation} \\begin{aligned} \\theta^{(d)}_{j} = {C^{DT}_{dj} + \\alpha \\over \\sum_{k =1}^{T}C_{dk}^{DT} + T\\alpha} \\end{aligned} \\tag{6.10} \\end{equation} \\] Add in the equations for the solution (phi and theta estimates- eq(4)) from (Steyvers and Griffiths 2007) —– Give general description of pseudo code - go through each doc, each word, get probability of topic, assign topic, repeat - this all gets lost in the math - it really is that simple, all the confusing components come from the derivation of the probability calculation …. What if I don’t want to generate docuements. What if my goal is to infer what topics are present in each document and what words belong to each topic? This is were LDA for inference comes into play. Before going through any derivations of how we infer the document topic distributions and the word distributions of each topic, I want to go over the process of inference more generally. The General Idea of the Inference Process Initialization: Randomly select a topic for each word in each document from a multinomial distribution. Gibbs Sampling: For i iterations For document d in documents: For each word in document d: assign a topic to the current word based on probability of the topic given the topic of all other words (except the current word) If you recall from the previous chapters on Gibbs sampling to infer the value of each \\(\\theta\\) we calculate the value of of $p(_{}|) Use Darling as an outline for the derivation process …. cite carpenter and heinrich (same as Darling does). Now let’s revisit the animal example from the first section of the book and break down what we see. This time we will also be taking a look at the code used to generate the example documents as well as the inference code. rm(list = ls()) get_topic &lt;- function(k){ which(rmultinom(1,size = 1,rep(1/k,k))[,1] == 1) } get_word &lt;- function(theta, phi){ topic &lt;- which(rmultinom(1,1,theta)==1) # sample word from topic new_word &lt;- which(rmultinom(1,1,phi[topic, ])==1) return(c(new_word, topic)) } cppFunction( &#39;List gibbsLda( NumericVector topic, NumericVector doc_id, NumericVector word, NumericMatrix n_doc_topic_count,NumericMatrix n_topic_term_count, NumericVector n_topic_sum, NumericVector n_doc_word_count){ int alpha = 1; int beta = 1; int cs_topic,cs_doc, cs_word, new_topic; int n_topics = max(topic)+1; int vocab_length = n_topic_term_count.ncol(); double p_sum = 0,num_doc, denom_doc, denom_term, num_term; NumericVector p_new(n_topics); IntegerVector topic_sample(n_topics); for (int iter = 0; iter &lt; 100; iter++){ for (int j = 0; j &lt; word.size(); ++j){ // change values outside of function to prevent confusion cs_topic = topic[j]; cs_doc = doc_id[j]; cs_word = word[j]; // decrement counts n_doc_topic_count(cs_doc,cs_topic) = n_doc_topic_count(cs_doc,cs_topic) - 1; n_topic_term_count(cs_topic , cs_word) = n_topic_term_count(cs_topic , cs_word) - 1; n_topic_sum[cs_topic] = n_topic_sum[cs_topic] -1; // get probability for each topic, select topic with highest prob for(int tpc = 0; tpc &lt; n_topics; tpc++){ // word cs_word topic tpc + beta num_term = n_topic_term_count(tpc, cs_word) + beta; // sum of all word counts w/ topic tpc + vocab length*beta denom_term = n_topic_sum[tpc] + vocab_length*beta; // count of topic tpc in cs_doc + alpha num_doc = n_doc_topic_count(cs_doc,tpc) + alpha; // total word count in cs_doc + n_topics*alpha denom_doc = n_doc_word_count[cs_doc] + n_topics*alpha; p_new[tpc] = (num_term/denom_term) * (num_doc/denom_doc); } // normalize the posteriors p_sum = std::accumulate(p_new.begin(), p_new.end(), 0.0); for(int tpc = 0; tpc &lt; n_topics; tpc++){ p_new[tpc] = p_new[tpc]/p_sum; } // sample new topic based on the posterior distribution R::rmultinom(1, p_new.begin(), n_topics, topic_sample.begin()); for(int tpc = 0; tpc &lt; n_topics; tpc++){ if(topic_sample[tpc]==1){ new_topic = tpc; } } // print(new_topic) // update counts n_doc_topic_count(cs_doc,new_topic) = n_doc_topic_count(cs_doc,new_topic) + 1; n_topic_term_count(new_topic , cs_word) = n_topic_term_count(new_topic , cs_word) + 1; n_topic_sum[new_topic] = n_topic_sum[new_topic] + 1; // update current_state topic[j] = new_topic; } } return List::create( n_topic_term_count, n_doc_topic_count); } &#39;) # 3 topics - land sea &amp; air # birds and amphibious have cross over # fish - sea 100 # land animals - 100 land beta &lt;- 1 k &lt;- 3 # number of topics M &lt;- 100 # let&#39;s create 10 documents alphas &lt;- rep(1,k) # topic document dirichlet parameters xi &lt;- 100 # average document length N &lt;- rpois(M, xi) #words in each document # whale1, whale2, FISH1, FISH2,OCTO sea_animals &lt;- c(&#39;\\U1F40B&#39;, &#39;\\U1F433&#39;,&#39;\\U1F41F&#39;, &#39;\\U1F420&#39;, &#39;\\U1F419&#39;) # crab, alligator, TURTLE,SNAKE amphibious &lt;- c(&#39;\\U1F980&#39;, &#39;\\U1F40A&#39;, &#39;\\U1F422&#39;, &#39;\\U1F40D&#39;) # CHICKEN, TURKEY, DUCK, PENGUIN birds &lt;- c(&#39;\\U1F413&#39;,&#39;\\U1F983&#39;,&#39;\\U1F426&#39;,&#39;\\U1F427&#39;) # SQUIRREL, ELEPHANT, COW, RAM, CAMEL land_animals&lt;- c(&#39;\\U1F43F&#39;,&#39;\\U1F418&#39;,&#39;\\U1F402&#39;,&#39;\\U1F411&#39;,&#39;\\U1F42A&#39;) vocab &lt;- c(sea_animals, amphibious, birds, land_animals) # equal probability 1/18 # 0 - animals that are not possible # 1 - for shared # 4 - non-shared shared &lt;- 2 non_shared &lt;- 4 not_present &lt;- 0 land_phi &lt;- c(rep(not_present, length(sea_animals)), rep(shared, length(amphibious)), rep(non_shared, 2), # turkey and chicken can&#39;t fly rep(shared, 2), # regular bird and pengiun rep(non_shared, length(land_animals))) land_phi &lt;- land_phi/sum(land_phi) sea_phi &lt;- c(rep(non_shared, length(sea_animals)), rep(shared, length(amphibious)), rep(not_present, 2), # turkey and chicken can&#39;t fly rep(shared, 2), # regular bird and pengiun rep(not_present, length(land_animals))) sea_phi &lt;- sea_phi/sum(sea_phi) air_phi &lt;- c(rep(not_present, length(sea_animals)), rep(not_present, length(amphibious)), rep(not_present, 2), # turkey and chicken can&#39;t fly non_shared, # regular bird not_present, # penguins can&#39;t fly rep(not_present, length(land_animals))) air_phi &lt;- air_phi/sum(air_phi) # calculate topic word distributions phi &lt;- matrix(c(land_phi, sea_phi, air_phi), nrow = k, ncol = length(vocab), byrow = TRUE, dimnames = list(c(&#39;land&#39;, &#39;sea&#39;, &#39;air&#39;))) theta_samples &lt;- rdirichlet(M, alphas) thetas &lt;- theta_samples[rep(1:nrow(theta_samples), times = N), ] new_words &lt;- t(apply(thetas, 1, function(x) get_word(x,phi))) ds &lt;-tibble(doc_id = rep(1:length(N), times = N), word = new_words[,1], topic = new_words[,2], theta_a = thetas[,1], theta_b = thetas[,2], theta_c = thetas[,3] ) ds %&gt;% filter(doc_id &lt; 3) %&gt;% group_by(doc_id) %&gt;% summarise( tokens = paste(vocab[word], collapse = &#39; &#39;) ) %&gt;% kable(col.names = c(&#39;Document&#39;, &#39;Animals&#39;), caption =&quot;Animals at the First Two Locations&quot;) Table 6.1: Animals at the First Two Locations Document Animals 1 🐑 🐋 🐂 🐓 🐑 🦃 🦃 🐪 🦃 🐓 🐑 🐂 🐋 🐪 🐢 🐂 🐑 🐓 🐍 🐑 🐟 🐍 🐙 🐿 🐪 🐘 🐑 🐙 🦃 🐘 🐑 🐂 🐂 🐊 🐂 🐘 🐿 🦃 🦃 🐧 🐪 🐍 🐿 🐘 🐍 🐂 🐙 🐦 🐂 🐟 🐍 🐠 🐑 🦃 🐓 🦃 🐂 🐘 🐠 🐓 🐢 🐘 🦃 🐙 🐘 🐍 🐢 🐑 🐍 🐘 🐦 🐧 🐙 🐦 🐂 🐋 🐘 🐠 🐓 🐿 🐘 🐿 🦀 🐓 🐿 🐊 🐓 🦀 🐂 🐪 🐓 🐠 🐪 🐦 🐙 🐠 🐘 🦃 🐘 🐑 🐊 🐑 🐳 2 🐠 🐟 🐋 🐋 🐢 🐳 🦀 🐦 🐘 🐦 🐓 🐦 🐦 🦃 🐦 🐦 🐂 🦀 🐢 🦀 🐊 🐿 🦃 🐳 🐦 🐠 🐪 🐍 🐠 🐪 🐧 🐂 🐿 🐋 🐦 🐦 🐦 🐿 🐋 🐿 🐠 🐦 🐳 🐂 🐦 🐙 🐟 🐑 🐪 🐪 🦀 🐑 🐊 🐦 🐦 🐿 🦃 🐙 🐿 🦃 🐓 🐊 🐦 🐢 🐙 🐊 🦀 🐙 🐍 🐓 🐍 🐳 🐋 🐙 🐧 🐢 🐋 🐘 🐑 🐍 🐑 🐳 🐦 🐠 🐂 🐧 🐙 🐍 The habitat (topic) distributions for the first couple of documents: Table 6.2: Distribution of Habitats in the First Two Locations Document Land Sea Air 1 0.8168897 0.1688578 0.0142525 2 0.3860697 0.4442854 0.1696449 With the help of LDA we can go through all of our documents and estimate the topic/word distributions and the topic/document distributions. This is our estimated values and our resulting values: ######### Inference ############### current_state &lt;- ds %&gt;% dplyr::select(doc_id, word, topic) current_state$topic &lt;- NA t &lt;- length(unique(current_state$word)) # n_doc_topic_count n_doc_topic_count &lt;- matrix(0, nrow = M, ncol = k) # document_topic_sum n_doc_topic_sum &lt;- rep(0,M) # topic_term_count n_topic_term_count &lt;- matrix(0, nrow = k, ncol = t) # colnames(n_topic_term_count) &lt;- unique(current_state$word) # topic_term_sum n_topic_sum &lt;- rep(0,k) p &lt;- rep(0, k) # initialize topics current_state$topic &lt;- replicate(nrow(current_state),get_topic(k)) # get word, topic, and document counts (used during inference process) n_doc_topic_count &lt;- current_state %&gt;% group_by(doc_id, topic) %&gt;% summarise( count = n() ) %&gt;% spread(key = topic, value = count) %&gt;% as.matrix() n_topic_sum &lt;- current_state %&gt;% group_by(topic) %&gt;% summarise( count = n() ) %&gt;% select(count) %&gt;% as.matrix() %&gt;% as.vector() n_topic_term_count &lt;- current_state %&gt;% group_by(topic, word) %&gt;% summarise( count = n() ) %&gt;% spread(word, count) %&gt;% as.matrix() # minus 1 in, add 1 out lda_counts &lt;- gibbsLda( current_state$topic-1 , current_state$doc_id-1, current_state$word-1, n_doc_topic_count[,-1], n_topic_term_count[,-1], n_topic_sum, N) # calculate estimates for phi and theta # phi - row apply to lda_counts[[1]] # rewrite this function and normalize by row so that they sum to 1 phi_est &lt;- apply(lda_counts[[1]], 1, function(x) (x + beta)/(sum(x)+length(vocab)*beta) ) rownames(phi_est) &lt;- vocab colnames(phi) &lt;- vocab theta_est &lt;- apply(lda_counts[[2]],2, function(x)(x+alphas[1])/(sum(x) + k*alphas[1])) theta_est &lt;- t(apply(theta_est, 1, function(x) x/sum(x))) kable(round(phi_est, 2),col.names = c(&#39;Topic 1&#39;, &#39;Topic 2&#39;, &#39;Topic 3&#39;),caption = &#39;Estimated word distribution for each topic&#39;) Table 6.3: Estimated word distribution for each topic Topic 1 Topic 2 Topic 3 🐋 0.00 0.00 0.12 🐳 0.00 0.02 0.12 🐟 0.00 0.01 0.12 🐠 0.00 0.01 0.12 🐙 0.00 0.01 0.13 🦀 0.00 0.05 0.06 🐊 0.01 0.04 0.06 🐢 0.00 0.05 0.07 🐍 0.01 0.06 0.05 🐓 0.01 0.09 0.00 🦃 0.00 0.11 0.00 🐦 0.92 0.01 0.08 🐧 0.01 0.05 0.06 🐿 0.01 0.09 0.01 🐘 0.00 0.10 0.00 🐂 0.00 0.11 0.00 🐑 0.00 0.10 0.00 🐪 0.00 0.11 0.00 Table 6.4: The word distribution for each topic used to build the documents land sea air 🐋 0.00 0.12 0 🐳 0.00 0.12 0 🐟 0.00 0.12 0 🐠 0.00 0.12 0 🐙 0.00 0.12 0 🦀 0.05 0.06 0 🐊 0.05 0.06 0 🐢 0.05 0.06 0 🐍 0.05 0.06 0 🐓 0.10 0.00 0 🦃 0.10 0.00 0 🐦 0.05 0.06 1 🐧 0.05 0.06 0 🐿 0.10 0.00 0 🐘 0.10 0.00 0 🐂 0.10 0.00 0 🐑 0.10 0.00 0 🐪 0.10 0.00 0 The document topic mixture estimates are shown below for the first 5 documents: ## 1 2 3 ## 1 0.04 0.90 0.06 ## 2 0.19 0.48 0.33 ## 3 0.31 0.13 0.56 ## 4 0.82 0.17 0.01 ## 5 0.31 0.31 0.39 ## 6 0.35 0.11 0.54 Here are our real mixtures for comparison: Table 6.5: The Real Topic Distributions for the First 5 Documents Land Sea Air 0.82 0.17 0.01 0.39 0.44 0.17 0.18 0.42 0.40 0.33 0.03 0.64 0.41 0.30 0.29 0.15 0.53 0.32 "],
["references.html", "References", " References "]
]
