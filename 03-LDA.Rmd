# LDA

## High Level Overview 
Edwin Chen's general explanation of how gibbs sampling is working in a practical sense. 

Pseudo code break down - nesting of for loops so that we can display 
for each topic, for each document ... 

Generally speaking LDA works as follows:
1) Initialization: For each document, go through each word and randomly assign a topic. To be clear the same word in different documents can have different assignments. 
2) Gibbs Sampling/Inference:
Until some condition for convergence is met:

  Repeat the following:
    Go through each document:
      Go through each word in the document:
        Whatever the last topic assignment for that word was - decrement it
        Now using the decremented/updated counts sample a multinomial distriubution (i.e.
        this is our gibbs sampling) to determing the word's next topic assignment. 
      
3) Calculate the values of phi (p(term|topic = k)) and theta (p(topic|document = d)). These are what we want for inference. I now know how likely a word is to show up given a specific topic and the probability of a topic given a specific document. We can think of this as the topic distribution over each document and the word distribution in each topic. From this I can do all kinds of cool things like create topic features for a document - i.e. dimensionality reduction.  
Now comes the nightmare fuel of this book:
The gibbs sampling derivation. If calculus is not your strongest suit, it might be wise to move ahead (provide a link to another section). If you want to see all the nuts and bolts then stick around.....


Gibbs sampling derivation, but needs to be accompanied with some other insight that makes it relatively less horrible to follow along if calculus is not your strength.


Discussion of Gibbs sampling used in LDA inference task 
Make the following points:
This though is somewhat incoherent and needs to be cleaned up, but this is how to drive home the point of what is actually happening without tons of calculus. 

1) This is similar to what we did in the bernoulli example as well as the change detection example. We identify our estimate for the posterior and plug in our counts and parameter values during each iteration. For example in the bernoulli/beta example, we estimate theta 1, then theta 2, then move to the next iteration and repeat. In the timeseries example it is a bit more complicated as our parameter estimations often feed the next parameter estimation. This was shown in the change point detection example. This is very similar to what is happening in each iteration of gibbs sampling for our topic assignment. This is the reason for the decrement in each step - we can't use the current parameter in the estimation of our current parameter, it has to be taken out of the equation. However we are going to use the new topic assignment given to all the other words during our estimations. 

## Components
Words
Documents
Topics
Alphas
Beta

## Generative Model

IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

(NOTE: You need to figure out a way to connect the code example of generating words/docs back to the math (or at least one of the other symbolic representations). This is necesary for explaining how to flip to inferenece. It is fine if the inference part leaves out the long process of derivation, but you need to be able to unstack the generative model so that the transfer to inference makes sense - this has been covered at the end of chapter 2 in the multinomial/dirichlet example, but should be expanded to show how plate diagrams work.....ugh I hate these)


LDA is know as a generative model. What is a generative model? Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space [@bishop2006pattern]. This means we can create documents with a mixture of topics and a mixture of words based on thosed topics. Let's start off with a simple example of generating unigrams. 


### Generating Documents 

#### Topic Word Mixtures, Document Topic Mixtures, and Document Length Known

Building on the document generating model in chapter two, let's try to create documents that have words drawn from more than one topic. To clarify the contraints of the model will be:

* set number of topics (2)
* constant topic distributions in each document
* constant word distribution in each topic 


Known values:
* 2 topics : word distributions of each topic below
    * $\phi_{1} = [ red = 0.8, blue = 0.2, green = 0.0 ]$
    * $\phi_{2} = [ red = 0.2, blue = 0.1, green = 0.7 ]$
* All Documents have same topic distribution:
    * $\theta = [ topic a = 0.5, topic b = 0.5 ]$
* All Documents contain 10 words
    

* For d = 1 to D where D is the number of documents
    * For w = 1 to W  where W is the number of words in document *d*
        + *Select the topic for word *w* *
        + $z_{i}$ ~ Multinomial($\theta_{d}$)
        + *Select word based on topic *z*'s word distribution  
        + $w_{i}$ ~ Multinomial($\phi^{(z_{i})})


```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(MCMCpack)
library(tidyverse)
library(knitr)
library(kableExtra) 
```



```{r, echo=TRUE, warning=FALSE, message=FALSE} 

k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters

phi <- matrix(c(0.1, 0, 0.9,
                0.4, 0.4, 0.2), 
              nrow = k, 
              ncol = length(vocab), 
              byrow = TRUE)

theta <- c(0.5, 0.5)

N <- 10 #words in each document
ds <-tibble(doc_id = rep(0,N*M), 
            word = rep('', N*M),
            topic = rep(0, N*M)
            ) 
            
row_index <- 1
for(m in 1:M){
  for(n in 1:N){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic)
    row_index <- row_index + 1
  }
}

ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word, collapse = ' ')
) %>% kable()
```
#### Topic Word Mixtures, Document Topic Mixtures, and Document Length Unknown

```{r} 
k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters

phi <- matrix(c(0.1, 0, 0.9,
                0.4, 0.4, 0.2), 
              nrow = k, 
              ncol = length(vocab), 
              byrow = TRUE)

theta <- c(0.5, 0.5)
xi <- 10 # average document length 
N <- rpois(M, xi) #words in each document
ds <-tibble(doc_id = rep(0,sum(N)), 
            word   = rep('', sum(N)),
            topic  = rep(0, sum(N))
            ) 
            
row_index <- 1
for(m in 1:M){
  for(n in 1:N[m]){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic)
    row_index <- row_index + 1
  }
}

ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word, collapse = ' ')
) %>% kable()
```

#### Static Topic Word Mixtures, Varying Document Topic Distributions

So this time we will introduce documents with different topic distributions. The document length is still known, as is the topic word mixtures, but we will be randomly sampling the topic distribution of each document. 
Provide an example of document generation based on a set # of topics, with set topic word distributions and set document topic distributions

Known values:
* 2 topics : word distributions of each topic below
    * $\phi_{1} = [ , ]$
    * $\phi_{2} = [ , ]$
* All Documents have same topic distribution:
    * $\theta = [ , ]$
* All Documents contain 10 words
    

* For d = 1 to D where D is the number of documents
    * For w = 1 to W  where W is the number of words in document *d*
        + *Select the topic for word *w* *
        + $z_{i}$ ~ Multinomial($\theta_{d}$)
        + *Select word based on topic *z*'s word distribution  
        + $w_{i}$ ~ Multinomial($\phi^{(z_{i})})



The generative process for LDA is shown below. 

1. For k = 1 to K where K is the total number of topics
    + *Sample parameters for word distribution of each topic*
    + $\phi^{(k)}$ ~ Dirichlet($\beta$) 
2. For d = 1 to D where number of documents is D
    + *Sample parameters for document topic distribution*
    + $\theta_{d}$ ~ Dirichlet($\alpha$)
    + For w = 1 to W  where W is the number of words in document *d*
        + *Select the topic for word *w* *
        + $z_{i}$ ~ Multinomial($\theta_{d}$)
        + *Select word based on topic *z*'s word distribution  
        + $w_{i}$ ~ Multinomial($\phi^{(z_{i})})




Then change example to be set # of topics, set topic word distributions, but document topic distributions are drawn from dirichlet

Then go to full LDA (example below)


```{r  echo = TRUE, warning=FALSE, message=FALSE}

k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters

phi <- matrix(c(0.1, 0, 0.9,
                0.4, 0.4, 0.2), 
              nrow = k, 
              ncol = length(vocab), 
              byrow = TRUE)


xi <- 10 # average document length 
N <- rpois(M, xi) #words in each document
ds <-tibble(doc_id = rep(0,sum(N)), 
            word   = rep('', sum(N)),
            topic  = rep(0, sum(N)), 
            theta_a = rep(0, sum(N)),
            theta_b = rep(0, sum(N))
            ) 
            
row_index <- 1
for(m in 1:M){
  theta <-  rdirichlet(1, alphas)
  
  for(n in 1:N[m]){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic,theta)
    row_index <- row_index + 1
  }
}

ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word, collapse = ' '), 
  topic_a = round(as.numeric(unique(theta_a)), 2), 
  topic_b = round(as.numeric(unique(theta_b)), 2) 
) %>% kable()

```

### LDA Generative Model 

```{r}

```
(NOTE: Spread the table so topics are side by side for each doc)

```{r r GenerativeTable, echo = FALSE, warning=FALSE, message=FALSE}
ds %>% group_by(document, topic) %>% 
  summarise(words = paste(word, collapse = ' '),
            topic_distribution_parameter = if_else(topic[1] == 1, prop_a[1], prop_b[1])
            ) %>% arrange(document, topic) %>%
  kable() %>% kable_styling(bootstrap_options = 'striped')

```

## Inference
What if I don't want to generate docuements. What if my goal is to infer what topics are present in each document and what words belong to each topic? This is were LDA for inference comes into play.


Use Darling as an outline for the derivation process .... cite carpenter and heinrich (same as Darling does). 


Toy Code example of inference on a simple set of docs - maybe another emoji example. 


