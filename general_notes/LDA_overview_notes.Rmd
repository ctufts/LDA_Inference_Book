

# BI/GIBBS

Where does the evidence term go? 

Make an example of this and cite Krutchke - I assume in the earlier chapters there is something present. 

For explanation #1 you can use the metropolis hastings algorithm examples. You are 
sampling from a posterior without a normalizing factor, but you use the proportions
 of the samples to determine the probability (i.e. you infer the normalization 
 term through sampling - it is basically built in at that point.)
[Explanation 1: The normalizing constant is not interesting, this is very common in bayesian statistics, . With Gibbs sampling, Metropolis-Hasting or any other Monte Carlo method, what you are doing is drawing samples from this posterior. That is, the more density around a point, the more samples you'll get there.

Then, once you have enough samples from this posterior distribution, you know that the normalized density at some point xx is the proportion of samples that felt at that point.

You can even plot an histogram on the samples to see the (unnormalized) posterior.

In other words, if I give you the samples 1,3,4,5,1.....,3,4,16,11,3,4,5,1.....,3,4,16,1 and I tell you these are samples from a density function, you know to compute the probability of every value.


..... Explanation 3 If you use conjugate priors for the individual variables, the denominator of their conditional probability will be always nice and familiar, and you will know what the normalizing constant in the denominator is. This is why Gibbs sampling is so popular when the joint probability is ugly but the conditional probabilities are nice.](https://stats.stackexchange.com/questions/138644/confusion-in-gibbs-sampling#138648)

For explanation #3 use the coin flip example... 
p(theta1, theta2|D) show how denominator p(d) drops out. 


# LDA

* Ways to explain LDA: 
  * generative to inference - this gets done regardless in the LDA chapters. I would open with the 
    animal example, then preface it by saying 'we are going to go through all the steps it takes to get to     this
    * What holes does this have?
      * Reader is an asshole like me that learns of LDA as a way to infer/summarise info
        and therefore only wants to skip to that part - we know this doesn't work
  * Edwin Chen style explanation
    * This sort of works but leave you sort of confused (especially his example)
      This might be a good spot to add in your animal example as I think it is easier to follow


To review book structure should go as follows:
* Intro to LDA and why we want it/what it is used for/etc.
  * High level with a code example only for showing that it 'DOES' work
  * We can generate docs or we can infer information from them 
    * start with inference because that is the one most LDA users are looking for
      * even though LDA can generate docs, they will be bag of words which isn't all that helpful
* Parameter estimation techniques (bernoulli, binomial)
* Introduction of Multinomial, expand on Gibbs sampling examples from prior chapter
* LDA - generative
* LDA - inferrence



IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

(NOTE: You need to figure out a way to connect the code example of generating words/docs back to the math (or at least one of the other symbolic representations). This is necesary for explaining how to flip to inferenece. It is fine if the inference part leaves out the long process of derivation, but you need to be able to unstack the generative model so that the transfer to inference makes sense - this has been covered at the end of chapter 2 in the multinomial/dirichlet example, but should be expanded to show how plate diagrams work.....ugh I hate these)


Before getting into code examples and the math, let's talk about what LDA is in more general terms. We've went over a model to generate a document based on predefined word proportions. LDA is essentially a more complicated extension of this example. It has two levels of latent structures: topic mixtures for each document and word mixtures for each topic. Based on these latent structures we can create a model for generating documents that are a combination of topics which have predefined word distributions. 

Edwin Chen's general explanation of how gibbs sampling is working in a practical sense. 

Pseudo code break down - nesting of for loops so that we can display 
for each topic, for each document ... 

Generally speaking LDA works as follows:
1) Initialization: For each document, go through each word and randomly assign a topic. To be clear the same word in different documents can have different assignments. 
2) Gibbs Sampling/Inference:
Until some condition for convergence is met:

Repeat the following:
Go through each document:
Go through each word in the document:
Whatever the last topic assignment for that word was - decrement it
Now using the decremented/updated counts sample a multinomial distriubution (i.e.
this is our gibbs sampling) to determing the word's next topic assignment. 

3) Calculate the values of phi (p(term|topic = k)) and theta (p(topic|document = d)). These are what we want for inference. I now know how likely a word is to show up given a specific topic and the probability of a topic given a specific document. We can think of this as the topic distribution over each document and the word distribution in each topic. From this I can do all kinds of cool things like create topic features for a document - i.e. dimensionality reduction.  
Now comes the nightmare fuel of this book:
  The gibbs sampling derivation. If calculus is not your strongest suit, it might be wise to move ahead (provide a link to another section). If you want to see all the nuts and bolts then stick around.....


Gibbs sampling derivation, but needs to be accompanied with some other insight that makes it relatively less horrible to follow along if calculus is not your strength.


Discussion of Gibbs sampling used in LDA inference task 
Make the following points:
  This though is somewhat incoherent and needs to be cleaned up, but this is how to drive home the point of what is actually happening without tons of calculus. 

1) This is similar to what we did in the bernoulli example as well as the change detection example. We identify our estimate for the posterior and plug in our counts and parameter values during each iteration. For example in the bernoulli/beta example, we estimate theta 1, then theta 2, then move to the next iteration and repeat. In the timeseries example it is a bit more complicated as our parameter estimations often feed the next parameter estimation. This was shown in the change point detection example. This is very similar to what is happening in each iteration of gibbs sampling for our topic assignment. This is the reason for the decrement in each step - we can't use the current parameter in the estimation of our current parameter, it has to be taken out of the equation. However we are going to use the new topic assignment given to all the other words during our estimations. 



Issues: 
Need to understand conditional independence as it relates to words/topics in LDA. I get the concept from here: 
https://en.wikipedia.org/wiki/Conditional_independence
but need to make a connection back to how/why it applies here. 

Looking at the loops for the generative model makes this pretty straight forward, you can see the selected topic is only dependent on the document/topic distributions which are preselected based on a prior. The same can be said about the word selected as it is only dependent on the topic selected, not the any of the 'non-selected' topics. This is my intuition, but I need to make sure this actually works as an explanation. 