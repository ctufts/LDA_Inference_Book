# LDA Inference

## General Overview
We have talked about LDA as a generative model, but now it is time to flip the problem around. What if I have a bunch of documents and I want to infer topics? To figure out a problem like this we are first going to assume the documents were generated using a generative model similar to the ones we created in the previous section. 



Now it is time to flip the problem around. What happens when I have a bunch of documents and I want to know what topics are present in each document and what words are present (most probable) in each topic? In terms of math, the values we need to answer the questions above are:

$$
\begin{equation}
p(\theta, \phi, z|w, \alpha, \beta) = {p(\theta, \phi, z, w|\alpha, \beta) \over p(w|\alpha, \beta)}
(\#eq:LDAInference)
\end{equation}
$$
Equation \@ref(eq:LDAInference) says the following:   
The probability of the topic proportion, the word distribution of each topic, and the topic label given a word (in a document) and the hyperparameters $\alpha$ and $\beta$. In a more basic term this means the probability of a specific topic _z_ given the word _w_ (and our prior assumptions, i.e. hyperparameters), once the topic label of a word is known, then we can answer derive $\phi$ and $\theta$.


Let's take a step from the math and just map out things we know versus the things we don't know in our current scenario:

**Known Parameters**

* **Documents:** We have a set number of documents we want to identify the topic strucutres in. 
* **Words:** We have a collection of words and word counts for each document which we place in a document word matrix. 
* **Vocabulary:** The unique list of words across all documents. 
* **Hyperparemeters:**
    * $\overrightarrow{\alpha}$
    * $\overrightarrow{\beta}$

And on to the parts we don't know....

** Unknown (Latent) Parameters **

* **Number of Topics:** We need to determine the number of topics we assume are present in the documents. For the purposes of this book I'm going to skip how to estimate this number properly. If you are interested in learning more about how to estimate the number of topics present in a document see REFERENCE LIST. 
* **Document Topic Mixture:** We need to determine the topic distribution in each document. 
* **Word Distribution of Each Topic:** We need to know the distribution of words in each topic. Obviously some words are going to occur very often in a topic while others may have zero probability of occurring in a topic. 
* **Word topic assignment:** This is actually the main thing we need to infer. To be clear, if we know the topic assignment of every word in every document, then we can derive the document topic mixture, $\theta$, and the word distribution, $\phi$, of each topic. 


_Quick note:   
Equation \@ref(eq:LDAInference) is based on the following statistical property_


$$
\begin{equation}
p(A, B | C) = {p(A,B,C) \over p(C)}
(\#eq:jointConditional)
\end{equation}
$$

The derivation connecting equation \@ref(eq:LDAInference) to the actual Gibbs sampling solution to determine _z_ for each word in each document, $\overrightarrow{\theta}$, and 
$\overrightarrow{\phi}$ is very complicated and I'm just going to gloss over a few steps. For complete derivations see (site heinrich and that other one from darling). 

As stated previously, the main goal of inference in LDA is to determine the topic of each word, $z_{i}$ (topic of word _i_), in each document. 

$$
\begin{equation}
p(z_{i}|z_{\neg i}, \alpha, \beta, w)
(\#eq:zPosteriorCond)
\end{equation}
$$


Notice that we are interested in identifying the topic of the current word, $z_{i}$, based on the topic assignments of all other words (not including word _i_), $z_{\neg i}$. The following derivations are all from [@darling2011theoretical]

$$
\begin{equation}
\begin{aligned}
p(z_{i}|z_{\neg i}, \alpha, \beta, w) 
  &= {p(z_{i},z_{\neg i}, w, | \alpha, \beta)    \over p(z_{\neg i},w | \alpha,   
  \beta)}\\
&\propto p(z_{i}, z_{\neg i}, w | \alpha, \beta)\\
&\propto p(z,w|\alpha, \beta)
\end{aligned}
(\#eq:zwConditional)
\end{equation}
$$

You may notice $p(z,w|\alpha, \beta)$ looks very similar to the definition of the generative process of LDA (cite Darling) from the previous chapter (equation \@ref(eq:generativeLDA)). The only difference is the absence of $\theta$ and $\phi$. This means we can swap in equation \@ref(eq:generativeLDA) and integrate out $\theta$ and $\phi$. 


$$
\begin{equation}
\begin{aligned}
p(w,z|\alpha, \beta) &= \int \int p(z, w, \theta, \phi|\alpha, \beta)d\theta d\phi\\
&= \int \int p(\phi|\beta)p(\theta|\alpha)p(z|\theta)p(w|\phi_{z})d\theta d\phi \\
&= \int p(z|\theta)p(\theta|\alpha)d \theta \int p(w|\phi_{z})p(\phi|\beta)d\phi
\end{aligned}
(\#eq:IntegrateOutPhiTheta)
\end{equation}
$$

As with previous examples in this book we are now going to expand equation \@ref(eq:zwConditional), plug in our conjugate priors, and get to a point where we can use a gibbs sampler to estimate our solution. 

ADD IN A NOTE ABOUT GIBBS SAMPLING FOR THE UNITIATED - PAPER - this follows the exact steps laid out in that paper. 


topic assignment/topic proportion of documents. i - documents, k - topics


Below you will see the derivation for the first term of equation \@ref(eq:IntegrateOutPhiTheta) which to identify the sampling distribution for the topic distribution in each document. The conjugate prior is utilized. The result is a dirichlet distribution with the parameter comprised of the sum of the number of words assigned each topic and the alpha value for that topic. (This needs to be more clear...)

$$
\begin{equation}
\begin{aligned}
\int p(z|\theta)p(\theta|\alpha)d \theta &= \int \prod_{i}{\theta_{d_{i},z_{i}}{1\over B(\alpha)}}\prod_{k}\theta_{d,k}^{\alpha k}\theta_{d} \\
&={1\over B(\alpha)} \int  \prod_{k}\theta_{d,k}^{n_{d,k} + \alpha k} \\
&={B(n_{d,.} + \alpha) \over B(\alpha)}
\end{aligned}
(\#eq:topicDirDerivation)
\end{equation}
$$

Similarly we can expand the second term of Equation \@ref(eq:IntegrateOutPhiTheta) and we find a solution with a similar form. 

$$
\begin{equation}
\begin{aligned}
\int p(w|\phi_{z})p(\phi|\beta)d\phi
  &= \int \prod_{d}\prod_{i}\phi_{z_{d,i},w_{d,i}}
  \prod_{k}{1 \over B(\beta)}\prod_{w}\phi^{B_{w}}_{k,w}d\phi_{k}\\
&= \prod_{k}{1\over B(\beta)} \int \prod_{w}\phi_{k,w}^{B_{w} +   
  n_{k,w}}d\phi_{k}\\
&=\prod_{k}{B(n_{k,.} + \beta) \over B(\beta)}
\end{aligned}
(\#eq:wordDirDeriv)
\end{equation}
$$

This leaves us with the following:

$$
\begin{equation}
\begin{aligned}
p(w,z|\alpha, \beta) &= 
  \prod_{d}{B(n_{d,.} + \alpha) \over B(\alpha)}
  \prod_{k}{B(n_{k,.} + \beta) \over B(\beta)}
\end{aligned}
(\#eq:postConditionalDirs)
\end{equation}
$$

The equation necessary for Gibbs sampling can be derived by utilizing \@ref(eq:postConditionalDirs).  This is accomplished via the chain rule. NEED TO ADD IN MORE INFORMATION ON THIS - DEFINITELY HAVE NOTES ABOUT THIS SOMEWHERE. 

$$
\begin{equation}
\begin{aligned}
p(z_{i}|z^{(-i)}, w) &= {p(w,z)\over {p(w,z^{(-i)})}} = {p(z)\over p(z^{(-i)})}{p(w|z)\over p(w^{(-i)}|z^{(-i)})p(w_{i})}\\
\\
&\propto \prod_{d}{B(n_{d,.} + \alpha) \over B(n_{d,.}^{(-i)}\alpha)}
  \prod_{k}{B(n_{k,.} + \beta) \over B(n_{k,.}^{(-i)} + \beta)}\\
  \\
&\propto {\Gamma(n_{d,k} + \alpha_{k})
  \Gamma(\sum_{k=1}^{K} n_{d,k}^{(-i)} + \alpha_{k}) \over 
  \Gamma(n_{d,k}^{(-i)} + \alpha_{k}) 
  \Gamma(\sum_{k=1}^{K} n_{d,k}+ \alpha_{k})}
  {\Gamma(n_{k,w} + \beta_{w})
  \Gamma(\sum_{w=1}^{W} n_{k,w}^{(-i)} + \beta_{w}) \over 
  \Gamma(n_{k,w}^{(-i)} + \beta_{w}) 
  \Gamma(\sum_{w=1}^{W} n_{k,w}+ \beta_{w})}\\
  \\
&\propto (n_{d,k}^{(-i)} + \alpha_{k}) {n_{k,w}^{(-i)} + \beta_{w} \over 
  \sum_{w'} n_{k,w'}^{(-i)} + \beta_{w'}}
\end{aligned}
(\#eq:LDAGibbsInference)
\end{equation}
$$


Need to explain the difference between primes/non-primes (w's)
We will now use Equation \@ref(eq:LDAGibbsInference) in the example below to complete the LDA Inference task on a random sample of documents. To calculate our word distributions in each topic we will use:
(NEED TO SWAP I, J'S WITH K,D,W TO MATCH ABOVE)
$$
\begin{equation}
\begin{aligned}
\phi_{i}^{j} = {C^{WT}_{ij}  + \beta \over  \sum_{k=1}^{W} C^{WT}_{kj} + W\beta}
\end{aligned}
(\#eq:phiEstimation)
\end{equation}
$$

The topic distribution in each document is calcuated using:

$$
\begin{equation}
\begin{aligned}
\theta^{(d)}_{j} = {C^{DT}_{dj} + \alpha \over \sum_{k =1}^{T}C_{dk}^{DT} + T\alpha}
\end{aligned}
(\#eq:thetaEstimation)
\end{equation}
$$

Add in the equations for the solution (phi and theta estimates- eq(4)) from [@steyvers2007probabilistic]





----- Give general description of pseudo code 
- go through each doc, each word, get probability of topic, assign topic, repeat
  - this all gets lost in the math - it really is that simple, all the confusing components 
come from the derivation of the probability calculation ....


What if I don't want to generate docuements. What if my goal is to infer what topics are present in each document and what words belong to each topic? This is were LDA for inference comes into play.

Before going through any derivations of how we infer the document topic distributions and the word distributions of each topic, I want to go over the process of inference more generally. 

<b>The General Idea of the Inference Process</b>

1. <b>Initialization:</b> Randomly select a topic for each word in each document from a multinomial distribution. 
2. <b>Gibbs Sampling:</b>  
* For <i>i</i> iterations
* For document d in documents:
* For each word in document d:
* <i>assign a topic to the current word based on probability of the topic given the topic of all other words (except the current word)</i>   

If you recall from the previous chapters on Gibbs sampling to infer the value of each $\theta$ we calculate the value of of 
$p(\theta_{}|)

Use Darling as an outline for the derivation process .... cite carpenter and heinrich (same as Darling does). 


Below is a toy example that creates a set of documents based on the 3 word (emoji) vocabulary. Each of the 10 documents has a differnt topic mixture and is assigned a random lenght. Our aim with inferrence is to infer the topic mixture of each document and the word distributions of each topic. 

```{r ldaInference, cache = TRUE}
get_topic <- function(k){ 
which(rmultinom(1,size = 1,rep(1/k,k))[,1] == 1)
} 


k <- 2 # number of topics
M <- 10 # let's create 10 documents
vocab <- c('\U1F4D8', '\U1F4D5', '\U1F4D7')
alphas <- rep(1,k) # topic document dirichlet parameters
beta <- 1


phi <- matrix(c(0.1, 0, 0.9,
                0.4, 0.4, 0.2), 
              nrow = k, 
              ncol = length(vocab), 
              byrow = TRUE)


xi <- 100 # average document length 
N <- rpois(M, xi) #words in each document
ds <-tibble(doc_id = rep(0,sum(N)), 
            word   = rep('', sum(N)),
            topic  = rep(0, sum(N)), 
            theta_a = rep(0, sum(N)),
            theta_b = rep(0, sum(N))
) 

row_index <- 1
for(m in 1:M){
  theta <-  rdirichlet(1, alphas)
  
  for(n in 1:N[m]){
    # sample topic index , i.e. select topic
    topic <- which(rmultinom(1,1,theta)==1)
    # sample word from topic
    new_word <- vocab[which(rmultinom(1,1,phi[topic, ])==1)]
    ds[row_index,] <- c(m,new_word, topic,theta)
    row_index <- row_index + 1
  }
}

ds$doc_id <- as.numeric(ds$doc_id)



ds %>% group_by(doc_id) %>% summarise(
  tokens = paste(word, collapse = ' '), 
  topic_a = round(as.numeric(unique(theta_a)), 2), 
  topic_b = round(as.numeric(unique(theta_b)), 2) 
) %>% kable()


######### Inference ############### 



current_state <- ds %>% dplyr::select(doc_id, word, topic)
current_state$topic <- NA


t <- length(unique(current_state$word))

# n_doc_topic_count  
n_doc_topic_count <- matrix(0, nrow = m, ncol = k)
# document_topic_sum
n_doc_topic_sum  <- rep(0,m)
# topic_term_count
n_topic_term_count <- matrix(0, nrow = k, ncol = t)
colnames(n_topic_term_count) <- unique(current_state$word)
# topic_term_sum
n_topic_sum  <- rep(0,k)
p <- rep(0, k)
# initialize topics
for( i in 1:nrow(current_state)){
  current_state$topic[i] <- get_topic(k)
  n_doc_topic_count[current_state$doc_id[i],current_state$topic[i]] <- n_doc_topic_count[current_state$doc_id[i],current_state$topic[i]] + 1
  n_doc_topic_sum[current_state$doc_id[i]] <- n_doc_topic_sum[current_state$doc_id[i]] + 1
  n_topic_term_count[current_state$topic[i] , current_state$word[i]] <- n_topic_term_count[current_state$topic[i] ,
                                                                                           current_state$word[i]] + 1
  n_topic_sum[current_state$topic[i]] = n_topic_sum[current_state$topic[i]] + 1
  
}



# topics mixtures keep converging to 1 an 0, this is incorrect,
# or it is a result fo the topics (word distributions) being too similar? 
# gibbs
for (iter in 1:100){
  
  for(j in 1:nrow(current_state)){
    
    # decrement counts
    cs_topic <- current_state$topic[j]
    cs_doc   <- current_state$doc_id[j]
    cs_word  <- current_state$word[j]
    
    n_doc_topic_count[cs_doc,cs_topic] <- n_doc_topic_count[cs_doc,cs_topic] - 1
    n_doc_topic_sum[cs_doc] <- n_doc_topic_sum[cs_doc] - 1
    n_topic_term_count[cs_topic , cs_word] <- n_topic_term_count[cs_topic , cs_word] - 1
    n_topic_sum[cs_topic] = n_topic_sum[cs_topic] -1
    
    # get probability for each topic, select topic with highest prob
    for(topic in 1:k){
      p[topic] <- (n_topic_term_count[topic, cs_word] + beta) *
        (n_doc_topic_count[cs_doc,topic] + alphas[k])/
        sum(n_topic_term_count[topic,] + beta)
    }
    new_topic <- which.max(p)
    
    # update counts
    n_doc_topic_count[cs_doc,new_topic] <- n_doc_topic_count[cs_doc,new_topic] + 1
    n_doc_topic_sum[cs_doc] <- n_doc_topic_sum[cs_doc] + 1
    n_topic_term_count[new_topic , cs_word] <- n_topic_term_count[new_topic , cs_word] + 1
    n_topic_sum[new_topic] = n_topic_sum[new_topic] + 1
    
    
    # update current_state
    current_state$topic[j] <- new_topic
  }
}
```


