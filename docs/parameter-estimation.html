<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>LDA Tutorial</title>
  <meta name="description" content="LDA Tutorial">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="LDA Tutorial" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="LDA Tutorial" />
  <meta name="twitter:site" content="@devlintufts" />
  
  

<meta name="author" content="Chris Tufts">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="what-is-lda.html">
<link rel="next" href="multinomial-distribution.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="" data-path="layout-of-book.html"><a href="layout-of-book.html"><i class="fa fa-check"></i>Layout of Book</a></li>
<li class="chapter" data-level="" data-path="package-references.html"><a href="package-references.html"><i class="fa fa-check"></i>Package References</a></li>
<li class="chapter" data-level="1" data-path="what-is-lda.html"><a href="what-is-lda.html"><i class="fa fa-check"></i><b>1</b> What is LDA?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#animal-generator"><i class="fa fa-check"></i><b>1.1</b> Animal Generator</a><ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#generating-the-mixtures"><i class="fa fa-check"></i><b>1.1.1</b> Generating the Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-lda.html"><a href="what-is-lda.html#inference"><i class="fa fa-check"></i><b>1.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#distributions"><i class="fa fa-check"></i><b>2.1</b> Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bernoulli"><i class="fa fa-check"></i><b>2.1.1</b> Bernoulli</a></li>
<li class="chapter" data-level="2.1.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#beta-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#inference-the-building-blocks"><i class="fa fa-check"></i><b>2.2</b> Inference: The Building Blocks</a></li>
<li class="chapter" data-level="2.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-likelihood"><i class="fa fa-check"></i><b>2.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-a-posteriori"><i class="fa fa-check"></i><b>2.4</b> Maximum a Posteriori</a></li>
<li class="chapter" data-level="2.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bayesian-inference"><i class="fa fa-check"></i><b>2.5</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.5.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#analytical-solution"><i class="fa fa-check"></i><b>2.5.1</b> Analytical Solution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.6</b> Gibbs Sampling</a><ul>
<li class="chapter" data-level="2.6.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#the-issue-of-intractability"><i class="fa fa-check"></i><b>2.6.1</b> The Issue of Intractability</a></li>
<li class="chapter" data-level="2.6.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#a-tale-of-two-mcs"><i class="fa fa-check"></i><b>2.6.2</b> A Tale of Two MC’s</a></li>
<li class="chapter" data-level="2.6.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#conjugate-distributions-and-priors"><i class="fa fa-check"></i><b>2.6.3</b> Conjugate Distributions and Priors</a></li>
<li class="chapter" data-level="2.6.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#gibbs-sampling-1"><i class="fa fa-check"></i><b>2.6.4</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.6.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bias-of-two-coins"><i class="fa fa-check"></i><b>2.6.5</b> Bias of Two Coins</a></li>
<li class="chapter" data-level="2.6.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html#change-point-example"><i class="fa fa-check"></i><b>2.6.6</b> Change Point Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html"><i class="fa fa-check"></i><b>3</b> Multinomial Distribution</a><ul>
<li class="chapter" data-level="3.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#comparison-of-dice-vs.words"><i class="fa fa-check"></i><b>3.1</b> Comparison of Dice vs. Words</a></li>
<li class="chapter" data-level="3.2" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#how-multinomial-and-bernoulli-relate"><i class="fa fa-check"></i><b>3.2</b> How Multinomial and Bernoulli Relate</a></li>
<li class="chapter" data-level="3.3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#conjugate-prior-dirichlet"><i class="fa fa-check"></i><b>3.3</b> Conjugate Prior: Dirichlet</a></li>
<li class="chapter" data-level="3.4" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#gibbs-sampling---multinomial-dirichlet"><i class="fa fa-check"></i><b>3.4</b> Gibbs Sampling - Multinomial &amp; Dirichlet</a><ul>
<li class="chapter" data-level="3.4.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#derivation-of-gibbs-sampling-solution-of-word-distribution-single-doc"><i class="fa fa-check"></i><b>3.4.1</b> Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="word-representations.html"><a href="word-representations.html"><i class="fa fa-check"></i><b>4</b> Word Representations</a><ul>
<li class="chapter" data-level="4.1" data-path="word-representations.html"><a href="word-representations.html#bag-of-words"><i class="fa fa-check"></i><b>4.1</b> Bag of Words</a></li>
<li class="chapter" data-level="4.2" data-path="word-representations.html"><a href="word-representations.html#word-counts"><i class="fa fa-check"></i><b>4.2</b> Word Counts</a></li>
<li class="chapter" data-level="4.3" data-path="word-representations.html"><a href="word-representations.html#plug-and-play-lda"><i class="fa fa-check"></i><b>4.3</b> Plug and Play LDA</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html"><i class="fa fa-check"></i><b>5</b> LDA as a Generative Model</a><ul>
<li class="chapter" data-level="5.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#general-terminology"><i class="fa fa-check"></i><b>5.1</b> General Terminology</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#selecting-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Selecting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generative-model"><i class="fa fa-check"></i><b>5.2</b> Generative Model</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generating-documents"><i class="fa fa-check"></i><b>5.2.1</b> Generating Documents</a></li>
<li class="chapter" data-level="5.2.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#lda-generative-model"><i class="fa fa-check"></i><b>5.2.2</b> LDA Generative Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lda-inference.html"><a href="lda-inference.html"><i class="fa fa-check"></i><b>6</b> LDA Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="lda-inference.html"><a href="lda-inference.html#general-overview"><i class="fa fa-check"></i><b>6.1</b> General Overview</a></li>
<li class="chapter" data-level="6.2" data-path="lda-inference.html"><a href="lda-inference.html#mathematical-derivations-for-inference"><i class="fa fa-check"></i><b>6.2</b> Mathematical Derivations for Inference</a></li>
<li class="chapter" data-level="6.3" data-path="lda-inference.html"><a href="lda-inference.html#animal-farm---code-example"><i class="fa fa-check"></i><b>6.3</b> Animal Farm - Code Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">LDA Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parameter-estimation" class="section level1">
<h1><span class="header-section-number">2</span> Parameter Estimation</h1>
<p>LDA is a generative probabilistic model, so to understand exactly how this works we need to understand the underlying probability distributions. In this chapter we will focus on the Bernoulli distribution and the Beta distribution. Both of these distributions are very closely related to (and also special cases of) the multinomial and Dirichlet distributions utilized by LDA, but they are a bit easier to comprehend. Once we have made our way through Bernoulli and beta, we will go over how they are linked to the multinomial and Dirichlet distributions.</p>
<p>Throughout the chapter I’m going to build off of a simple example - a single coin flip.</p>
<div id="distributions" class="section level2">
<h2><span class="header-section-number">2.1</span> Distributions</h2>
<div id="bernoulli" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Bernoulli</h3>
<p>When you flip a coin you get either heads or tails as an outcome (barring the possibility it lands on it’s side). This single coin flip is an example of a Bernoulli trial and we can use the Bernoulli distribution to calculate the probability of either outcome. Any <i>single</i> trial with two possible outcomes can be modeled as a Bernoulli trial: team wins/loses, pitch is a strike/ball, coin comes up heads or tails, etc.</p>
<div id="bernoulli-a-special-case-of-the-binomial-distribution" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Bernoulli: A Special Case of the Binomial Distribution</h4>
<p>You will often see Bernoulli distribution mentioned as a special case of the Binomial distribution. The binomial model consists of <em>n</em> Bernoulli trials, where each trial is independent and the probability of success does not change between trials.<span class="citation">(Kerns <a href="references.html#ref-kerns2010introduction">2010</a>)</span> The Bernoulli distribution is the case of a single trial (<em>n</em>=1).</p>
<p><em>Quick Note:<br />
I will use the term <i>success</i> interchangably with the term <i>heads</i> when describing Bernoulli distribution. In reality <i>success</i> could be <i>tails</i> if you choose to define it that way. </em></p>
<p>To clarify, if I want to calculate the probability of getting heads on a single coin flip I will use a bernoulli distribution. However, if I want to know the probability of getting 2 heads (or more) in a row, we would use the binomial distribution. For our purposes we are only concerned about the outcome of a single coin flip and will therefore stick to the Bernoulli distribution.</p>
<div class="figure"><span id="fig:BernVSBin"></span>
<img src="Images/Bern_Binomial.png" alt="Bernoulli and Binomial Distributions"  />
<p class="caption">
Figure 2.1: Bernoulli and Binomial Distributions
</p>
</div>
</div>
<div id="bernoulli---distribution-notation" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> Bernoulli - Distribution Notation</h4>
<p>The probability mass function of the bernoulli distribution is shown in Equation <a href="parameter-estimation.html#eq:bernPMF">(2.1)</a>.</p>
<p><span class="math display" id="eq:bernPMF">\[
\begin{equation}
  f_{x}(x)=P(X=x)=\theta^{x}(1-\theta)^{1-x}, \hspace{1cm} x = \{0,1\} 
  \tag{2.1}
\end{equation}
\]</span></p>
<p>The only parameter of the bernoulli distribution is <span class="math inline">\(\theta\)</span>, which defines the probability of success during a bernoulli trial. The value of <em>x</em> is 0 for a failure and 1 for a success. In a practical example you can think of this as 0 for tails and 1 for heads during a coin flip. <a href="parameter-estimation.html#eq:bernPMFExample">(2.2)</a> is an example where the value of <span class="math inline">\(\theta\)</span> is set to 0.7. We can see the probability of getting a success is 0.7, while the probability of failure is 0.3.</p>
<p><span class="math display" id="eq:bernPMFExample">\[
\begin{equation}  
\begin{aligned}
P(X=1)&amp;=\theta^{1}(1-\theta)^{1-1}, \hspace{1cm} \theta=0.7 \\
P(X=1)&amp;=0.7*1=0.7 \\\\
P(X=0)&amp;=0.7^{0}(1-0.7)^{1-0}\\
P(X=0)&amp;=0.3
\end{aligned}
\tag{2.2}
\end{equation}
\]</span></p>
</div>
</div>
<div id="beta-distribution" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Beta Distribution</h3>
<p>The beta distribution can be thought of as a probability distribution of probabilities <span class="citation">(Robinson <a href="references.html#ref-Robinson2014beta">2014</a>)</span>. The Bernoulli distribution is a distribution that gives us a probability of success (coin comes up heads). The beta distribution provides a distribution of these probabilities. To clear this up we will be working through some examples.</p>
<p>If you flip a coin 2 times resulting in 1 heads and 1 tails how sure are you that the coin is fair? Probably not all that sure. But what if you flipped the coin 200 times and it resulted in 100 heads and 100 tails? You would be much more confident that the coin is fair. The beta distribution gives us a way to quantify the probabilities (our confidence in the coin being fair).</p>
<p>The beta distribution has 2 shape parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. These can be thought of as the results from the coin flips we just talked about (i.e. <span class="math inline">\(\alpha=200\)</span>, <span class="math inline">\(\beta=200\)</span>). Below the probability density for different values of <span class="math inline">\(\theta\)</span> is displayed based on different values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. In general, the higher the value of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> the narrower the density curve is. Another way to think of this is that the more information (<em>coin flip results</em>) we have, the more confident we can be in our coin’s bias value (i.e. is it fair, head heavy, etc.).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>)
b &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>)
params &lt;-<span class="st"> </span><span class="kw">cbind</span>(a,b)
ds &lt;-<span class="st"> </span><span class="ot">NULL</span>
n &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(params)){
  ds &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> n, <span class="dt">y =</span> <span class="kw">dbeta</span>(n, params[i,<span class="dv">1</span>], params[i,<span class="dv">2</span>]),
                             <span class="dt">parameters =</span> <span class="kw">paste0</span>(<span class="st">&quot;\U03B1 = &quot;</span>,params[i,<span class="dv">1</span>],
                                                 <span class="st">&quot;, \U03B2 = &quot;</span>, params[i,<span class="dv">2</span>])), ds)
}

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">color=</span>parameters)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Probability Density&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">name=</span><span class="ot">NULL</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:betashape"></span>
<img src="_main_files/figure-html/betashape-1.png" alt="Beta Distribution" width="672" />
<p class="caption">
Figure 2.2: Beta Distribution
</p>
</div>
<p>What about the cases where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are not equal or close to equal? In those cases you would probably assume a bit of skew in the distribution, i.e. your coin may be biased toward head (density skewed toward 1) or tails (density skewed toward 0) as shown below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">2</span>)
b &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">8</span>)
params &lt;-<span class="st"> </span><span class="kw">cbind</span>(a,b)
ds &lt;-<span class="st"> </span><span class="ot">NULL</span>
n &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(params)){
  ds &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> n, <span class="dt">y =</span> <span class="kw">dbeta</span>(n, params[i,<span class="dv">1</span>], params[i,<span class="dv">2</span>]),
                             <span class="dt">parameters =</span> <span class="kw">paste0</span>(<span class="st">&quot;\U03B1 = &quot;</span>,params[i,<span class="dv">1</span>],
                                                 <span class="st">&quot;, \U03B2 = &quot;</span>, params[i,<span class="dv">2</span>])), ds)
}

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">color=</span>parameters)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Probability Density&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">name=</span><span class="ot">NULL</span>, <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#7A99AC&quot;</span>, <span class="st">&quot;#E4002B&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:betaShapeSkewed"></span>
<img src="_main_files/figure-html/betaShapeSkewed-1.png" alt="Beta Distribution - Skewed" width="672" />
<p class="caption">
Figure 2.3: Beta Distribution - Skewed
</p>
</div>
<p>The probability distribution function for the beta distribution can be found in Equation <a href="parameter-estimation.html#eq:betaPDF">(2.3)</a>.</p>
<p><span class="math display" id="eq:betaPDF">\[
\begin{equation}
f(\theta;\alpha,\beta) ={{\theta^{(\alpha-1)}(1-\theta)^{(\beta-1)}}\over B(\alpha,\beta)}
\tag{2.3}
\end{equation}
\]</span></p>
<p>Quick Note:</p>
<ul>
<li>The <em>Beta</em> function, <em>B</em> is the ratio of the product of the <em>Gamma</em> function, <span class="math inline">\(\Gamma\)</span>, of each parameter divided by the <em>Gamma</em> function of the sum of the parameters. The <em>Beta</em> function is <strong>not</strong> the same as the beta distribution. The <em>Beta</em> function is shown below along with the <em>Gamma</em> function, which is used in the <em>Beta</em> function.</li>
</ul>
<p><span class="math display" id="eq:betaFunction">\[
\begin{equation}
\beta(a,b) = {\Gamma(a)\Gamma(b) \over{\Gamma(a+b)}}
\tag{2.4}
\end{equation}
\]</span></p>
<ul>
<li>The <em>Gamma</em> function is the factorial of the parameter minus 1.</li>
</ul>
<p><span class="math display" id="eq:gammaFunction">\[
\begin{equation}
\Gamma(a) = (a-1)!
\tag{2.5}
\end{equation}
\]</span></p>
</div>
</div>
<div id="inference-the-building-blocks" class="section level2">
<h2><span class="header-section-number">2.2</span> Inference: The Building Blocks</h2>
<p>Equation <a href="parameter-estimation.html#eq:posterior">(2.6)</a> is a fundamental to understanding parameter estimation and inference.</p>
<p><span class="math display" id="eq:posterior">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior} = {\overbrace{p(D|\theta)}^{likelihood}
  \overbrace{p(\theta)}^{prior} \over \underbrace{p(D)}_{evidence}}
  \tag{2.6}
\end{equation}
\]</span></p>
<p>The 4 components are:</p>
<ul>
<li><strong>Prior</strong>: The probability of the parameter(s). Defines our prior beliefs of the parameter. Do we <em>believe</em> to a good degree of certainty that the coin is fair? Maybe take a step back and ask yourself ‘do I trust the manufacturer of this coin?’. If this manufacturer has always had great quality (i.e. fair coins) then you would have confidence your coin is fair. If you know nothing about where the coin came from you would be more skeptical of the coin’s bias.<br />
</li>
<li><strong>Posterior</strong>: The probability of the parameter <strong>given</strong> the evidence. Think of it this way, given 100 coin flips with 47 heads and 53 tails what is the probability that theta is 0.5 (coin is fair)? The only way to know this value is if you already obtained the evidence. However we can estimate the posterior in various ways some of which will be covered in this chapter.</li>
<li><strong>Likelihood</strong>: The probability of the evidence <strong>given</strong> the parameter. Given that we know the coin is fair (theta = 0.5) what is the probability of having 47 heads out of 100 flips?</li>
<li><strong>Evidence</strong>: The probability of all possible outcomes. Probability of 1/100 heads, 2/100 heads, …, 100/100 heads.</li>
</ul>
<p><strong>Conditioning your brain for LDA</strong> : We are starting with a coin flip, but the eventual goal is to link this back to words appearing in a document. Try to keep in mind that we think of a word similar to the outcome of a coin: a word exists in the document (heads!) or a word doesn’t exist in the document (tails!).</p>
</div>
<div id="maximum-likelihood" class="section level2">
<h2><span class="header-section-number">2.3</span> Maximum Likelihood</h2>
<p>The simplest method of parameter estimation is the maximum likelihood (ML) method. Effectively we calculate the parameter (<span class="math inline">\(\theta\)</span>) that maximizes the likelihood.</p>
<p><span class="math display" id="eq:posteriorLIKE">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior} = {\overbrace{\bf \Large p(D|\theta)}^{\bf \Large LIKELIHOOD}
  \overbrace{p(\theta)}^{prior} \over \underbrace{p(D)}_{evidence}}
\tag{2.7}
\end{equation}
\]</span></p>
<p>Let’s first discuss what the likelihood is. The likelihood can be described as the probability of getting observed data given a specified value of the parameter, <span class="math inline">\(\theta\)</span>. For example, let’s say I’ve flipped a coin 10 times and got 5 heads, 5 tails. Assuming the coin is fair, <span class="math inline">\(\theta\)</span> equals 0.5, what is the likelihood of observing 5 heads and 5 tails.</p>
<p>To calculate the likelihood of a parameter given an outcome we would use the probability mass function in Equation <a href="parameter-estimation.html#eq:bernPMF2">(2.8)</a>.</p>
<p><span class="math display" id="eq:bernPMF2">\[
\begin{equation}
P(X=x)=\theta^{x}(1-\theta)^{1-x}, \hspace{1cm} x = \{0,1\} 
\tag{2.8}
\end{equation}
\]</span> Where an outcome, <em>x</em>, of heads equals 1 and an outcome of tails is 0. Now let’s say we have carried out 10 coin flips:</p>
<p><span class="math display" id="eq:bernL">\[
\begin{equation}
\begin{aligned}
P(X_{1}=x_{1},X_{2}=x_{2},...,X_{10}=x_{10}) &amp;= \prod\limits_{n=1}^{10} \theta^{x}(1-\theta)^{1-x}\\
L(\theta) &amp;= \prod\limits_{n=1}^{10} \theta^{x}(1-\theta)^{1-x}
\end{aligned}
\tag{2.9}
\end{equation}
\]</span></p>
<p>What is shown in Equation <a href="parameter-estimation.html#eq:bernL">(2.9)</a> is the joint probability mass function. Each coin flip is independent so we calculate the product of the PMF’s for each trial. This is known as the likelihood function - the likelihood of a value of <span class="math inline">\(\theta\)</span> given our observed data.</p>
<p><strong>Back to the maximum likelihood….</strong></p>
<p>Our goal is to find the value of <span class="math inline">\(\theta\)</span> which maximizes the likelihood of the observed data. To derive the maximum likelihood we start by taking the log of the likelihood, <span class="math inline">\(\mathcal{L}\)</span>.</p>
<p><span class="math display" id="eq:bernLogL">\[
\begin{equation}
\begin{aligned}
\mathcal{L} &amp;= log \prod\limits_{n=1}^N \theta^{x}(1-\theta)^{1-x} \\\\
 &amp;= \sum\limits_{n=1}^N log(\theta^{x}(1-\theta)^{1-x}) \\\\
 &amp;= n^{(1)}log(\theta) + n^{(0)}log(1-\theta)
\end{aligned}
\tag{2.10}
\end{equation}
\]</span></p>
<p>Then differentiate with respect to <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display" id="eq:bernDiffL">\[
\begin{equation}
{d\mathcal{L} \over d\theta} =  {n^{(1)}\over \theta} - {n^{(0)}\over 1-\theta}
\tag{2.11}
\end{equation}
\]</span></p>
<p>Then set the left side of the equation equal to zero and solve:</p>
<p><span class="math display" id="eq:bernLTheta">\[
\begin{equation}
\begin{aligned}
0 &amp;= {n^{(1)}\over \theta} - {n^{(0)}\over 1-\theta}   \\ \\
{n^{(1)}\over\theta} &amp;= {n^{(0)}\over 1-\theta} \\ \\
{n^{1} - \theta n^{1}} &amp;= {\theta n^{0}} \\ \\
n^{(1)} &amp;= \theta(n^{(1)} + n^{0}) \\ \\
\theta &amp;= {n^{(1)} \over N}
\end{aligned}
\tag{2.12}
\end{equation}
\]</span></p>
<p>The value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood is the number of heads over the number of flips. The maximum likelihood value for 1 out of 10 flips equal to head up to 10 heads out of 10 flips is shown in Figure <a href="parameter-estimation.html#fig:bernoulliml">2.4</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">heads =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span>
flips =<span class="st"> </span><span class="dv">10</span>
ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(heads, <span class="dt">flips =</span> <span class="kw">rep</span>(flips, <span class="kw">length</span>(heads)))
ds<span class="op">$</span>theta &lt;-<span class="st"> </span>ds<span class="op">$</span>heads<span class="op">/</span>ds<span class="op">$</span>flips


<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x =</span> heads,
               <span class="dt">y =</span> theta)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color =</span><span class="st">&#39;#1520c1&#39;</span>, <span class="dt">size =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="kw">aes</span>(<span class="dt">x=</span>heads,
                     <span class="dt">y=</span><span class="ot">NULL</span>, <span class="dt">ymax=</span>theta,
                     <span class="dt">ymin=</span><span class="dv">0</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">2</span>), <span class="dt">labels =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">2</span>)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">x=</span><span class="st">&quot;Number of Heads&quot;</span>, <span class="dt">title =</span><span class="st">&quot;ML Parameter Estimation: 10 Bernoulli Trials&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:bernoulliml"></span>
<img src="_main_files/figure-html/bernoulliml-1.png" alt="Bernoulli Maximum Likelihood" width="672" />
<p class="caption">
Figure 2.4: Bernoulli Maximum Likelihood
</p>
</div>
</div>
<div id="maximum-a-posteriori" class="section level2">
<h2><span class="header-section-number">2.4</span> Maximum a Posteriori</h2>
<p>Maximum a Posteriorir (MAP) is similar to the method of maximum likelihood estimation, but it also includes information about our prior beliefs. Unlike ML estimation, MAP estimates parameters by maximizing the posterior.</p>
<p><span class="math display" id="eq:bernMAP">\[
\begin{equation}
\theta_{MAP}=\underset{\theta}{\operatorname{argmax}} P(\theta|X)
\tag{2.13}
\end{equation}
\]</span></p>
<p><span class="math display" id="eq:bernPMFDropEvidence">\[
\begin{equation}
\require{enclose}
\theta_{MAP}=\underset{\theta}{\operatorname{argmax}}{\overbrace{p(D|\theta)}^{likelihood}
  \overbrace{p(\theta)}^{prior} \over \underbrace{\enclose{horizontalstrike}{p(D)}}_{evidence}}
\tag{2.14}
\end{equation}
\]</span></p>
<p>The formula for the MAP estimation of <span class="math inline">\(\theta\)</span> is shown in <a href="parameter-estimation.html#eq:bernPMFDropEvidence">(2.14)</a>. The evidence term is dropped during the calculation of <span class="math inline">\(\theta_{MAP}\)</span> since it is not a function of <span class="math inline">\(\theta\)</span> and our only concern is maximizing the posterior based on <span class="math inline">\(\theta\)</span>. Similar to calculating the ML estimate, the first step is to apply the log function to the remaining terms (Equation <a href="parameter-estimation.html#eq:bernMAPLog">(2.15)</a>).</p>
<p><span class="math display" id="eq:bernMAPLog">\[
\begin{equation}
\begin{aligned}
\theta_{MAP} &amp;=\underset{\theta}{\operatorname{argmax}}p(D|\theta)p(\theta) \\\\
&amp;= \mathcal{L}(\theta|D) + log(p(\theta))
\end{aligned}
\tag{2.15}
\end{equation}
\]</span></p>
<p>We have already derived the log likelihood during our derivation of the maximum likelihood, so let’s now focus on the prior. The prior for the Bernoulli distribution is the Beta distribution and can be used to describe <span class="math inline">\(p(\theta)\)</span>. The probability distribution function, PDF, for the Beta distribution is shown in Equation <a href="parameter-estimation.html#eq:bernPrior">(2.16)</a>.</p>
<p><span class="math display" id="eq:bernPrior">\[
\begin{equation}
p(\theta|\alpha,\beta) = {\theta^{\alpha-1}(1-\theta)^{\beta-1}\over{B(\alpha, \beta)}}
\tag{2.16}
\end{equation}
\]</span></p>
<p>Next, we plug in the PDF of the beta distribution in the place of the prior (Equation <a href="parameter-estimation.html#eq:bernMAPTheta">(2.17)</a>).</p>
<p><span class="math display" id="eq:bernMAPTheta">\[
\begin{equation}
\begin{aligned}
\theta_{MAP}&amp;= \mathcal{L}(\theta|D) + log(p(\theta)) \\\\
\theta_{MAP}&amp;= n^{(1)}log(\theta) + n^{(0)}log(1-\theta) + log({\theta^{\alpha-1}(1-\theta)^{\beta-1}\over{B(\alpha, \beta)}}) \\\\
\theta_{MAP}&amp;= n^{(1)}log(\theta) + n^{(0)}log(1-\theta)  + \\ &amp;\quad log({\theta^{\alpha-1}) + log((1-\theta)^{\beta-1}) - log({B(\alpha, \beta)}}) \\\\
{d \over d\theta} \mathcal{L}(\theta|D) + log(p(\theta)) &amp;= {n^{(1)}\over \theta} - {n^{(0)}\over 1-\theta} + {\alpha - 1\over\theta} - {\beta - 1 \over 1-\theta}
\\\\
0 &amp;= {n^{(1)}\over \theta} - {n^{(0)}\over 1-\theta} + {\alpha - 1\over\theta} - {\beta - 1 \over 1-\theta}
\\\\
\theta_{MAP}&amp;= {{n^{(1)} + \alpha -1} \over {n^{(1)} + n^{0} + \alpha + \beta - 2}}
\end{aligned}
\tag{2.17}
\end{equation}
\]</span></p>
<p>Now that we know how to calculate the parameter <span class="math inline">\(\theta\)</span> that maximizes the posterior, lets take a look at how choices of different priors and different numbers of observed trials effects our outcome.</p>
<p>In the Figure <a href="parameter-estimation.html#fig:mapSmallnUninformedPrior">2.5</a> we see the MAP estimation of <span class="math inline">\(\theta\)</span> with a relatively uninformed prior, low <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> values and a small number of observed experiments (n=20). Uninformed means we are going to make a weak assumption about the prior. In more general terms, this means that we don’t have a strong intuition that our coin is fair or unfair. In a mathematical sense, uninformed means a low sum of the two parameters, if you had <span class="math inline">\(\alpha=1000\)</span> and <span class="math inline">\(\beta=1\)</span> this would <strong>not</strong> be considered as uninformed whereas <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> both equal to 1 is normally referred to as uniformed or naive. The Beta distribution has <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> parameters of 2 resulting in the blue density curve shown in Figure <a href="parameter-estimation.html#fig:mapSmallnUninformedPrior">2.5</a>.</p>
<p><strong>NOTE</strong>: The terms weak, uninformed, and naive are often used interchangeably when referencing priors. The same goes for the terms strong and informed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">20</span>
heads &lt;-<span class="st"> </span><span class="dv">14</span>
tails &lt;-<span class="st"> </span><span class="dv">6</span>


B &lt;-<span class="st"> </span><span class="dv">2</span>
alpha &lt;-<span class="st"> </span><span class="dv">2</span>

map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(heads <span class="op">+</span><span class="st"> </span>tails <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>B <span class="op">-</span><span class="dv">2</span>)
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">y=</span><span class="fl">0.5</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[MAP]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:mapSmallnUninformedPrior"></span>
<img src="_main_files/figure-html/mapSmallnUninformedPrior-1.png" alt="MAP: Small number of experiments and uninformed prior" width="672" />
<p class="caption">
Figure 2.5: MAP: Small number of experiments and uninformed prior
</p>
</div>
<p>In the next example we have the same number of samples, but we assume a much stronger prior. In particular, we assume the coin is most likely fair by selecting <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> parameters both equal to 100. These parameters represent a Beta distribution that is centered at 0.5 and is very dense around that point. We can see the resuting MAP estimate is much closer to 0.5 than our previous example with the uniformed prior.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">20</span>
heads &lt;-<span class="st"> </span><span class="dv">14</span>
tails &lt;-<span class="st"> </span><span class="dv">6</span>

B &lt;-<span class="st"> </span><span class="dv">100</span>
alpha &lt;-<span class="st"> </span><span class="dv">100</span>

map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(heads <span class="op">+</span><span class="st"> </span>tails <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>B <span class="op">-</span><span class="dv">2</span>)
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">y=</span><span class="dv">11</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[MAP]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:mapSmallnInformedPrior"></span>
<img src="_main_files/figure-html/mapSmallnInformedPrior-1.png" alt="MAP: Small number of experiments and informed prior" width="672" />
<p class="caption">
Figure 2.6: MAP: Small number of experiments and informed prior
</p>
</div>
<p>What happens when we have a much larger number of observed experiments and an uninformed prior? In this case the MAP estimate will give us a value that is very close to a maximum likelihood estimate, i.e. heads divide by number of trials.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
heads &lt;-<span class="st"> </span><span class="dv">723</span>
tails &lt;-<span class="st"> </span>n<span class="op">-</span>heads


B &lt;-<span class="st"> </span><span class="dv">2</span>
alpha &lt;-<span class="st"> </span><span class="dv">2</span>
map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(heads <span class="op">+</span><span class="st"> </span>tails <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>B <span class="op">-</span><span class="dv">2</span>)
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">y=</span><span class="fl">1.2</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[MAP]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:mapLargenUninformedPrior"></span>
<img src="_main_files/figure-html/mapLargenUninformedPrior-1.png" alt="MAP: Large number of experiments and uninformed prior" width="672" />
<p class="caption">
Figure 2.7: MAP: Large number of experiments and uninformed prior
</p>
</div>
<p>Now let’s assume a much stronger prior, i.e. more confidence a priori the coin is fair, while using the same number of experiments. Notice that when we use a larger number of experiments it overpowers the strong prior and gives us a very similar MAP estimate in comparison to the example with the uninformed prior and the same number of experiments (Uninformed - Figure <a href="parameter-estimation.html#fig:mapLargenUninformedPrior">2.7</a> and Informed - Figure <a href="parameter-estimation.html#fig:mapLargeInformedPrior">2.8</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
heads &lt;-<span class="st"> </span><span class="dv">723</span>
tails &lt;-<span class="st"> </span>n<span class="op">-</span>heads

B &lt;-<span class="st"> </span><span class="dv">100</span>
alpha &lt;-<span class="st"> </span><span class="dv">100</span>
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(heads <span class="op">+</span><span class="st"> </span>tails <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>B <span class="op">-</span><span class="dv">2</span>)
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">y=</span><span class="fl">8.2</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[MAP]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:mapLargeInformedPrior"></span>
<img src="_main_files/figure-html/mapLargeInformedPrior-1.png" alt="MAP: Large number of experiments and informed prior" width="672" />
<p class="caption">
Figure 2.8: MAP: Large number of experiments and informed prior
</p>
</div>
<p>In summary:</p>
<ul>
<li>The stronger your prior assumptions are, the more observations you will need to overcome an incorrect prior estimation/belief.</li>
<li>Stronger prior - MAP estimate moves toward most dense area of prior distribution</li>
<li>Weaker prior - MAP looks more like a maximum likelihood</li>
</ul>
</div>
<div id="bayesian-inference" class="section level2">
<h2><span class="header-section-number">2.5</span> Bayesian Inference</h2>
<div id="analytical-solution" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Analytical Solution</h3>
<p>Another option we have for estimating parameters is to estimate the posterior of the distribution via Bayesian inference. In our MAP estimation example we included prior assumptions as part of our calculation. We are going to do the same via Bayesian inference however instead of a point estimate of <span class="math inline">\(\theta\)</span>, we will be calculating the posterior distribution over all possible values of <span class="math inline">\(\theta\)</span>. From this we can take the expected value of <span class="math inline">\(\theta\)</span> as our estimated parameter.</p>
<p>In the case of the coin flip, Bayesian inference can be solved analytically. We have reviewed the likelihood and the prior, but when estimating the posterior we need to include the evidence term. This is somewhat tricky.</p>
<p><span class="math display" id="eq:PosteriorBI">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior} = { p(D|\theta)
  p(\theta) \over \underbrace{p(D)}_{evidence}}
\tag{2.18}
\end{equation}
\]</span></p>
<p><span class="math display" id="eq:evidence">\[
\begin{equation}
p(D) = \int_{\theta}p(D|\theta)p(\theta)d\theta
\tag{2.19}
\end{equation}
\]</span></p>
<p><span class="math display" id="eq:bernBI">\[
\begin{equation}
{p(\theta|z,N)} = {\overbrace{\theta^z(1-\theta)^{(N-z)}}^{likelihood} \overbrace{{\theta^{(a-1)}(1-\theta)^{(b-1)}}\over \beta(a,b)}^{prior}\over{\underbrace{\int_{0}^1 \theta^z(1-\theta)^{(N-z)}{{\theta^{(a-1)}(1-\theta)^{(b-1)}}\over \beta(a,b)}d\theta}_{Evidence}}}
\tag{2.20}
\end{equation}
\]</span></p>
<p>In Equation <a href="parameter-estimation.html#eq:evidence">(2.19)</a> we see the evidence term is an integral over all values of <span class="math inline">\(\theta\)</span>, 0 to 1. This means the value is a constant. Recall that the evidence term acts as a scaling factor to ensure our probabilities sum to 1. We will plug in a generic placeholder of our evidence value in <a href="parameter-estimation.html#eq:bernBIFinal">(2.21)</a> since we have established that it is a constant.</p>
<p><span class="math display" id="eq:bernBIFinal">\[
\begin{equation}
\begin{aligned}
p(\theta|z, N) &amp;= {\theta^{z}(1-\theta)^{(N-z)}{{\theta^{(a-1)}(1-\theta)^{(b-1)}}\over \beta(a,b)}\over{C}} \\\\
 &amp;= {\theta^{(z+a-1)}(1-\theta)^{(N-z+b-1)} \over \beta(z+a, N-z+b)}\\\\
 &amp;= Beta(z+a, N-z+b)
\end{aligned}
\tag{2.21}
\end{equation}
\]</span> In the second step of Equation @(eq:bernBIFinal) the beta function (the denomitor of the prior) takes the place of the constant (i.e. the evidence value). The beta function is a constant value and we need it to do the same job as the evidence term; it needs to ensure the probabilities sum to 1. To ensure this we alter the input parameters to the beta function, so that we are left with a beta distribution for our posterior that includes our observed experiments and our prior’s hyperparameters.</p>
<p>To estimate <span class="math inline">\(\theta\)</span>, we calculate the expected value of a Beta distribution as shown in Equation @(eq:bernBIExp).</p>
<p><span class="math display" id="eq:bernBIExp">\[
\begin{equation}
E[\theta]={\alpha \over \alpha + \beta}
\tag{2.22}
\end{equation}
\]</span></p>
<p>Plugging in our parameters, as per the derivation in Equation @(eq:bernBIFinal), we get the resulting expected value of <span class="math inline">\(\theta\)</span> shown in @(eq:bernBIExpFinal)</p>
<p><span class="math display" id="eq:bernBIExpFinal">\[
\begin{equation}
\begin{aligned}
E[\theta] &amp;= {z+a \over {z+a + n-z+b}}\\\\
&amp;={z+a \over {a+N+b}}\\\\
\end{aligned}
\tag{2.23}
\end{equation}
\]</span></p>
<p>Figure <a href="parameter-estimation.html#fig:BISmallUninformedPrior">2.9</a> shows an example of the analytical Bayesian inference solution for a small number of flips, 20, resulting in 14 heads while assuming a weak prior (<span class="math inline">\(\alpha\)</span>=<span class="math inline">\(\beta\)</span> = 2). It is worth noting that Bayesian inference is effected by the selection of a prior and the number of observations in the same way MAP estimation is.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">20</span>
heads &lt;-<span class="st"> </span><span class="dv">14</span>
tails &lt;-<span class="st"> </span>n<span class="op">-</span>heads

B &lt;-<span class="st"> </span><span class="dv">2</span>
alpha &lt;-<span class="st"> </span><span class="dv">2</span>
possible_theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)
beta_ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta =</span> possible_theta, <span class="dt">density =</span> <span class="kw">dbeta</span>(possible_theta, alpha,B))
map_theta &lt;-<span class="st"> </span>(heads <span class="op">+</span><span class="st"> </span>alpha)<span class="op">/</span>(alpha <span class="op">+</span><span class="st"> </span>n <span class="op">+</span><span class="st"> </span>B) 
<span class="kw">ggplot</span>(beta_ds, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> density)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span>map_theta, <span class="dt">color =</span> <span class="st">&#39;#ba0223&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> map_theta <span class="op">+</span><span class="st"> </span><span class="fl">0.08</span>, <span class="dt">y=</span><span class="fl">1.4</span>, <span class="dt">label=</span> <span class="kw">paste</span>(<span class="st">&quot;\U03B8[BI]==&quot;</span>, <span class="kw">round</span>(map_theta,<span class="dv">2</span>)), <span class="dt">parse=</span>T)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:BISmallUninformedPrior"></span>
<img src="_main_files/figure-html/BISmallUninformedPrior-1.png" alt="Bayesian Inference: Analytical Solution (n=20)" width="672" />
<p class="caption">
Figure 2.9: Bayesian Inference: Analytical Solution (n=20)
</p>
</div>
</div>
</div>
<div id="gibbs-sampling" class="section level2">
<h2><span class="header-section-number">2.6</span> Gibbs Sampling</h2>
<p>The remainder of this chapter will tackle the concept of Gibbs sampling for estimating a posterior distribution. Before getting into specifics let’s touch on why a person may need to use a method such as Gibbs sampling.</p>
<div id="the-issue-of-intractability" class="section level3">
<h3><span class="header-section-number">2.6.1</span> The Issue of Intractability</h3>
<p>The coin flip example solved via Bayesian inference was capable of being solved analytically. However in many cases of Bayesian inference this is not possible due to the intractability of solving for the marginal likelihood (evidence term). We do have other options for solutions such as Gibbs sampling, expectation-maximization, and Metropolis-Hastings methods. This book will only focus on Gibbs Sampling, but be aware other types of solvers are used for Bayesian inference problems including LDA.</p>
</div>
<div id="a-tale-of-two-mcs" class="section level3">
<h3><span class="header-section-number">2.6.2</span> A Tale of Two MC’s</h3>
<p>Gibbs Sampling is a Markov Chain Monte Carlo (MCMC) technique for parameter estimation. Let’s break down the two MC’s. A markov chain is a process where the next state is determined by the current state. More importantly it does not rely on any information prior to the current state.</p>
<div class="figure" style="text-align: center"><span id="fig:MarkovChain"></span>
<img src="Images/MarkovChain.png" alt="Markov Chain: Rain or Shine" width="50%" height="50%" />
<p class="caption">
Figure 2.10: Markov Chain: Rain or Shine
</p>
</div>
<p>The other MC, Monte Carlo, is a technique used to solve a variety of problems by repeated random sampling. A common example used to showcase Monte Carlo methods is the estimation of the value of <span class="math inline">\(\pi\)</span>. <span class="citation">(Resnik and Hardisty <a href="references.html#ref-resnik2010gibbs">2010</a>)</span> All we need is some chalk and a bucket of rice. First I draw a circle on the ground. Then I then draw a square along the circumference of the square. Now for the bucket of rice, aka our random number generator. I stand over the square and uniformly pour rice over the area of the square. Now comes the monotonous part, counting the rice. First I count the number of pieces of rice inside the circle. Next I tally the number of pieces of rice remaining. (Side note: you could save yourself a bunch of time and just weight the two proportions of rice instead of straining your vision and wasting your time.)</p>
<div class="figure" style="text-align: center"><span id="fig:MonteCarlo"></span>
<img src="Images/MonteCarlo.png" alt="Monte Carlo: Estimating &amp;#960;" width="75%" height="75%" />
<p class="caption">
Figure 2.11: Monte Carlo: Estimating π
</p>
</div>
<p>From here I can infer the value of <span class="math inline">\(\pi\)</span> as follows:</p>
<p><span class="math display" id="eq:MCRice">\[
\begin{equation}
\begin{aligned}
{\pi r^{2} \over (2r)^{2}} &amp;= {Rice_{circle}\over Rice_{circle + square}} \\\\
\pi &amp;\approx {4Rice_{circle}\over Rice_{circle + square}}\\
\hat{\pi} &amp;= {4*1kg \over 1.2kg} = 3.333
\end{aligned}
\tag{2.24}
\end{equation}
\]</span></p>
</div>
<div id="conjugate-distributions-and-priors" class="section level3">
<h3><span class="header-section-number">2.6.3</span> Conjugate Distributions and Priors</h3>
<p>Before wading into the deeper water that is Gibbs sampling I need to touch on the concept of conjugate distributions and priors. A <strong>conjugate distribution</strong> pair is a posterior distribution that have the same form as the prior distribution. Bernoulli and beta distributions are conjugate distributions. The posterior of the bernoulli distribution has the same general form as the prior, the beta distribution. Therefore we call the beta distribution a <strong>conjugate prior</strong> of the bernoulli distribution. We will cover this specific conjugate relationship in detail in the following section, but you can look ahead at Equations <a href="parameter-estimation.html#eq:bernGibbs">(2.27)</a> <a href="parameter-estimation.html#eq:bernGibbsFinal">(2.28)</a> to see the similar form of the posterior compared to the prior.</p>
<p>Ok, but what does that mean for us? We can use the conjugate relationship to:</p>
<ol style="list-style-type: decimal">
<li>Simplify our posterior estimation to the same from as the prior.</li>
<li>Step 1 allows us to then sample from a known distribution to estimate our posterior distribution.</li>
</ol>
<p>Step 2 is a crucial component in Gibbs sampling - it is the Monte Carlo piece of the method. To help reinforce the concept of conjugate priors we will go through an example.</p>
<div id="bernoulli-beta" class="section level4">
<h4><span class="header-section-number">2.6.3.1</span> Bernoulli &amp; Beta</h4>
<p>We return to our coin flip example. We want to estimate the posterior, the probability of <span class="math inline">\(\theta\)</span> given the the results of all experiments (this includes experiments that we haven’t yet completed). Obviously we don’t have data which does not yet exist (i.e. all experiments) so we will need to use the likelihood and the prior to estimate the posterior.</p>
<p>In the previous example we used the Beta distribution for our prior. As we now know, the Beta distribution is the conjugate prior of the Bernoulli distribution. We will take a closer look at this relationship now.</p>
<p>We start with our base relationship between posterior, likelihood, prior, and posterior as shown in Equation <a href="parameter-estimation.html#eq:posteriorGibbs">(2.25)</a>.</p>
<p><span class="math display" id="eq:posteriorGibbs">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior} = {\overbrace{p(D|\theta)}^{likelihood}
  \overbrace{p(\theta)}^{prior} \over \underbrace{p(D)}_{evidence}}
\tag{2.25}
\end{equation}
\]</span></p>
<p>When estimating the posterior we drop out the evidence term. To reiterate, we can drop out the evidence term because it is constant and is used as a normalizing factor for scaling our probabilities to so that they sum to 1. We will see in the upcoming section that Gibbs sampling accounts for the scaling issue and allows us to infer probabilities at the correct scale.</p>
<p>MAYBE ADD SOMETHING HERE ALONG THESE LINES OTHERWISE JUST SAVE IT FOR GIBBS SECTION….(Gibbs sampling accomplishes this normalization by taking a large number of samples and then dividing by the samples by the sum of all samples resulting in a proportion between 0 and 1. )</p>
<p><span class="math display" id="eq:posteriorPrompto">\[
\begin{equation}
\underbrace{p(\theta|D)}_{posterior}  \propto  {\overbrace{p(D|\theta)}^{likelihood}
  \overbrace{p(\theta)}^{prior}}
\tag{2.26}
\end{equation}
\]</span></p>
<p>To estimate the posterior of the bernoulli distribution we plug in our likelihood and prior (<em>Beta distribution</em>) equations in Equation <a href="parameter-estimation.html#eq:bernGibbs">(2.27)</a>.</p>
<p><span class="math display" id="eq:bernGibbs">\[
\begin{equation}
p(\theta|z,N) \propto \overbrace{\theta^z(1-\theta)^{(N-z)}}^{likelihood} \overbrace{{\theta^{(a-1)}(1-\theta)^{(b-1)}}\over \beta(a,b)}^{prior}
\tag{2.27}
\end{equation}
\]</span></p>
<p>After combining terms we end up with Equation <a href="parameter-estimation.html#eq:bernGibbsFinal">(2.28)</a> which looks very similar to @(bernBIFinal). This is a very simple example where an analytical solution for the posterior is possible, so the equations are effectively the same with the exeption of the <span class="math inline">\(\propto\)</span> in place of the <span class="math inline">\(=\)</span>. The <span class="math inline">\(\propto\)</span> is used because we are going to base our posterior off of random samples from the Beta distribution with the parameters shown in <a href="parameter-estimation.html#eq:bernGibbsFinal">(2.28)</a>.</p>
<p><span class="math display" id="eq:bernGibbsFinal">\[
\begin{equation}
\begin{aligned}
p(\theta|z,N) &amp;\propto {\overbrace{\theta^{(a + z -1)}(1-\theta)^{(N-z+b-1)}}^{Same \hspace{1 mm} Pattern \hspace{1 mm} as \hspace{1 mm} Prior}\over{\beta(a,b)}}\\
p(\theta|z,N) &amp;\propto Beta(a+z, N-z+b)
\end{aligned}
\tag{2.28}
\end{equation}
\]</span></p>
</div>
</div>
<div id="gibbs-sampling-1" class="section level3">
<h3><span class="header-section-number">2.6.4</span> Gibbs Sampling</h3>
<p>Gibbs sampling is a Markov Chain Monte Carlo technique that can be used for estimating parameters by walking through a given parameter space. A generalized way to think of Gibbs sampling is estimating a given parameter based on what we currently know about observed data and the other parameters of the model. You may be thinking ‘What other parameters?’. Gibbs sampling is only really applicable when you are trying to estimate multiple parameters otherwise you would use ML, MAP, or analytical Bayesian inference.</p>
<p>Since Gibbs sampling is suited for the estimation of multiple parameters our coin flip example will be expanded slightly. We will now attempt to calculate the bias of two coins and determine if there is a difference in bias between the two using Gibbs sampling.</p>
<p>For the purposes of Gibbs Sampling we need to do a bit of math to determine the equations for our posterior conditional equations. The posterior conditional is the posterior of a single parameter, given the current values of all other parameters in the model. This is where the Markov Chain component of Gibbs sampling comes into play. The conditional posterior of a parameter, i.e. the next state, is determined based on all the current parameter values, i.e. the current state.</p>
<p>Now is a good time to highlight the general structure for using your conjugate priors and how to get to the equations required for the sampling process.</p>
<p>The general process for derivation of our sampling distribution, as outlined in <span class="citation">(Yildirim <a href="references.html#ref-yildirim2012bayesian">2012</a>)</span>, is as follows:</p>
<ol style="list-style-type: decimal">
<li>Derive the full joint density.</li>
<li>Derive the posterior conditionals for each of the random variables in the model.</li>
<li>Simulate samples from the posterior joint distribution based on the posterior conditionals.</li>
</ol>
<p>So how do we get the posterior conditionals for our two coin problem? First let’s go over what we need to know:</p>
<ul>
<li><span class="math inline">\(\theta_{1}\)</span> : Bias of coin 1</li>
<li><span class="math inline">\(\theta_{2}\)</span> : Bias of coin 2</li>
</ul>
<p>The full joint density that defines our problem is shown in Equation <a href="parameter-estimation.html#eq:bernFullJoint">(2.29)</a>.</p>
<p><span class="math display" id="eq:bernFullJoint">\[
\begin{equation}
\begin{aligned}
p(\theta_{1}, \theta_{2}|z_{1}, z_{2}, N) &amp;\propto p(\theta_{1}|z_{1}, N)p(\theta_{2}|z_{2}, N) \\
p(\theta_{1}, \theta_{2}|z_{1}, z_{2}, N) &amp;\propto p(z_{1}|\theta_{1})p(z_{2}|\theta_{2})p(\theta_{1})p(\theta_{2})
\end{aligned}
\tag{2.29}
\end{equation}
\]</span></p>
<p>Then we break down the full joint density into our posterior conditionals: <span class="math display">\[
\begin{equation}
\begin{aligned}
p(\theta_{1}|z_{1}, N) &amp;\propto p(z_{1}|\theta_{1})p(\theta_{1}) \\
p(\theta_{2}|z_{2}, N) &amp;\propto p(z_{2}|\theta_{2})p(\theta_{2}) 
(\#eq:bernFullJointStep2)
\end{aligned}
\end{equation}
\]</span></p>
<p>We plug in posterior conditional for a single coin, which we derived in Equation <a href="parameter-estimation.html#eq:bernGibbsFinal">(2.28)</a>. The resulting posterior conditionals are shown below in Equation <a href="parameter-estimation.html#eq:bernPosteriorConditionals">(2.30)</a>.</p>
<p><span class="math display" id="eq:bernPosteriorConditionals">\[
\begin{equation}
\begin{aligned}
p(\theta_{1}|z_{1},N) &amp;\propto Beta(a_{1}+z_{1}, N-z_{1}+b_{1}) \\
p(\theta_{2}|z_{2},N) &amp;\propto Beta(a_{2}+z_{2}, N-z_{2}+b_{2})
\end{aligned}
\tag{2.30}
\end{equation}
\]</span></p>
<p>Now that we have our posterior conditionals we can move on to estimating our parameters via Gibbs sampling. The general form of Gibbs Sampling is shown in Equation <a href="parameter-estimation.html#eq:GibbsGeneral">(2.31)</a>.</p>
<p><span class="math display" id="eq:GibbsGeneral">\[
\begin{equation}
\begin{aligned}
For \ i \ in \ iterations:\\
&amp;p(\theta_{1}^{i+1}) \sim p(\theta_{1}^{i}|\theta_{2}^{i}, \theta_{3}^{i},..., \theta{n}^{i}) \\
&amp;p(\theta_{2}^{i+1}) \sim p(\theta_{2}^{i}|\theta_{1}^{i+1}, \theta_{3}^{i},..., \theta{n}^{i}) \\
&amp;p(\theta_{3}^{i+1}) \sim p(\theta_{3}^{i}|\theta_{1}^{i+1}, \theta_{2}^{i+1},..., \theta{n}^{i}) \\
&amp;................................ \\ 
&amp;p(\theta_{n}^{i+1}) \sim p(\theta_{n}^{i}|\theta_{1}^{i+1}, \theta_{2}^{i+1},..., \theta_{n-1}^{i+1}) \\
\end{aligned}
\tag{2.31}
\end{equation}
\]</span></p>
<p>Gibbs sampling works by estimating all parameters via the posterior conditional iteratively for a set number of iterations or a distinct stopping criteria/convergence measure. For the sake of our example we will stick with a set number of iterations. In Equation <a href="parameter-estimation.html#eq:GibbsGeneral">(2.31)</a> we see the next estimate, <em>i+1</em>, of <span class="math inline">\(\theta_{1}\)</span> is based on all other current parameter values. When estimating <span class="math inline">\(\theta_{2}\)</span> the <em>i+1</em> value of <span class="math inline">\(\theta_{1}\)</span> is used along with all of the current (<em>i</em>) parameter values. This continues for all parameter values. After the the <em>nth</em> parameter is estimated, the process starts all over again for the next iteration.</p>
<p>The process without the math is shown in Figure <a href="parameter-estimation.html#fig:GibbsViz">2.12</a>. The red circles represent the parameters yet to be estimated in this iteration where the blue represent those that have been previously estimated during the current iteration. Note that the purple circle in each row is the parameter currently being estimated in that step (the current row) and that it takes into account all the available info, i.e. all the red and blue circles in that row.</p>
<div class="figure" style="text-align: center"><span id="fig:GibbsViz"></span>
<img src="Images/GibbsSamplingViz.png" alt="Gibbs Sampling - The Process" width="75%" height="75%" />
<p class="caption">
Figure 2.12: Gibbs Sampling - The Process
</p>
</div>
<p>The calculations used for our example are shown in Equation <a href="parameter-estimation.html#eq:TwoCoinsGibbsGeneral">(2.32)</a>. Note that since the two coins are independent of one another, we are sampling from a Beta distribution based on each coin’s priors and evidence then repeating.</p>
<p><span class="math display" id="eq:TwoCoinsGibbsGeneral">\[
\begin{equation}
\begin{aligned}
For \ i \ in \ iterations:\\
p(\theta_{1}^{i+1}|z_{1},N) &amp;\propto Beta(a_{1}+z_{1}, N-z_{1}+b_{1}) \\
p(\theta_{2}^{i+1}|z_{2},N) &amp;\propto Beta(a_{2}+z_{2}, N-z_{2}+b_{2})
\end{aligned}
\tag{2.32}
\end{equation}
\]</span></p>
<p><strong>Burn In Period</strong></p>
<p>The purpose of Gibbs Sampling is to sample from the posterior and estimate a parameter value assuming our sampling converges on the true parameter. However it often takes time, i.e. many samples, to move into an area of convergence. To be clear, this is a non-issue for the current example due to the independence between the coins. In the change point example that follows, the parameters are not independent of one another and therefore a burn-in period will be necessary.</p>
</div>
<div id="bias-of-two-coins" class="section level3">
<h3><span class="header-section-number">2.6.5</span> Bias of Two Coins</h3>
<p>The following example will generate samples from the posterior distributions of two different coins via Gibbs sampling for the purposes of estimating each coin’s bias. We use a fairly weak prior by setting <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to a value of 2 for each coins. We have the results of 20 coin flips for each coin. Coin 1 has 10 heads out of 20 flips while coin 2 has 3 heads out of 20 flips.</p>
<p>The Gibbs sampling code below is based on the example provided on Duke’s Computational Statistics and Statistical Computing course website. <span class="citation">(Chan and McCarthy <a href="references.html#ref-DukeMCMC">2017</a>)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a =<span class="st"> </span><span class="dv">2</span>
b =<span class="st"> </span><span class="dv">2</span>

z1 =<span class="st"> </span><span class="dv">10</span>
N1 =<span class="st"> </span><span class="dv">20</span>
z2 =<span class="st"> </span><span class="dv">3</span>
N2 =<span class="st"> </span><span class="dv">20</span>


theta =<span class="st"> </span><span class="kw">rep</span>(<span class="fl">0.5</span>,<span class="dv">2</span>)
niters =<span class="st"> </span><span class="dv">10000</span>
burnin =<span class="st"> </span><span class="dv">500</span>

thetas =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> (niters<span class="op">-</span>burnin), <span class="dt">ncol=</span><span class="dv">2</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>niters){

  theta1 =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>z1, <span class="dt">shape2 =</span> b <span class="op">+</span><span class="st"> </span>N1 <span class="op">-</span><span class="st"> </span>z1)
  <span class="co"># get value theta2| all other vars</span>
  theta2 =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>z2, <span class="dt">shape2 =</span>b <span class="op">+</span><span class="st"> </span>N2 <span class="op">-</span><span class="st"> </span>z2)
  
  <span class="cf">if</span> (i <span class="op">&gt;=</span><span class="st"> </span>burnin){
    thetas[(i<span class="op">-</span>burnin), ] =<span class="st"> </span><span class="kw">c</span>(theta1, theta2)
  }
}


ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">theta1 =</span> thetas[,<span class="dv">1</span>], <span class="dt">theta2=</span> thetas[,<span class="dv">2</span>])
<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x=</span>theta1)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y=</span>..density..),<span class="dt">color=</span><span class="st">&#39;#1A384A&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(theta[<span class="dv">1</span>]<span class="op">~</span>Estimate), <span class="dt">x=</span><span class="kw">expression</span>(theta[<span class="dv">1</span>]), <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ds<span class="op">$</span>theta1), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:2coins"></span>
<img src="_main_files/figure-html/2coins-1.png" alt="Bias of Two Coins: Theta 1" width="672" />
<p class="caption">
Figure 2.13: Bias of Two Coins: Theta 1
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x=</span>theta2)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y=</span>..density..),<span class="dt">color=</span><span class="st">&#39;#1A384A&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(theta[<span class="dv">2</span>]<span class="op">~</span>Estimate), <span class="dt">x=</span><span class="kw">expression</span>(theta[<span class="dv">2</span>]), <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(ds<span class="op">$</span>theta2), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:2coins2"></span>
<img src="_main_files/figure-html/2coins2-1.png" alt="Bias of Two Coins - Theta 2" width="672" />
<p class="caption">
Figure 2.14: Bias of Two Coins - Theta 2
</p>
</div>
<p>In the Figures <a href="parameter-estimation.html#fig:2coins2">2.14</a> and <a href="parameter-estimation.html#fig:2coins">2.13</a> we can see the distribution of samples drawn from the Beta distribution using the prior parameters and the observed data we have. The red line represents the mean of the samples for each coin. Our resulting bias values are:</p>
<ul>
<li><span class="math inline">\(\theta_{1}\)</span> = 0.5</li>
<li><span class="math inline">\(\theta_{2}\)</span> = 0.21</li>
</ul>
</div>
<div id="change-point-example" class="section level3">
<h3><span class="header-section-number">2.6.6</span> Change Point Example</h3>
<div class="figure"><span id="fig:bernChangePointExampleFig"></span>
<img src="_main_files/figure-html/bernChangePointExampleFig-1.png" alt="Change Point Example" width="672" />
<p class="caption">
Figure 2.15: Change Point Example
</p>
</div>
<p>What if the problem we are solving is more complicated? Let’s say I flip a coin repeatedly, but at some point I switch to another coin with a different bias (<span class="math inline">\(\theta\)</span>) as shown in Figure <a href="parameter-estimation.html#fig:bernChangePointExampleFig">2.15</a>. I want to detect the point in time when coin 1 was swapped out for coin 2. We can use Gibbs sampling to solve this problem.</p>
<p>In the previous example we only needed to get a posterior of a single variable (technically there were two variables, but both have the same posterior conditional and were independent of one another). In this case we have 3 variables that we need to estimate:</p>
<ol style="list-style-type: decimal">
<li>Coin bias for coin 1: <span class="math inline">\(\theta_{1}\)</span></li>
<li>Coin bias for coin 2: <span class="math inline">\(\theta_{2}\)</span></li>
<li>The point in time, i.e. on which flip, the coin was swapped from coin 1 to coin 2: <em>n</em></li>
</ol>
<p>We have three variables, but how do we describe the actual process we are modeling? In Equation <a href="parameter-estimation.html#eq:cptDistributions">(2.33)</a> the distrutions for each variable are displayed. The coinflips, <em>x</em>, are drawn from a bernoulli distribution, but are dependent on the coin being flipped and the bias of that coin, <span class="math inline">\(\theta_{i}\)</span>. From the previous examples we know <span class="math inline">\(\theta\)</span> is modeled using a beta distribution. The change point, <em>n</em>, is the time when coin 2 replaces coin 1. Therefore the possible values of <em>n</em> are between 2, the second sample in discrete time, and <em>N</em>, the final discrete time in our example. All values of <em>n</em> are equally probable and therefore can be modeled as a uniform distribution.</p>
<p><span class="math display" id="eq:cptDistributions">\[
\begin{equation}
\begin{aligned}
  x &amp;\sim
    \begin{cases}
      Bern(x_{i};\theta_{1}) \quad 1 \le i \le n \\
      Bern(x_{i};\theta_{2}) \quad n \lt i \lt N 
    \end{cases} \\
  n &amp;\sim Uniform(2...N) \\
  \theta_{i} &amp;\sim Beta(\theta_{i}, a,b)
\end{aligned}
\tag{2.33}
\end{equation}
\]</span></p>
<div id="derivation-of-joint-distribution" class="section level4">
<h4><span class="header-section-number">2.6.6.1</span> Derivation of Joint Distribution</h4>
<p>Now that we have defined all the unknowns of our model, we derive the full joint distribution.</p>
<p><span class="math display" id="eq:cptJointDistribution">\[
\begin{equation}
p(\theta_{1}, \theta_{2}, n| x_{1:n}) \propto \overbrace{p(x_{1:N}|\theta_{1})
p(x_{n+1:N}|\theta_{2})}^{Likelihoods}
\overbrace{p(\theta_{1})p(\theta_{2})p(n)}^{Priors}
\tag{2.34}
\end{equation}
\]</span></p>
<p>Our goal here is to get one equation to estimate the posterior of each of our variables, aka the posterior conditionals. The easiest way to accomplish this task is to identify the terms of the joint distribution that contain the variable you want the posterior conditional for.</p>
<p>Let’s start by breaking <a href="parameter-estimation.html#eq:cptJointDistribution">(2.34)</a> a bit further.</p>
$$
<span class="math display" id="eq:cptJointDistExpand">\[\begin{equation}
\begin{aligned}
p(\theta_{1}, \theta_{2}, n| x_{1:n}) &amp;\propto 
  (\prod_{1}^{n}p(x_{i}|\theta_{1}))
  (\prod_{n+1}^{N}p(x_{i}|\theta_{2}))
  p(\theta_{1})p(\theta_{2})p(n)\\
&amp;\propto [\theta_{1}^{z_{1}}(1-\theta_{1})^{n-z_{1}}]
  [\theta_{2}^{z_{2}}(1-\theta_{2})^{N-(n+1)-z_{2}}]
  p(\theta_{1})p(\theta_{2})p(n)\\
  \\
&amp;\propto [\theta_{1}^{z_{1}}(1-\theta_{1})^{n-z_{1}}]
  [\theta_{2}^{z_{2}}(1-\theta_{2})^{N-(n+1)-z_{2}}]
  {{\theta_{1}^{(a_{1}-1)}(1-\theta_{1})^{(b_{1}-1)}}\over \beta(a_{1},b_{1})}
  {{\theta_{2}^{(a_{2}-1)}(1-\theta_{2})^{(b_{2}-1)}}\over \beta(a_{2},b_{2})}
  {1\over N}\\
  \\

\end{aligned}
\tag{2.35}
\end{equation}\]</span>
<p>$$</p>
<p>Then we move on to solving for <em>n</em>’s posterior conditional. In Equation <a href="parameter-estimation.html#eq:cptJointDistExpand">(2.35)</a> we see that only the likelihood terms and the priors for the <span class="math inline">\(\theta\)</span>’s contain <em>n</em>. Using these terms we can solve for the posterior conditional. While we are at it we will also take the log of the conditional posterior as is good practice to prevent issues such as underflow.</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
p(n| x_{1:n}, \theta_{1}, \theta_{2}) &amp;\propto  [\theta_{1}^{z_{1}}(1-\theta_{1})^{n-z_{1}}]
  [\theta_{2}^{z_{2}}(1-\theta_{2})^{N-(n+1)-z_{2}}]\\
log(p(n| x_{1:n}, \theta_{1}, \theta_{2})) &amp;\propto  
  log([\theta_{1}^{z_{1}}(1-\theta_{1})^{n-z_{1}}]) +
  log([\theta_{2}^{z_{2}}(1-\theta_{2})^{N-(n+1)-z_{2}}])
\end{aligned}
\end{equation}
\]</span></p>
<p>To get the posterior conditionals for the <span class="math inline">\(\theta\)</span> values we will need to utilize the conjugate prior relationship between the likelihoods and the priors. First we will collapse the priors and likelihoods for the <span class="math inline">\(\theta\)</span> values.</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
p(\theta_{1}, \theta_{2}, n| x_{1:n}) &amp;\propto            
  [\theta_{1}^{(z_{1}+a_{1}-1)}(1-\theta_{1})^{(n-z_{1}+b_{1}-1)}]
  [\theta_{2}^{(z_{2}+a_{2}-1)}(1-\theta_{2})^{(N-n-1-z_{2}+b_{2}-1)}]({1 \over N})\\
&amp;\propto Beta(a_{1}+z_{1}, n-z_{1}+b_{1}) Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})({1\over N})
\end{aligned}
\end{equation}
\]</span></p>
<p>Now we can solve for each of the <span class="math inline">\(\theta\)</span>’s posterior conditionals.</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
p(\theta_{1}| x_{1:n},\theta_{2}, n) &amp;\propto Beta(a_{1}+z_{1}, n-z_{1}+b_{1})\\
log(p(\theta_{1}| x_{1:n},\theta_{2}, n)) &amp;\propto log(Beta(a_{1}+z_{1}, n-z_{1}+b_{1}))
\end{aligned}
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
p(\theta_{2}| x_{1:n},\theta_{1}, n) &amp;\propto Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2})\\
log(p(\theta_{2}| x_{1:n},\theta_{1}, n)) &amp;\propto log(Beta(z_{2}+a_{2}, N-n-1-z_{2}+b_{2}))
\end{aligned}
\end{equation}
\]</span></p>
<p>Now let’s put our derived posteriors to work use Gibbs sampling to estimate our change point and coin biases.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">real_thetas &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.6</span>)
N &lt;-<span class="st"> </span><span class="dv">300</span>
a =<span class="st"> </span><span class="dv">2</span>
b =<span class="st"> </span><span class="dv">3</span>
change_point &lt;-<span class="st"> </span><span class="dv">100</span>
x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rbinom</span>(<span class="dv">1</span><span class="op">:</span>change_point, <span class="dv">1</span>, real_thetas[<span class="dv">1</span>]),<span class="kw">rbinom</span>((change_point<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>N, <span class="dv">1</span>, real_thetas[<span class="dv">2</span>]))


## Initialize all parameters

<span class="co"># n ~ uniform </span>
n &lt;-<span class="st"> </span><span class="kw">round</span>(N<span class="op">*</span><span class="kw">runif</span>(<span class="dv">1</span>))
<span class="co"># theta1 ~ beta(a,b)</span>
theta1 &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1</span>, a, b)
<span class="co"># theta2 ~ beta(a,b)</span>
theta2 &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1</span>, a, b)



niters =<span class="st"> </span><span class="dv">3000</span>
burnin =<span class="st"> </span><span class="dv">1000</span>

params =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> (niters<span class="op">-</span>burnin), <span class="dt">ncol=</span><span class="dv">3</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>niters){
  
  
  z1 &lt;-<span class="st"> </span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>n])
  <span class="cf">if</span>(n <span class="op">==</span><span class="st"> </span>N){
    z2 &lt;-<span class="st"> </span><span class="dv">0</span>
  }<span class="cf">else</span>{
    z2 &lt;-<span class="st"> </span><span class="kw">sum</span>(x[(n<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>N])
  }
  theta1 =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>z1, <span class="dt">shape2 =</span> b <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span>z1)
  <span class="co"># get value theta2| all other vars</span>
  theta2 =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">shape1 =</span> a <span class="op">+</span><span class="st"> </span>z2, <span class="dt">shape2 =</span>N<span class="op">-</span>n<span class="op">-</span><span class="dv">1</span><span class="op">-</span>z2<span class="op">+</span>b)
  
  
  ## 2 things: 1 - should I be summing all the values over these? 
  <span class="co"># No - the product is being calculated due to the sum - should be fine</span>
  n_multi &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)
  <span class="cf">for</span>(steps <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>N){
    <span class="cf">if</span>(steps<span class="op">==</span>N <span class="op">||</span><span class="st"> </span>theta2 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>){
      n_multi[steps] &lt;-<span class="st"> </span><span class="kw">log</span>(theta1<span class="op">^</span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>steps]) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>theta1)<span class="op">^</span>(steps<span class="op">-</span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>steps])))
    }<span class="cf">else</span>{
      n_multi[steps] &lt;-<span class="st"> </span><span class="kw">log</span>(theta1<span class="op">^</span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>steps]) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>theta1)<span class="op">^</span>(steps<span class="op">-</span><span class="kw">sum</span>(x[<span class="dv">1</span><span class="op">:</span>steps]))) <span class="op">+</span>
<span class="st">        </span><span class="kw">log</span>(theta2<span class="op">^</span><span class="kw">sum</span>(x[(steps <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>N]) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>theta2)<span class="op">^</span>(N<span class="op">-</span>steps<span class="op">-</span><span class="dv">1</span><span class="op">-</span><span class="kw">sum</span>(x[(steps<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>N])))
    }
  }
  
  n_multi &lt;-<span class="st"> </span><span class="kw">exp</span>(n_multi[<span class="dv">2</span><span class="op">:</span>N] <span class="op">-</span><span class="st"> </span><span class="kw">max</span>(n_multi[<span class="dv">2</span><span class="op">:</span>N]))
  <span class="co"># offset by 1 </span>
  <span class="co"># you n is equally probably between 2 and N and zero at n=1</span>
  <span class="co"># we only calculate p(n) from n=2 to N</span>
  n &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, n_multi<span class="op">/</span><span class="kw">sum</span>(n_multi))[,<span class="dv">1</span>] <span class="op">==</span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  <span class="cf">if</span> (i <span class="op">&gt;=</span><span class="st"> </span>burnin){
    params[(i<span class="op">-</span>burnin), ] =<span class="st"> </span><span class="kw">c</span>(theta1,theta2, n)
  }
}

ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">theta =</span> <span class="kw">c</span>(<span class="kw">rep</span>(real_thetas[<span class="dv">1</span>],N<span class="op">-</span>change_point),
                                  <span class="kw">rep</span>(real_thetas[<span class="dv">2</span>],change_point)), 
                 <span class="dt">sample_index =</span> <span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">params_df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(params)
<span class="kw">names</span>(params_df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;theta1&#39;</span>, <span class="st">&#39;theta2&#39;</span>, <span class="st">&#39;change_point&#39;</span>)

<span class="kw">ggplot</span>(params_df, <span class="kw">aes</span>(<span class="dt">x =</span> change_point)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill=</span><span class="st">&quot;#7A99AC&quot;</span>, <span class="dt">color =</span><span class="st">&#39;#1A384A&#39;</span>, <span class="dt">binwidth =</span> <span class="dv">5</span>, <span class="dt">bins =</span> <span class="kw">floor</span>(N<span class="op">/</span><span class="dv">5</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>,N)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(params_df<span class="op">$</span>change_point), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&#39;Change Point Estimate&#39;</span>, <span class="dt">x=</span><span class="st">&#39;Change Point&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) </code></pre></div>
<div class="figure"><span id="fig:bernChangePointN"></span>
<img src="_main_files/figure-html/bernChangePointN-1.png" alt="Estimated Change Point" width="672" />
<p class="caption">
Figure 2.16: Estimated Change Point
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(params_df, <span class="kw">aes</span>(<span class="dt">x =</span> theta1)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill=</span><span class="st">&quot;#7A99AC&quot;</span>, <span class="dt">color =</span><span class="st">&#39;#1A384A&#39;</span> , <span class="dt">binwidth =</span> <span class="fl">0.025</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(params_df<span class="op">$</span>theta1), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(theta[<span class="dv">1</span>]<span class="op">~</span>Estimate), <span class="dt">x=</span><span class="kw">expression</span>(theta[<span class="dv">1</span>]), <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) </code></pre></div>
<div class="figure"><span id="fig:bernChangePointTheta1"></span>
<img src="_main_files/figure-html/bernChangePointTheta1-1.png" alt="Estimated Theta 1" width="672" />
<p class="caption">
Figure 2.17: Estimated Theta 1
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(params_df, <span class="kw">aes</span>(<span class="dt">x =</span> theta2)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">fill=</span><span class="st">&quot;#7A99AC&quot;</span>, <span class="dt">color =</span><span class="st">&#39;#1A384A&#39;</span> , <span class="dt">binwidth =</span> <span class="fl">0.025</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(params_df<span class="op">$</span>theta2), <span class="dt">color=</span><span class="st">&#39;#b7091a&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(theta[<span class="dv">2</span>]<span class="op">~</span>Estimate), <span class="dt">x=</span><span class="kw">expression</span>(theta[<span class="dv">2</span>]), <span class="dt">y =</span> <span class="st">&#39;Density&#39;</span>) </code></pre></div>
<div class="figure"><span id="fig:bernChangePointTheta2"></span>
<img src="_main_files/figure-html/bernChangePointTheta2-1.png" alt="Estimated Theta 2" width="672" />
<p class="caption">
Figure 2.18: Estimated Theta 2
</p>
</div>
<p>The resulting estimates for each of our parameters is shown below along with the real value used to generate the dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(<span class="dt">theta_1 =</span> <span class="kw">c</span>(real_thetas[<span class="dv">1</span>], <span class="kw">mean</span>(params_df<span class="op">$</span>theta1)),
       <span class="dt">theta_2 =</span> <span class="kw">c</span>(real_thetas[<span class="dv">2</span>], <span class="kw">mean</span>(params_df<span class="op">$</span>theta2)),
       <span class="dt">n =</span> <span class="kw">c</span>(change_point, <span class="kw">floor</span>(<span class="kw">mean</span>(params_df<span class="op">$</span>change_point))),
       <span class="dt">row.names =</span> <span class="kw">c</span>(<span class="st">&#39;True&#39;</span>, <span class="st">&#39;Estimated&#39;</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>(<span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;\u03b8\u2081&#39;</span>, <span class="st">&#39;\u03b8\u2082&#39;</span>, <span class="st">&#39;n&#39;</span>),
        <span class="dt">digits =</span> <span class="dv">2</span>,
        <span class="dt">row.names =</span> <span class="ot">TRUE</span>, 
        <span class="dt">caption =</span> <span class="st">&#39;Change Point Parameters and Estimates&#39;</span>)</code></pre></div>
<table>
<caption><span id="tab:changePointEstimates">Table 2.1: </span>Change Point Parameters and Estimates</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">θ₁</th>
<th align="right">θ₂</th>
<th align="right">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>True</td>
<td align="right">0.20</td>
<td align="right">0.6</td>
<td align="right">100</td>
</tr>
<tr class="even">
<td>Estimated</td>
<td align="right">0.21</td>
<td align="right">0.6</td>
<td align="right">103</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="what-is-lda.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multinomial-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
