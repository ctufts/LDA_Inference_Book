<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The Little Book of LDA</title>
  <meta name="description" content="A comprehensive overview of LDA and Gibbs Sampling.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="The Little Book of LDA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ldabook.com/" />
  <meta property="og:image" content="https://ldabook.com/Images/cover_image_small.png" />
  <meta property="og:description" content="A comprehensive overview of LDA and Gibbs Sampling." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="The Little Book of LDA" />
  <meta name="twitter:site" content="@devlintufts" />
  <meta name="twitter:description" content="A comprehensive overview of LDA and Gibbs Sampling." />
  <meta name="twitter:image" content="https://ldabook.com/Images/cover_image_small.png" />

<meta name="author" content="Chris Tufts">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  <link rel="shortcut icon" href="Images/favicon.ico" type="image/x-icon">
<link rel="prev" href="parameter-estimation.html">
<link rel="next" href="word-representations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-62188022-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-62188022-3');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to The Little Book of LDA</a></li>
<li class="chapter" data-level="" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="" data-path="package-references.html"><a href="package-references.html"><i class="fa fa-check"></i>Package References</a></li>
<li class="chapter" data-level="1" data-path="what-is-lda.html"><a href="what-is-lda.html"><i class="fa fa-check"></i><b>1</b> What is LDA?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#animal-generator"><i class="fa fa-check"></i><b>1.1</b> Animal Generator</a><ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#generating-the-mixtures"><i class="fa fa-check"></i><b>1.1.1</b> Generating the Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-lda.html"><a href="what-is-lda.html#inference"><i class="fa fa-check"></i><b>1.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#distributions"><i class="fa fa-check"></i><b>2.1</b> Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bernoulli"><i class="fa fa-check"></i><b>2.1.1</b> Bernoulli</a></li>
<li class="chapter" data-level="2.1.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#beta-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#inference-the-building-blocks"><i class="fa fa-check"></i><b>2.2</b> Inference: The Building Blocks</a></li>
<li class="chapter" data-level="2.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-likelihood"><i class="fa fa-check"></i><b>2.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-a-posteriori"><i class="fa fa-check"></i><b>2.4</b> Maximum a Posteriori</a></li>
<li class="chapter" data-level="2.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bayesian-inference"><i class="fa fa-check"></i><b>2.5</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.5.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#analytical-solution"><i class="fa fa-check"></i><b>2.5.1</b> Analytical Solution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.6</b> Gibbs Sampling</a><ul>
<li class="chapter" data-level="2.6.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#the-issue-of-intractability"><i class="fa fa-check"></i><b>2.6.1</b> The Issue of Intractability</a></li>
<li class="chapter" data-level="2.6.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#a-tale-of-two-mcs"><i class="fa fa-check"></i><b>2.6.2</b> A Tale of Two MCâ€™s</a></li>
<li class="chapter" data-level="2.6.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#conjugate-distributions-and-priors"><i class="fa fa-check"></i><b>2.6.3</b> Conjugate Distributions and Priors</a></li>
<li class="chapter" data-level="2.6.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#gibbs-sampling-1"><i class="fa fa-check"></i><b>2.6.4</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.6.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bias-of-two-coins"><i class="fa fa-check"></i><b>2.6.5</b> Bias of Two Coins</a></li>
<li class="chapter" data-level="2.6.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html#change-point-example"><i class="fa fa-check"></i><b>2.6.6</b> Change Point Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html"><i class="fa fa-check"></i><b>3</b> Multinomial Distribution</a><ul>
<li class="chapter" data-level="3.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#comparison-of-dice-vs.words"><i class="fa fa-check"></i><b>3.1</b> Comparison of Dice vs.Â Words</a></li>
<li class="chapter" data-level="3.2" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#how-multinomial-and-bernoulli-relate"><i class="fa fa-check"></i><b>3.2</b> How Multinomial and Bernoulli Relate</a></li>
<li class="chapter" data-level="3.3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#conjugate-prior-dirichlet"><i class="fa fa-check"></i><b>3.3</b> Conjugate Prior: Dirichlet</a></li>
<li class="chapter" data-level="3.4" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#gibbs-sampling---multinomial-dirichlet"><i class="fa fa-check"></i><b>3.4</b> Gibbs Sampling - Multinomial &amp; Dirichlet</a><ul>
<li class="chapter" data-level="3.4.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#derivation-of-gibbs-sampling-solution-of-word-distribution-single-doc"><i class="fa fa-check"></i><b>3.4.1</b> Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="word-representations.html"><a href="word-representations.html"><i class="fa fa-check"></i><b>4</b> Word Representations</a><ul>
<li class="chapter" data-level="4.1" data-path="word-representations.html"><a href="word-representations.html#bag-of-words"><i class="fa fa-check"></i><b>4.1</b> Bag of Words</a></li>
<li class="chapter" data-level="4.2" data-path="word-representations.html"><a href="word-representations.html#word-counts"><i class="fa fa-check"></i><b>4.2</b> Word Counts</a></li>
<li class="chapter" data-level="4.3" data-path="word-representations.html"><a href="word-representations.html#plug-and-play-lda"><i class="fa fa-check"></i><b>4.3</b> Plug and Play LDA</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html"><i class="fa fa-check"></i><b>5</b> LDA as a Generative Model</a><ul>
<li class="chapter" data-level="5.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#general-terminology"><i class="fa fa-check"></i><b>5.1</b> General Terminology</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#selecting-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Selecting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generative-model"><i class="fa fa-check"></i><b>5.2</b> Generative Model</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generating-documents"><i class="fa fa-check"></i><b>5.2.1</b> Generating Documents</a></li>
<li class="chapter" data-level="5.2.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#lda-generative-model"><i class="fa fa-check"></i><b>5.2.2</b> LDA Generative Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lda-inference.html"><a href="lda-inference.html"><i class="fa fa-check"></i><b>6</b> LDA Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="lda-inference.html"><a href="lda-inference.html#general-overview"><i class="fa fa-check"></i><b>6.1</b> General Overview</a></li>
<li class="chapter" data-level="6.2" data-path="lda-inference.html"><a href="lda-inference.html#mathematical-derivations-for-inference"><i class="fa fa-check"></i><b>6.2</b> Mathematical Derivations for Inference</a></li>
<li class="chapter" data-level="6.3" data-path="lda-inference.html"><a href="lda-inference.html#animal-farm---code-example"><i class="fa fa-check"></i><b>6.3</b> Animal Farm - Code Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Little Book of LDA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multinomial-distribution" class="section level1">
<h1><span class="header-section-number">3</span> Multinomial Distribution</h1>
<p>This chapter focuses on the multinomial distribution which we will use to model the probability of words in a document. Similar to the previous chapter, we will cover the conjugate prior for the multinomial distributoin, the Dirichlet distribution.</p>
<div id="comparison-of-dice-vs.words" class="section level2">
<h2><span class="header-section-number">3.1</span> Comparison of Dice vs.Â Words</h2>
<p>The premise of Chapter 2 was to lay a foundation for understanding the multinomial distribution. We are now familiar with the binomial distribution which is actually a special case of the multinomial distribution where the number of possible outcomes is 2. A multinomial distribution can have 2 or more outcomes and therefore is normally shown through examples using a 6-sided die. Instead of using a die with numbers on each side, letâ€™s label the sides with the following words:</p>
<p>â€œLatentâ€ â€œDirichletâ€ â€œAllocationâ€ â€œhasâ€ â€œmanyâ€ â€œpeicesâ€</p>
<div class="figure"><span id="fig:WordDie"></span>
<img src="Images/LDA_Dice.png" alt="Die for Word Selection" width="33%" height="33%" />
<p class="caption">
Figure 3.1: Die for Word Selection
</p>
</div>
<p>In the example above, we assume it is a fair die which would result in equal probabilities of â€˜rollingâ€™ any of the 6 unique words. Therefore each word has a probability of being randomly sampled from the die of 1/6.</p>
<p>Below is an empirical example where we take each word and assign it to a single side of a fair die. The experiment, a single roll of the die, is repeated 10,000 times. We can see each word comes up at roughly the same frequency.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw 1000 samples from multinomial distribution</span>
outcomes &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10000</span>, <span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">6</span>,<span class="dv">6</span>))<span class="op">==</span><span class="dv">1</span>))
words &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">strsplit</span>(<span class="st">&quot;Latent Dirichlet Allocation has many peices&quot;</span>, <span class="st">&#39; &#39;</span>))
ds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> outcomes, <span class="dt">word =</span> <span class="kw">sapply</span>(outcomes, <span class="cf">function</span>(x)words[x]))
<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">x=</span> word)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">stat=</span><span class="st">&#39;count&#39;</span>,<span class="dt">color=</span><span class="st">&#39;#1A384A&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#7A99AC&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">labels =</span> words) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre></div>
<div class="figure"><span id="fig:multinomialDist"></span>
<img src="_main_files/figure-html/multinomialDist-1.png" alt="Sampling from Multinomial with Equal Parameters" width="672" />
<p class="caption">
Figure 3.2: Sampling from Multinomial with Equal Parameters
</p>
</div>

<div class="rmdnote">
Letâ€™s think of how this will be used in the case of LDA. If I wanted to generate a document based on a model, I could use a multinomial distribution to determine what words would be in the document. If I knew the probability of a word I could use the example above to draw a new word with each sample. Obviously some words occur much more often than others, so the â€˜fair dieâ€™ example wouldnâ€™t work for generation of document. In later sections we will build on this concept, but its good to start thinking about how this extends to applications in language.
</div>

</div>
<div id="how-multinomial-and-bernoulli-relate" class="section level2">
<h2><span class="header-section-number">3.2</span> How Multinomial and Bernoulli Relate</h2>
<p>The probability mass function for the multinomial distribution is shown in Equation <a href="multinomial-distribution.html#eq:multiPMF">(3.1)</a>:</p>
<p><span class="math display" id="eq:multiPMF">\[
\begin{equation}
f(x)=\dfrac{n!}{x_1!x_2!\cdots x_k!}\theta_1^{x_1} \theta_2^{x_2} \cdots \theta_k^{x_k}
\tag{3.1}
\end{equation}
\]</span></p>
<ul>
<li><i>k</i> - number of sides on the die</li>
<li><i>n</i> - number of times the die will be rolled</li>
</ul>
<p>The multinomial representation of the distributions weâ€™ve already discussed, binomial and bernoulli, would use the following parameters:</p>
<ul>
<li><em>k</em> sided die rolled <em>n</em> times
<ul>
<li><em>n</em> = 1, <em>k</em> = 2 is bernoulli distribution: coin has 2 sides and we are only concerned with single experiments</li>
<li><em>n</em> &gt; 1 , <em>k</em> = 2 is binomial distribution: coin has 2 sides, but we are concerned with the probability of the outcome of a series of flips.</li>
</ul></li>
</ul>
<p>To help illustrate the relationship between Bernoulli and Multinomial distributions we need to recall the bernoulli probability mass function shown in Equation <a href="multinomial-distribution.html#eq:bernPMFmulti">(3.2)</a>.</p>
<p><span class="math display" id="eq:bernPMFmulti">\[
\begin{equation}
f(x)=P(X=x)=\theta^{x}(1-\theta)^{1-x}, \hspace{1cm} x = \{0,1\}
\tag{3.2}
\end{equation}
\]</span> Letâ€™s swap in some different terms. We can replace the first term, <span class="math inline">\(\theta\)</span>, with <span class="math inline">\(\theta_{1}\)</span> and the second term, <span class="math inline">\((1-\theta)\)</span> as <span class="math inline">\(\theta_{2}\)</span>. This can be thought of as <span class="math inline">\(\theta_{heads}\)</span> and <span class="math inline">\(\theta_{tails}\)</span> for our coin flip example.</p>
<p><span class="math display" id="eq:bern2Multi">\[
\begin{equation}
f(x)={\theta_{1}}^{x_{1}}{\theta_{2}}^{x_{2}} \\
\tag{3.3}
\end{equation}
\]</span></p>
<p>You can see this looks a bit more like the multinomial PDF,Equation <a href="multinomial-distribution.html#eq:bernPMFmulti">(3.2)</a>, but the factorial terms are not present. In this case <em>k</em>=2 and <em>n</em>=1 and <em>x</em>â€™s can only have a value of 1 or zero. The values of a bernoulli distribution are plugged into the multinomial PDF in Equation <a href="multinomial-distribution.html#eq:multiWBernSettings">(3.4)</a>. Factorial of <em>n</em> in the numerator is always 1 since it is a single trial, i.e. <em>n</em>=1, and the denominator is also always 1 since our only possible outcomes are 0 and 1 (heads or tails). Once we simplify, we see the relationship that was stated previously: the bernoulli distribution is a special case of the multinomial distribution.</p>
<p><span class="math display" id="eq:multiWBernSettings">\[
\begin{equation}
\begin{aligned}
f(x)&amp;=\dfrac{n!}{x_1!x_2!\cdots x_k!}\theta_1^{x_1} \theta_2^{x_2} \cdots \theta_k^{x_k} \\
f(x)&amp;=\dfrac{1}{1}\theta_1^{x_1} \theta_2^{x_2} \\
f(x)&amp;={\theta_{1}}^{x_{1}}{\theta_{2}}^{x_{2}}
\end{aligned}
\tag{3.4}
\end{equation}
\]</span></p>
</div>
<div id="conjugate-prior-dirichlet" class="section level2">
<h2><span class="header-section-number">3.3</span> Conjugate Prior: Dirichlet</h2>
<p>The conjugate prior for the multinomial distribution is the Dirichlet distribution. Similar to the beta distribution, Dirichlet can be thought of as a distribution of distributions. Also note that the beta distribution is the special case of a Dirichlet distribution where the number of possible outcome is 2. This is similar to the relationship between the binomial and multinomial distributions.</p>
<p>The probability distribution function for the Dirichlet distribution is shown in Equation <a href="multinomial-distribution.html#eq:dirPDF">(3.5)</a>.</p>
<p><span class="math display" id="eq:dirPDF">\[
\begin{equation}
Dir(\overrightarrow{\theta}|\overrightarrow{\alpha})=
{ 
  {\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}
  \over{\prod _{i=1}^{K}\Gamma (\alpha _{i})}
}
\prod _{i=1}^{K}\theta_{i}^{\alpha _{i}-1}
\tag{3.5}
\end{equation}
\]</span></p>
<p>Equation <a href="multinomial-distribution.html#eq:dirPDF">(3.5)</a> is often written using the Beta function in place of the first term as seen below:</p>
<p><span class="math display">\[
Dir(\overrightarrow{\theta}|\overrightarrow{\alpha})=
{ 
  1 \over B(\alpha)
}
\prod _{i=1}^{K}\theta_{i}^{\alpha _{i}-1} 
\]</span></p>
<p>Where: <span class="math display">\[
{1\over B(\alpha)}={ 
  {\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}
  \over{\prod _{i=1}^{K}\Gamma (\alpha _{i})}
}
\]</span></p>
<p>The Dirichlet distribution is an extension of the beta distribution for <i>k</i> categories, similar to the relationship between multinomial and bernoulli distributions. To get a better sense of what the distributions look like, letâ€™s visualize a few examples at <i>k</i>=3. In both of the box plots below 10,000 random samples were drawn from a Dirichlet distribution where <i>k</i>=3 and <span class="math inline">\(\alpha\)</span> is the same for each <i>k</i> in the given plot. The first plot shows the distribution of values drawn when <span class="math inline">\(\alpha\)</span> = 100.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">100</span>,<span class="dv">100</span>)
trials &lt;-<span class="st"> </span><span class="dv">10000</span>
x &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(trials, alpha)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;theta_1&#39;</span>, <span class="st">&#39;theta_2&#39;</span>, <span class="st">&#39;theta_3&#39;</span>)
ds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.tibble</span>(x), <span class="dt">trial =</span> <span class="dv">1</span><span class="op">:</span>trials) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(theta, word, <span class="op">-</span>trial)

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">color =</span> theta, <span class="dt">fill =</span> theta, <span class="dt">x =</span> theta, <span class="dt">y =</span> word)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">x =</span> <span class="st">&#39;&#39;</span>, <span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;\U03B1 = &quot;</span>,<span class="kw">unique</span>(alpha)) ) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">1</span>]),
                              <span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">2</span>]),
                              <span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">3</span>]))) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>)<span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<div class="figure"><span id="fig:DirichletAlpha100"></span>
<img src="_main_files/figure-html/DirichletAlpha100-1.png" alt="Sampling from Dirichlet: Î±=100" width="672" />
<p class="caption">
Figure 3.3: Sampling from Dirichlet: Î±=100
</p>
</div>
<p>Below the process is repeated, but this time the <span class="math inline">\(\alpha\)</span> values are set to 1 for each category. We can see the range of distribution of values sampled with the higher <span class="math inline">\(\alpha\)</span> value is much narrower than the distribution of values sampled using <span class="math inline">\(\alpha\)</span> values of 1. This is the same pattern we saw with the beta distribution; as the shape parameters increased the distribution became more dense and the shape of the distribution narrowed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
  x &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(trials, alpha)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;theta_1&#39;</span>, <span class="st">&#39;theta_2&#39;</span>, <span class="st">&#39;theta_3&#39;</span>)
ds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.tibble</span>(x), <span class="dt">trial =</span> <span class="dv">1</span><span class="op">:</span>trials) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(theta, word, <span class="op">-</span>trial)

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">color =</span> theta, <span class="dt">fill =</span> theta, <span class="dt">x =</span> theta, <span class="dt">y =</span> word)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;\U03B8&#39;</span>, <span class="dt">x =</span> <span class="st">&#39;&#39;</span>, <span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;\U03B1 = &quot;</span>,<span class="kw">unique</span>(alpha)) ) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">1</span>]),
                              <span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">2</span>]),
                              <span class="kw">expression</span>(<span class="st">&quot;\U03B1&quot;</span>[<span class="dv">3</span>]))) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<div class="figure"><span id="fig:DirichletAlpha1"></span>
<img src="_main_files/figure-html/DirichletAlpha1-1.png" alt="Sampling from Dirichlet - Î¸=1" width="672" />
<p class="caption">
Figure 3.4: Sampling from Dirichlet - Î¸=1
</p>
</div>
<p>So what happens when the <span class="math inline">\(\alpha\)</span> values are not the same, i.e.Â the distribution is asymmetrical? In the histogram below, we see the distribution of values sampled from the dirichlet distribution for each category. The distribution of samples for each category (<span class="math inline">\(\alpha_{i}\)</span> value), are approximately centered at the ratio of the <span class="math inline">\(\alpha_{i}\)</span> value to the sum of all <span class="math inline">\(\alpha\)</span> values. This is similar to the shift of the beta distribution when using hyperparameters that were unequal (see Figure <a href="parameter-estimation.html#fig:betaShapeSkewed">2.3</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">50</span>,<span class="dv">20</span>)
alpha_prop &lt;-<span class="st"> </span>alpha<span class="op">/</span><span class="kw">sum</span>(alpha)
x &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(trials, alpha)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;theta_1&#39;</span>, <span class="st">&#39;theta_2&#39;</span>, <span class="st">&#39;theta_3&#39;</span>)
ds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.tibble</span>(x), <span class="dt">trial =</span> <span class="dv">1</span><span class="op">:</span>trials) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(theta, word, <span class="op">-</span>trial)

<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">color =</span> theta, <span class="dt">fill=</span>theta, <span class="dt">x =</span> word)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">position=</span><span class="st">&#39;identity&#39;</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="co"># geom_line(stat=&#39;density&#39;) + </span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;\U03B8&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Count&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">label =</span> alpha,
                       <span class="dt">name =</span> <span class="st">&quot;\U03B1&quot;</span> ) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="dt">label =</span> alpha,
                       <span class="dt">name =</span> <span class="st">&quot;\U03B1&quot;</span> )</code></pre></div>
<div class="figure"><span id="fig:DirichletAlphaMixed"></span>
<img src="_main_files/figure-html/DirichletAlphaMixed-1.png" alt="Sampling from Dirichlet: Î¸=[10,50,20]" width="672" />
<p class="caption">
Figure 3.5: Sampling from Dirichlet: Î¸=[10,50,20]
</p>
</div>
</div>
<div id="gibbs-sampling---multinomial-dirichlet" class="section level2">
<h2><span class="header-section-number">3.4</span> Gibbs Sampling - Multinomial &amp; Dirichlet</h2>
<p>Prior to getting into an example of Gibbs sampling as it applies to inferring the parameters of a multinomial distribution, letâ€™s first describe a model which generates words for a single document. As you can imagine this would be modeled as a multinomial distribution with parameters <span class="math inline">\(\overrightarrow{\theta} = \theta_{1}, \theta_{2}, ... \theta_{n}\)</span> for words 1 to n. The model would be capable of generating a bag of words representation of a document. The term <i>â€˜bag of wordsâ€™</i> refers to words in no particular order, i.e.Â the document we would be generating would not have structured sentences, but would contain all the components of the document. The model will be used to generate a document using a limited vocabulary, only 3 distinct words: ğŸ“˜,ğŸ“•,ğŸ“—.</p>
<p>First we are going to create a seed document which I will refer to as an <em>ideal</em> document. The document is used as a basis of our <span class="math inline">\(\alpha\)</span>â€™s for our prior described by the Dirichlet distribution. To create this document we first define the mixture of words in the document:</p>
<ul>
<li>ğŸ“˜ : 10%</li>
<li>ğŸ“• : 10%</li>
<li>ğŸ“— : 80%</li>
</ul>
<p>To clarify, this means the document will contain 80% blue books, 10% green books, and 10% red books:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># use letters function as your vocabulary</span>
v &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>, <span class="st">&#39;blue&#39;</span>)
nwords &lt;-<span class="st"> </span><span class="dv">10</span>
doc_theta &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">8</span>)
document&lt;-<span class="kw">rep</span>(v, doc_theta<span class="op">*</span>nwords)
books &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">label =</span> <span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>), 
                    <span class="dt">code =</span> <span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>))

<span class="kw">cat</span>(<span class="kw">sapply</span>(document, <span class="cf">function</span>(x) books<span class="op">$</span>code[<span class="kw">which</span>(books<span class="op">$</span>label <span class="op">==</span><span class="st"> </span>x)]))</code></pre></div>
<pre><code>## ğŸ“• ğŸ“— ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜</code></pre>
<p>Do you recall the beta/bernoulli example? The way we informed our prior was using some prior information we had, i.e.Â the number of heads and tails previously obtained from flipping the two coins. We will use the document above as the basis of our <span class="math inline">\(\alpha\)</span> paramters for the Dirichlet distribution, i.e.Â our prior for the multinomial. In more general language, we want to generate documents similar to our â€˜idealâ€™ document.</p>
<p>So letâ€™s generate a new document using the word counts from our <i>ideal</i> document as our <span class="math inline">\(\alpha\)</span> values for the dirichlet prior. Then we use the <span class="math inline">\(\theta\)</span> values generated by the dirichlet prior as the parameters for a multinomial distribution to generate the next term in the document.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">words &lt;-<span class="st"> </span>document
word_counts &lt;-<span class="st"> </span><span class="kw">table</span>(words)
alphas &lt;-<span class="st">  </span>word_counts
new_doc &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="st">&#39;&#39;</span>, nwords)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nwords){
  <span class="kw">set.seed</span>(i)
  p =<span class="st"> </span><span class="kw">rdirichlet</span>(<span class="dv">1</span>,alphas)
  <span class="kw">set.seed</span>(i)
  new_doc[i] &lt;-<span class="st"> </span><span class="kw">names</span>(word_counts)[<span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, p) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)]
}
<span class="kw">cat</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>, <span class="kw">sapply</span>(new_doc, <span class="cf">function</span>(x) books<span class="op">$</span>code[<span class="kw">which</span>(books<span class="op">$</span>label <span class="op">==</span><span class="st"> </span>x)]))</code></pre></div>
<pre><code>## 
##  ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“• ğŸ“˜ ğŸ“˜ ğŸ“˜</code></pre>
<p>Itâ€™s not quite the same as the original, but that should be expected. This is a model that generates documents probabalistically based on some prior information. So letâ€™s make a few more documents and see how this changes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_counts &lt;-<span class="st"> </span><span class="kw">table</span>(words)
alphas &lt;-<span class="st">  </span>word_counts
nwords &lt;-<span class="st"> </span><span class="dv">10</span>
ndocs  &lt;-<span class="st"> </span><span class="dv">5</span>
word_encodings &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">label =</span> <span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>), 
                <span class="dt">code =</span> <span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>), 
                <span class="dt">word_props =</span> <span class="kw">c</span>(.<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">8</span>))

thetas &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(ndocs<span class="op">*</span>nwords, alphas)
<span class="kw">print</span>(<span class="kw">head</span>(thetas))</code></pre></div>
<pre><code>##           [,1]       [,2]        [,3]
## [1,] 0.6259047 0.33535589 0.038739455
## [2,] 0.8641536 0.07229474 0.063551631
## [3,] 0.8292271 0.02713615 0.143636766
## [4,] 0.9399257 0.05494752 0.005126745
## [5,] 0.7641922 0.19525569 0.040552159
## [6,] 0.9015058 0.02198210 0.076512137</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">selected_words &lt;-<span class="st"> </span><span class="kw">apply</span>(thetas, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,x)<span class="op">==</span><span class="dv">1</span>))

ds &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">doc_id =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>ndocs, <span class="dt">each =</span> nwords),
             <span class="dt">word =</span> word_encodings<span class="op">$</span>label[selected_words], 
             <span class="dt">word_uni =</span> word_encodings<span class="op">$</span>code[selected_words])


ds <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(
  <span class="dt">tokens =</span> <span class="kw">paste</span>(word_uni, <span class="dt">collapse =</span> <span class="st">&#39; &#39;</span>)
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>(<span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&#39;Document&#39;</span>, <span class="st">&#39;Words&#39;</span>))</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">Document</th>
<th align="left">Words</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">ğŸ“˜ ğŸ“• ğŸ“˜ ğŸ“˜ ğŸ“• ğŸ“˜ ğŸ“• ğŸ“• ğŸ“˜ ğŸ“˜</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“• ğŸ“˜ ğŸ“˜</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“— ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“• ğŸ“˜</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">ğŸ“˜ ğŸ“˜ ğŸ“— ğŸ“• ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">ğŸ“• ğŸ“˜ ğŸ“— ğŸ“˜ ğŸ“• ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜ ğŸ“˜</td>
</tr>
</tbody>
</table>
<p>As we can see each document composition is similar, but the word counts and order are different each time. This is to be expected as we are using a model with some degree of randomness to generate our documents.</p>
<p>So now onto inferernce â€¦.</p>
<p>The process above is known as a generative model. We created documents using a model with a given set of parameters. Inference is going to take this general concept and look at it from a different angle. Instead of generating documents with our model we are going to take a series of pre-existing documents and infer what model created them. We are going to make the assumption that the structure of the model is the same as the generative example, i.e.Â all documents are generated based on the same word mixture ratios:</p>
<ul>
<li>ğŸ“˜ : 10%</li>
<li>ğŸ“• : 10%</li>
<li>ğŸ“— : 80%</li>
</ul>
<p>Letâ€™s use the 5 documents we previously generated as our basis and infer the parameters used to generate them via Gibbs sampling.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alphas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(books))
n &lt;-<span class="st"> </span><span class="kw">table</span>(ds<span class="op">$</span>word)

niters =<span class="st"> </span><span class="dv">2000</span>
burnin =<span class="st"> </span><span class="dv">500</span>

thetas =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> (niters<span class="op">-</span>burnin), <span class="dt">ncol=</span><span class="kw">nrow</span>(books), 
                <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="ot">NULL</span>, <span class="kw">c</span>(<span class="kw">names</span>(n))))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>niters){
  theta =<span class="st"> </span><span class="kw">rdirichlet</span>(<span class="dv">1</span>,n<span class="op">+</span>alphas)

  
  <span class="cf">if</span> (i <span class="op">&gt;=</span><span class="st"> </span>burnin){
    thetas[(i<span class="op">-</span>burnin), ] =<span class="st"> </span>theta
  }
}


 df &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(thetas) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">   </span><span class="kw">gather</span>(word, theta)
<span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">y=</span>theta, <span class="dt">x =</span> word, <span class="dt">fill=</span>word)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_violin</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values=</span><span class="kw">c</span>(<span class="st">&#39;#0057e7&#39;</span>,  <span class="st">&#39;#008744&#39;</span>,<span class="st">&#39;#d62d20&#39;</span>))</code></pre></div>
<div class="figure"><span id="fig:unigramInference10"></span>
<img src="_main_files/figure-html/unigramInference10-1.png" alt="Gibbs Sampling with 5 Documents" width="672" />
<p class="caption">
Figure 3.6: Gibbs Sampling with 5 Documents
</p>
</div>
<div id="derivation-of-gibbs-sampling-solution-of-word-distribution-single-doc" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc)</h3>
<p>Below is a general overview of how inferrence can be carried out using Gibbs sampling. Recall a conjugate priors has the same form as the posterior distribution. In equation <a href="multinomial-distribution.html#eq:uniGramGibbs">(3.6)</a> we start with the proportional solution, i.e.Â no evidence term, for estimating a posterior through sampling. We need the likelihood, which is derived from the multinomial distribution, and the prior, which is derived from the dirichlet distribution. Once we plug in the prior and likelihood and simplify, we find that we are left with a Dirichlet PDF with the input parameters of <span class="math inline">\(\overrightarrow{\alpha} + \overrightarrow{n}\)</span> where <em>n</em> are the observed word counts.</p>
<p><span class="math display" id="eq:uniGramGibbs">\[
\begin{equation}
\begin{aligned}
p(\theta|D) &amp;\propto p(D|\theta)p(\theta)\\
&amp;\propto \prod _{i=1}^{K}\theta^{n(k)} { 
  {\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}
  \over{\prod _{i=1}^{K}\Gamma (\alpha _{i})}
}
\prod _{i=1}^{K}\theta_{i}^{\alpha _{i}-1} \\
&amp;\propto{ 
  {\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}
  \over{\prod _{i=1}^{K}\Gamma (\alpha _{i})}
}\prod _{i=1}^{K}\theta_{i}^{\alpha _{i}+n_{k}-1} \\
&amp;\propto Dir(\overrightarrow{\alpha} + \overrightarrow{n})
\end{aligned}
\tag{3.6}
\end{equation}
\]</span></p>
<p>We can see our mixture estimates are significantly different from the real model used to generate the documents in Figure <a href="multinomial-distribution.html#fig:unigramInference10">3.6</a>. So why is this? One of the issues here is that our sample are documents with only 10 words. Therefore an average document has 1 ğŸ“˜, 1 ğŸ“•, and 8 ğŸ“—, but it is not unusual to see a slight variation which causes mixture shifts of 10% or more. Letâ€™s try the same example, but this time instead of only generating 5 documents we will genertate 500 and use this as our sample to infer the word mixtures from.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">word_counts &lt;-<span class="st"> </span><span class="kw">table</span>(words)
alphas &lt;-<span class="st">  </span>word_counts
nwords &lt;-<span class="st"> </span><span class="dv">10</span>
ndocs  &lt;-<span class="st"> </span><span class="dv">500</span>
word_encodings &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">label =</span> <span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>), 
                <span class="dt">code =</span> <span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>), 
                <span class="dt">word_props =</span> <span class="kw">c</span>(.<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">8</span>))

thetas &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(ndocs<span class="op">*</span>nwords, alphas)
<span class="kw">print</span>(<span class="kw">head</span>(thetas))</code></pre></div>
<pre><code>##           [,1]       [,2]       [,3]
## [1,] 0.5154606 0.40401130 0.08052810
## [2,] 0.8526076 0.12103847 0.02635393
## [3,] 0.7801023 0.14641427 0.07348343
## [4,] 0.8444930 0.06278609 0.09272094
## [5,] 0.7224804 0.11597181 0.16154776
## [6,] 0.8639552 0.01028122 0.12576353</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">selected_words &lt;-<span class="st"> </span><span class="kw">apply</span>(thetas, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,x)<span class="op">==</span><span class="dv">1</span>))

ds &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">doc_id =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>ndocs, <span class="dt">each =</span> nwords),
             <span class="dt">word =</span> word_encodings<span class="op">$</span>label[selected_words], 
             <span class="dt">word_uni =</span> word_encodings<span class="op">$</span>code[selected_words])


alphas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(books))
n &lt;-<span class="st"> </span><span class="kw">table</span>(ds<span class="op">$</span>word)
<span class="kw">head</span>(n)</code></pre></div>
<pre><code>## 
##  blue green   red 
##  4008   507   485</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">niters =<span class="st"> </span><span class="dv">2000</span>
burnin =<span class="st"> </span><span class="dv">500</span>

thetas =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> (niters<span class="op">-</span>burnin), <span class="dt">ncol=</span><span class="kw">nrow</span>(books), 
                <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="ot">NULL</span>, <span class="kw">c</span>(<span class="kw">names</span>(n))))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>niters){
  theta =<span class="st"> </span><span class="kw">rdirichlet</span>(<span class="dv">1</span>,n<span class="op">+</span>alphas)

  
  <span class="cf">if</span> (i <span class="op">&gt;=</span><span class="st"> </span>burnin){
    thetas[(i<span class="op">-</span>burnin), ] =<span class="st"> </span>theta
  }
}


 df &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(thetas) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">   </span><span class="kw">gather</span>(word, theta)
<span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">y=</span>theta, <span class="dt">x =</span> word, <span class="dt">fill=</span>word)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_violin</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values=</span><span class="kw">c</span>(<span class="st">&#39;#0057e7&#39;</span>,  <span class="st">&#39;#008744&#39;</span>,<span class="st">&#39;#d62d20&#39;</span>))</code></pre></div>
<div class="figure"><span id="fig:unigramInference500"></span>
<img src="_main_files/figure-html/unigramInference500-1.png" alt="Gibbs Sampling with 500 Documents" width="672" />
<p class="caption">
Figure 3.7: Gibbs Sampling with 500 Documents
</p>
</div>
<p>In Figure <a href="multinomial-distribution.html#fig:unigramInference500">3.7</a> we can see the distribution of estimates are more dense and centered close to the true document word proportion values used to generate the sample documents.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="parameter-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="word-representations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
