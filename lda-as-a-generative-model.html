<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>LDA Tutorial</title>
  <meta name="description" content="LDA Tutorial">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="LDA Tutorial" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="LDA Tutorial" />
  <meta name="twitter:site" content="@devlintufts" />
  
  

<meta name="author" content="Chris Tufts">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="word-representations.html">
<link rel="next" href="lda-inference.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="" data-path="layout-of-book.html"><a href="layout-of-book.html"><i class="fa fa-check"></i>Layout of Book</a></li>
<li class="chapter" data-level="" data-path="package-references.html"><a href="package-references.html"><i class="fa fa-check"></i>Package References</a></li>
<li class="chapter" data-level="1" data-path="what-is-lda.html"><a href="what-is-lda.html"><i class="fa fa-check"></i><b>1</b> What is LDA?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#animal-generator"><i class="fa fa-check"></i><b>1.1</b> Animal Generator</a><ul>
<li class="chapter" data-level="1.1.1" data-path="what-is-lda.html"><a href="what-is-lda.html#generating-the-mixtures"><i class="fa fa-check"></i><b>1.1.1</b> Generating the Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-lda.html"><a href="what-is-lda.html#inference"><i class="fa fa-check"></i><b>1.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#distributions"><i class="fa fa-check"></i><b>2.1</b> Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bernoulli"><i class="fa fa-check"></i><b>2.1.1</b> Bernoulli</a></li>
<li class="chapter" data-level="2.1.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#beta-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#inference-the-building-blocks"><i class="fa fa-check"></i><b>2.2</b> Inference: The Building Blocks</a></li>
<li class="chapter" data-level="2.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-likelihood"><i class="fa fa-check"></i><b>2.3</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#maximum-a-posteriori"><i class="fa fa-check"></i><b>2.4</b> Maximum a Posteriori</a></li>
<li class="chapter" data-level="2.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bayesian-inference"><i class="fa fa-check"></i><b>2.5</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.5.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#analytical-solution"><i class="fa fa-check"></i><b>2.5.1</b> Analytical Solution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.6</b> Gibbs Sampling</a><ul>
<li class="chapter" data-level="2.6.1" data-path="parameter-estimation.html"><a href="parameter-estimation.html#the-issue-of-intractability"><i class="fa fa-check"></i><b>2.6.1</b> The Issue of Intractability</a></li>
<li class="chapter" data-level="2.6.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html#a-tale-of-two-mcs"><i class="fa fa-check"></i><b>2.6.2</b> A Tale of Two MC’s</a></li>
<li class="chapter" data-level="2.6.3" data-path="parameter-estimation.html"><a href="parameter-estimation.html#conjugate-distributions-and-priors"><i class="fa fa-check"></i><b>2.6.3</b> Conjugate Distributions and Priors</a></li>
<li class="chapter" data-level="2.6.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html#gibbs-sampling-1"><i class="fa fa-check"></i><b>2.6.4</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.6.5" data-path="parameter-estimation.html"><a href="parameter-estimation.html#bias-of-two-coins"><i class="fa fa-check"></i><b>2.6.5</b> Bias of Two Coins</a></li>
<li class="chapter" data-level="2.6.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html#change-point-example"><i class="fa fa-check"></i><b>2.6.6</b> Change Point Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html"><i class="fa fa-check"></i><b>3</b> Multinomial Distribution</a><ul>
<li class="chapter" data-level="3.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#comparison-of-dice-vs.words"><i class="fa fa-check"></i><b>3.1</b> Comparison of Dice vs. Words</a></li>
<li class="chapter" data-level="3.2" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#how-multinomial-and-bernoulli-relate"><i class="fa fa-check"></i><b>3.2</b> How Multinomial and Bernoulli Relate</a></li>
<li class="chapter" data-level="3.3" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#conjugate-prior-dirichlet"><i class="fa fa-check"></i><b>3.3</b> Conjugate Prior: Dirichlet</a></li>
<li class="chapter" data-level="3.4" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#gibbs-sampling---multinomial-dirichlet"><i class="fa fa-check"></i><b>3.4</b> Gibbs Sampling - Multinomial &amp; Dirichlet</a><ul>
<li class="chapter" data-level="3.4.1" data-path="multinomial-distribution.html"><a href="multinomial-distribution.html#derivation-of-gibbs-sampling-solution-of-word-distribution-single-doc"><i class="fa fa-check"></i><b>3.4.1</b> Derivation of Gibbs Sampling Solution of Word Distribution (Single Doc)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="word-representations.html"><a href="word-representations.html"><i class="fa fa-check"></i><b>4</b> Word Representations</a><ul>
<li class="chapter" data-level="4.1" data-path="word-representations.html"><a href="word-representations.html#bag-of-words"><i class="fa fa-check"></i><b>4.1</b> Bag of Words</a></li>
<li class="chapter" data-level="4.2" data-path="word-representations.html"><a href="word-representations.html#word-counts"><i class="fa fa-check"></i><b>4.2</b> Word Counts</a></li>
<li class="chapter" data-level="4.3" data-path="word-representations.html"><a href="word-representations.html#plug-and-play-lda"><i class="fa fa-check"></i><b>4.3</b> Plug and Play LDA</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html"><i class="fa fa-check"></i><b>5</b> LDA as a Generative Model</a><ul>
<li class="chapter" data-level="5.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#general-terminology"><i class="fa fa-check"></i><b>5.1</b> General Terminology</a><ul>
<li class="chapter" data-level="5.1.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#selecting-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Selecting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generative-model"><i class="fa fa-check"></i><b>5.2</b> Generative Model</a><ul>
<li class="chapter" data-level="5.2.1" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#generating-documents"><i class="fa fa-check"></i><b>5.2.1</b> Generating Documents</a></li>
<li class="chapter" data-level="5.2.2" data-path="lda-as-a-generative-model.html"><a href="lda-as-a-generative-model.html#lda-generative-model"><i class="fa fa-check"></i><b>5.2.2</b> LDA Generative Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lda-inference.html"><a href="lda-inference.html"><i class="fa fa-check"></i><b>6</b> LDA Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="lda-inference.html"><a href="lda-inference.html#general-overview"><i class="fa fa-check"></i><b>6.1</b> General Overview</a></li>
<li class="chapter" data-level="6.2" data-path="lda-inference.html"><a href="lda-inference.html#mathematical-derivations-for-inference"><i class="fa fa-check"></i><b>6.2</b> Mathematical Derivations for Inference</a></li>
<li class="chapter" data-level="6.3" data-path="lda-inference.html"><a href="lda-inference.html#animal-farm---code-example"><i class="fa fa-check"></i><b>6.3</b> Animal Farm - Code Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">LDA Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lda-as-a-generative-model" class="section level1">
<h1><span class="header-section-number">5</span> LDA as a Generative Model</h1>
<p>This chapter is going to focus on LDA as a generative model. I’m going to build on the unigram generation example from the last chapter and with each new example a new variable will be added until we work our way up to LDA. After getting a grasp of LDA as a generative model in this chapter, the following chapter will focus on working backwards to answer the following question: “If I have a bunch of documents, how do I infer topic information (word distributions, topic mixtures) from them?”</p>
<div id="general-terminology" class="section level2">
<h2><span class="header-section-number">5.1</span> General Terminology</h2>
<p>Let’s get the ugly part out of the way, the parameters and variables that are going to be used in the model.</p>
<ul>
<li><p><b>alpha</b> (<span class="math inline">\(\overrightarrow{\alpha}\)</span>) : In order to determine the value of <span class="math inline">\(\theta\)</span>, the topic distirbution of the document, we sample from a dirichlet distribution using <span class="math inline">\(\overrightarrow{\alpha}\)</span> as the input parameter. What does this mean? The <span class="math inline">\(\overrightarrow{\alpha}\)</span> values are our prior information about the topic mixtures for that document. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the total number of words from each topic across all documents as the <span class="math inline">\(\overrightarrow{\beta}\)</span> values.</p></li>
<li><p><b>beta</b> (<span class="math inline">\(\overrightarrow{\beta}\)</span>) : In order to determine the value of <span class="math inline">\(\phi\)</span>, the word distirbution of a given topic, we sample from a dirichlet distribution using <span class="math inline">\(\overrightarrow{\beta}\)</span> as the input parameter. What does this mean? The <span class="math inline">\(\overrightarrow{\beta}\)</span> values are our prior information about the word distribution in a topic. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the number of times each word was used for a given topic as the <span class="math inline">\(\overrightarrow{\beta}\)</span> values.</p></li>
<li><p><b>theta</b> (<span class="math inline">\(\theta\)</span>) : Is the topic proportion of a given document. More importantly it will be used as the parameter for the multinomial distribution used to identify the topic of the next word. To clarify, the selected topic’s word distribution will then be used to select a word <em>w</em>.</p></li>
<li><p><b>phi</b> (<span class="math inline">\(\phi\)</span>) : Is the word distribution of each topic, i.e. the probability of each word in the vocabulary being generated if a given topic, <em>z</em> (z ranges from 1 to k), is selected.</p></li>
<li><p><b>xi</b> (<span class="math inline">\(\xi\)</span>) : In the case of a variable lenght document, the document length is determined by sampling from a Poisson distribution with an average length of <span class="math inline">\(\xi\)</span></p></li>
<li><b>k</b> : Topic index</li>
<li><b>z</b> : Topic selected for the next word to be generated.</li>
<li><b>w</b> : Generated Word</li>
<li><p><b>d</b> : Current Document</p></li>
</ul>
<p>Outside of the variables above all the distributions should be familiar from the previous chapter.</p>
<div id="selecting-parameters" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Selecting Parameters</h3>
<p>The intent of this section is not aimed at delving into different methods of parameter estimation for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, but to give a general understanding of how those values effect your model. For ease of understanding I will also stick with an assumption of symmetry, i.e. all values in <span class="math inline">\(\overrightarrow{\alpha}\)</span> are equal to one another and all values in <span class="math inline">\(\overrightarrow{\beta}\)</span> are equal to one another. Symmetry can be thought of as each topic having equal probability in each document for <span class="math inline">\(\alpha\)</span> and each word having an equal probability in <span class="math inline">\(\beta\)</span>.</p>
<p>In previous sections we have outlined how the <span class="math inline">\(alpha\)</span> parameters effect a Dirichlet distribution, but now it is time to connect the dots to how this effects our documents.</p>
</div>
</div>
<div id="generative-model" class="section level2">
<h2><span class="header-section-number">5.2</span> Generative Model</h2>
<p>LDA is know as a generative model. What is a generative model? Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space <span class="citation">(Bishop <a href="references.html#ref-bishop2006pattern">2006</a>)</span>. This means we can create documents with a mixture of topics and a mixture of words based on thosed topics. Let’s start off with a simple example of generating unigrams.</p>
<div id="generating-documents" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Generating Documents</h3>
<div id="topic-word-mixtures-document-topic-mixtures-and-document-length-static" class="section level4">
<h4><span class="header-section-number">5.2.1.1</span> Topic Word Mixtures, Document Topic Mixtures, and Document Length Static</h4>
<p>Building on the document generating model in chapter two, let’s try to create documents that have words drawn from more than one topic. To clarify the contraints of the model will be:</p>
<ul>
<li>set number of topics (2)</li>
<li>constant topic distributions in each document</li>
<li>constant word distribution in each topic</li>
</ul>
<p><em>Known values:</em></p>
<ul>
<li>2 topics : word distributions of each topic below
<ul>
<li><span class="math inline">\(\phi_{1}\)</span> = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ]</li>
<li><span class="math inline">\(\phi_{2}\)</span> = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ]</li>
</ul></li>
<li>All Documents have same topic distribution:
<ul>
<li><span class="math inline">\(\theta = [ topic a = 0.5, topic b = 0.5 ]\)</span></li>
</ul></li>
<li>All Documents contain 10 words</li>
</ul>
<p><em>Generative Model Pseudocode</em></p>
<ul>
<li>For d = 1 to D where D is the number of documents
<ul>
<li>For w = 1 to W where W is the number of words in document <em>d</em>
<ul>
<li><em>Select the topic for word w </em></li>
<li><span class="math inline">\(z_{i}\)</span> ~ Multinomial(<span class="math inline">\(\theta_{d}\)</span>)</li>
<li><em>Select word based on topic z’s word distribution</em></li>
<li><span class="math inline">\(w_{i}\)</span> ~ Multinomial(<span class="math inline">\(\phi^{(z_{i})}\)</span>)</li>
</ul></li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># number of topics</span>
M &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># let&#39;s create 10 documents</span>
vocab &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>)
alphas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,k) <span class="co"># topic document dirichlet parameters</span>

phi &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">0.9</span>,
                <span class="fl">0.4</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>), 
              <span class="dt">nrow =</span> k, 
              <span class="dt">ncol =</span> <span class="kw">length</span>(vocab), 
              <span class="dt">byrow =</span> <span class="ot">TRUE</span>)

theta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)

N &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co">#words in each document</span>
ds &lt;-<span class="kw">tibble</span>(<span class="dt">doc_id =</span> <span class="kw">rep</span>(<span class="dv">0</span>,N<span class="op">*</span>M), 
            <span class="dt">word =</span> <span class="kw">rep</span>(<span class="st">&#39;&#39;</span>, N<span class="op">*</span>M),
            <span class="dt">topic =</span> <span class="kw">rep</span>(<span class="dv">0</span>, N<span class="op">*</span>M)
            ) 
            
row_index &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M){
  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){
    <span class="co"># sample topic index , i.e. select topic</span>
    topic &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,theta)<span class="op">==</span><span class="dv">1</span>)
    <span class="co"># sample word from topic</span>
    new_word &lt;-<span class="st"> </span>vocab[<span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,phi[topic, ])<span class="op">==</span><span class="dv">1</span>)]
    ds[row_index,] &lt;-<span class="st"> </span><span class="kw">c</span>(m,new_word, topic)
    row_index &lt;-<span class="st"> </span>row_index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  }
}

ds <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(
  <span class="dt">tokens =</span> <span class="kw">paste</span>(word, <span class="dt">collapse =</span> <span class="st">&#39; &#39;</span>)
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">doc_id</th>
<th align="left">tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">📘 📕 📗 📘 📘 📘 📗 📘 📗 📗</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">📗 📘 📕 📘 📘 📗 📕 📕 📗 📗</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">📗 📗 📘 📗 📗 📗 📗 📗 📘 📗</td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">📕 📗 📗 📗 📗 📕 📗 📕 📕 📗</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left">📗 📘 📗 📗 📗 📗 📕 📗 📘 📗</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">📕 📗 📗 📘 📗 📘 📘 📗 📗 📗</td>
</tr>
<tr class="odd">
<td align="left">6</td>
<td align="left">📗 📘 📗 📘 📗 📕 📕 📗 📗 📗</td>
</tr>
<tr class="even">
<td align="left">7</td>
<td align="left">📘 📗 📗 📗 📗 📗 📕 📕 📗 📕</td>
</tr>
<tr class="odd">
<td align="left">8</td>
<td align="left">📗 📗 📗 📗 📗 📗 📘 📕 📗 📕</td>
</tr>
<tr class="even">
<td align="left">9</td>
<td align="left">📗 📘 📗 📗 📗 📗 📘 📘 📘 📕</td>
</tr>
</tbody>
</table>
</div>
<div id="topic-word-mixtures-document-topic-mixtures-static-document-length-varying" class="section level4">
<h4><span class="header-section-number">5.2.1.2</span> Topic Word Mixtures &amp; Document Topic Mixtures Static, Document Length Varying</h4>
<p>This next example is going to be very similar, but it now allows for varying document length. The length of each document is determined by a Poisson distribution with an average document length of 10.</p>
<p><em>Known values:</em></p>
<ul>
<li>2 topics : word distributions of each topic below
<ul>
<li><span class="math inline">\(\phi_{1}\)</span> = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ]</li>
<li><span class="math inline">\(\phi_{2}\)</span> = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ]</li>
</ul></li>
<li>All Documents have same topic distribution:
<ul>
<li><span class="math inline">\(\theta = [ topic a = 0.5, topic b = 0.5 ]\)</span></li>
</ul></li>
</ul>
<p><em>Generative Model Pseudocode</em></p>
<ul>
<li>For d = 1 to D where D is the number of documents
<ul>
<li><em>Determine length of document</em></li>
<li><span class="math inline">\(W ~ Poisson(\xi)\)</span></li>
<li>For w = 1 to W where W is the number of words in document <em>d</em>
<ul>
<li><em>Select the topic for word w </em></li>
<li><span class="math inline">\(z_{i}\)</span> ~ Multinomial(<span class="math inline">\(\theta_{d}\)</span>)</li>
<li><em>Select word based on topic z’s word distribution </em></li>
<li><span class="math inline">\(w_{i}\)</span> ~ Multinomial(<span class="math inline">\(\phi^{(z_{i})}\)</span>)</li>
</ul></li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># number of topics</span>
M &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># let&#39;s create 10 documents</span>
vocab &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>)
alphas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,k) <span class="co"># topic document dirichlet parameters</span>

phi &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">0.9</span>,
                <span class="fl">0.4</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>), 
              <span class="dt">nrow =</span> k, 
              <span class="dt">ncol =</span> <span class="kw">length</span>(vocab), 
              <span class="dt">byrow =</span> <span class="ot">TRUE</span>)

theta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)
xi &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># average document length </span>
N &lt;-<span class="st"> </span><span class="kw">rpois</span>(M, xi) <span class="co">#words in each document</span>
ds &lt;-<span class="kw">tibble</span>(<span class="dt">doc_id =</span> <span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">sum</span>(N)), 
            <span class="dt">word   =</span> <span class="kw">rep</span>(<span class="st">&#39;&#39;</span>, <span class="kw">sum</span>(N)),
            <span class="dt">topic  =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">sum</span>(N))
            ) 
            
row_index &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M){
  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N[m]){
    <span class="co"># sample topic index , i.e. select topic</span>
    topic &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,theta)<span class="op">==</span><span class="dv">1</span>)
    <span class="co"># sample word from topic</span>
    new_word &lt;-<span class="st"> </span>vocab[<span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,phi[topic, ])<span class="op">==</span><span class="dv">1</span>)]
    ds[row_index,] &lt;-<span class="st"> </span><span class="kw">c</span>(m,new_word, topic)
    row_index &lt;-<span class="st"> </span>row_index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  }
}

ds <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(
  <span class="dt">tokens =</span> <span class="kw">paste</span>(word, <span class="dt">collapse =</span> <span class="st">&#39; &#39;</span>)
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">doc_id</th>
<th align="left">tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">📗 📘 📗 📘 📗 📗 📗 📕 📕 📗 📗 📘</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">📕 📕 📘 📗 📘 📗 📗 📗 📕 📗</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">📗 📗 📗 📗 📗 📕 📗 📘 📗 📕 📗 📘 📗</td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">📕 📗 📗 📕 📘 📘 📗 📘 📕 📘 📕 📘 📕 📘 📘 📗</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left">📗 📕 📕 📗 📘 📗 📘 📘 📘 📕 📗 📘 📗 📕 📘 📕</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">📗 📗 📗 📗 📗 📘 📕 📗 📗 📘 📕 📗 📗 📘 📗</td>
</tr>
<tr class="odd">
<td align="left">6</td>
<td align="left">📗 📘 📘 📗 📗 📗 📗 📘 📗 📗 📗 📗 📕 📗</td>
</tr>
<tr class="even">
<td align="left">7</td>
<td align="left">📘 📘 📘 📗 📕 📗 📘 📘 📗 📘 📘 📗 📕 📘 📘 📗 📗 📕 📗 📗 📘 📗</td>
</tr>
<tr class="odd">
<td align="left">8</td>
<td align="left">📘 📗 📗 📕 📕 📗 📗 📕 📗 📗 📗 📕 📗 📗 📗 📕 📕 📕 📘</td>
</tr>
<tr class="even">
<td align="left">9</td>
<td align="left">📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗</td>
</tr>
</tbody>
</table>
</div>
<div id="topic-word-mixtures-static-varying-document-topic-distributions-and-document-length" class="section level4">
<h4><span class="header-section-number">5.2.1.3</span> Topic Word Mixtures Static, Varying Document Topic Distributions and Document Length</h4>
<p>So this time we will introduce documents with different topic distributions and length.The word distributions for each topic are still fixed.</p>
<p><em>Known values:</em></p>
<ul>
<li>2 topics : word distributions of each topic below
<ul>
<li><span class="math inline">\(\phi_{1}\)</span> = [ 📕 = 0.8, 📘 = 0.2, 📗 = 0.0 ]</li>
<li><span class="math inline">\(\phi_{2}\)</span> = [ 📕 = 0.2, 📘 = 0.1, 📗 = 0.7 ]</li>
</ul></li>
</ul>
<p><em>Generative Model Pseudocode</em></p>
<p>For d = 1 to D where number of documents is D + <em>Sample parameters for document topic distribution</em> + <span class="math inline">\(\theta_{d}\)</span> ~ Dirichlet(<span class="math inline">\(\alpha\)</span>) + For w = 1 to W where W is the number of words in document <em>d</em> + <em>Select the topic for word w </em> + <span class="math inline">\(z_{i}\)</span> ~ Multinomial(<span class="math inline">\(\theta_{d}\)</span>) + <em>Select word based on topic z’s word distribution </em> + <span class="math inline">\(w_{i}\)</span> ~ Multinomial(<span class="math inline">\(\phi^{(z_{i})}\)</span>)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># number of topics</span>
M &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># let&#39;s create 10 documents</span>
vocab &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>)
alphas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,k) <span class="co"># topic document dirichlet parameters</span>



phi &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">0.9</span>,
                <span class="fl">0.4</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>), 
              <span class="dt">nrow =</span> k, 
              <span class="dt">ncol =</span> <span class="kw">length</span>(vocab), 
              <span class="dt">byrow =</span> <span class="ot">TRUE</span>)


xi &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># average document length </span>
N &lt;-<span class="st"> </span><span class="kw">rpois</span>(M, xi) <span class="co">#words in each document</span>
ds &lt;-<span class="kw">tibble</span>(<span class="dt">doc_id =</span> <span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">sum</span>(N)), 
            <span class="dt">word   =</span> <span class="kw">rep</span>(<span class="st">&#39;&#39;</span>, <span class="kw">sum</span>(N)),
            <span class="dt">topic  =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">sum</span>(N)), 
            <span class="dt">theta_a =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">sum</span>(N)),
            <span class="dt">theta_b =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">sum</span>(N))
            ) 
            
row_index &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M){
  theta &lt;-<span class="st">  </span><span class="kw">rdirichlet</span>(<span class="dv">1</span>, alphas)
  
  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N[m]){
    <span class="co"># sample topic index , i.e. select topic</span>
    topic &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,theta)<span class="op">==</span><span class="dv">1</span>)
    <span class="co"># sample word from topic</span>
    new_word &lt;-<span class="st"> </span>vocab[<span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,phi[topic, ])<span class="op">==</span><span class="dv">1</span>)]
    ds[row_index,] &lt;-<span class="st"> </span><span class="kw">c</span>(m,new_word, topic,theta)
    row_index &lt;-<span class="st"> </span>row_index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  }
}

ds <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(
  <span class="dt">tokens =</span> <span class="kw">paste</span>(word, <span class="dt">collapse =</span> <span class="st">&#39; &#39;</span>), 
  <span class="dt">topic_a =</span> <span class="kw">round</span>(<span class="kw">as.numeric</span>(<span class="kw">unique</span>(theta_a)), <span class="dv">2</span>), 
  <span class="dt">topic_b =</span> <span class="kw">round</span>(<span class="kw">as.numeric</span>(<span class="kw">unique</span>(theta_b)), <span class="dv">2</span>) 
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">doc_id</th>
<th align="left">tokens</th>
<th align="right">topic_a</th>
<th align="right">topic_b</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">📕 📕 📗 📕 📗 📘 📗 📗 📗 📗 📗</td>
<td align="right">0.08</td>
<td align="right">0.92</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">📘 📕 📘 📘 📘 📘 📘 📘 📗 📘</td>
<td align="right">0.11</td>
<td align="right">0.89</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">📘 📗 📗 📗 📘 📕 📗 📕 📘</td>
<td align="right">0.39</td>
<td align="right">0.61</td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">📗 📗 📗 📗 📗 📗 📘 📗 📘 📗 📗</td>
<td align="right">0.43</td>
<td align="right">0.57</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left">📗 📘 📕 📕 📘 📗 📗 📕 📘 📕 📗</td>
<td align="right">0.40</td>
<td align="right">0.60</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">📗 📘 📘 📗</td>
<td align="right">0.82</td>
<td align="right">0.18</td>
</tr>
<tr class="odd">
<td align="left">6</td>
<td align="left">📘 📗 📘 📕 📗 📕 📘 📘 📕 📗 📗 📕 📗 📘 📕</td>
<td align="right">0.11</td>
<td align="right">0.89</td>
</tr>
<tr class="even">
<td align="left">7</td>
<td align="left">📗 📗 📗 📗 📗 📘 📗</td>
<td align="right">0.66</td>
<td align="right">0.34</td>
</tr>
<tr class="odd">
<td align="left">8</td>
<td align="left">📘 📘 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗 📗</td>
<td align="right">0.68</td>
<td align="right">0.32</td>
</tr>
<tr class="even">
<td align="left">9</td>
<td align="left">📕 📘 📕 📘 📗 📘 📗 📕 📘 📗 📘 📕</td>
<td align="right">0.01</td>
<td align="right">0.99</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="lda-generative-model" class="section level3">
<h3><span class="header-section-number">5.2.2</span> LDA Generative Model</h3>
<p>We are finally at the full generative model for LDA. The word distributions for each topic vary based on a dirichlet distribtion, as do the topic distribution for each document, and the document length is drawn from a Poisson distribution.</p>
<p><em>Generative Model Pseudocode</em></p>
<ol style="list-style-type: decimal">
<li>For k = 1 to K where K is the total number of topics
<ul>
<li><em>Sample parameters for word distribution of each topic</em></li>
<li><span class="math inline">\(\phi^{(k)}\)</span> ~ Dirichlet(<span class="math inline">\(\beta\)</span>)</li>
</ul></li>
<li>For d = 1 to D where number of documents is D
<ul>
<li><em>Sample parameters for document topic distribution</em></li>
<li><span class="math inline">\(\theta_{d}\)</span> ~ Dirichlet(<span class="math inline">\(\alpha\)</span>)</li>
<li>For w = 1 to W where W is the number of words in document <em>d</em>
<ul>
<li><em>Select the topic for word w </em></li>
<li><span class="math inline">\(z_{i}\)</span> ~ Multinomial(<span class="math inline">\(\theta_{d}\)</span>)</li>
<li><em>Select word based on topic z’s word distribution </em></li>
<li><span class="math inline">\(w_{i}\)</span> ~ Multinomial(<span class="math inline">\(\phi^{(z_{i})}\)</span>)</li>
</ul></li>
</ul></li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># number of topics</span>
M &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># let&#39;s create 10 documents</span>
vocab &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;\U1F4D8&#39;</span>, <span class="st">&#39;\U1F4D5&#39;</span>, <span class="st">&#39;\U1F4D7&#39;</span>)
alphas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,k) <span class="co"># topic document dirichlet parameters</span>


betas &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">length</span>(vocab)) <span class="co"># dirichlet parameters for topic word distributions</span>
phi &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(k, betas)


xi &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># average document length </span>
N &lt;-<span class="st"> </span><span class="kw">rpois</span>(M, xi) <span class="co">#words in each document</span>
ds &lt;-<span class="kw">tibble</span>(<span class="dt">doc_id =</span> <span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">sum</span>(N)), 
            <span class="dt">word   =</span> <span class="kw">rep</span>(<span class="st">&#39;&#39;</span>, <span class="kw">sum</span>(N)),
            <span class="dt">topic  =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">sum</span>(N)), 
            <span class="dt">theta_a =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">sum</span>(N)),
            <span class="dt">theta_b =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">sum</span>(N))
            ) 
            
row_index &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M){
  theta &lt;-<span class="st">  </span><span class="kw">rdirichlet</span>(<span class="dv">1</span>, alphas)
  
  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N[m]){
    <span class="co"># sample topic index , i.e. select topic</span>
    topic &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,theta)<span class="op">==</span><span class="dv">1</span>)
    <span class="co"># sample word from topic</span>
    new_word &lt;-<span class="st"> </span>vocab[<span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,phi[topic, ])<span class="op">==</span><span class="dv">1</span>)]
    ds[row_index,] &lt;-<span class="st"> </span><span class="kw">c</span>(m,new_word, topic,theta)
    row_index &lt;-<span class="st"> </span>row_index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  }
}

ds <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(doc_id, topic) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(
  <span class="dt">tokens =</span> <span class="kw">paste</span>(word, <span class="dt">collapse =</span> <span class="st">&#39; &#39;</span>), 
  <span class="dt">topic_a =</span> <span class="kw">round</span>(<span class="kw">as.numeric</span>(<span class="kw">unique</span>(theta_a)), <span class="dv">2</span>), 
  <span class="dt">topic_b =</span> <span class="kw">round</span>(<span class="kw">as.numeric</span>(<span class="kw">unique</span>(theta_b)), <span class="dv">2</span>) 
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>() </code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">doc_id</th>
<th align="left">topic</th>
<th align="left">tokens</th>
<th align="right">topic_a</th>
<th align="right">topic_b</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">1</td>
<td align="left">📕 📘 📘 📕 📗 📘 📘 📕 📕 📗 📕</td>
<td align="right">0.75</td>
<td align="right">0.25</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">2</td>
<td align="left">📕 📕 📘</td>
<td align="right">0.75</td>
<td align="right">0.25</td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="left">2</td>
<td align="left">📘 📘 📘 📘 📘 📘</td>
<td align="right">0.08</td>
<td align="right">0.92</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">1</td>
<td align="left">📕 📕 📗 📕 📕 📗 📘</td>
<td align="right">0.56</td>
<td align="right">0.44</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">2</td>
<td align="left">📕 📘 📗</td>
<td align="right">0.56</td>
<td align="right">0.44</td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">2</td>
<td align="left">📘 📗 📘 📗 📘 📘 📕 📕 📘</td>
<td align="right">0.01</td>
<td align="right">0.99</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left">1</td>
<td align="left">📕</td>
<td align="right">0.13</td>
<td align="right">0.87</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">2</td>
<td align="left">📕 📘 📗 📗 📕 📕 📘</td>
<td align="right">0.13</td>
<td align="right">0.87</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">1</td>
<td align="left">📗 📗 📗 📕 📘 📘 📕 📘 📕 📗 📘 📗 📘 📘 📗</td>
<td align="right">0.87</td>
<td align="right">0.13</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">2</td>
<td align="left">📘 📕</td>
<td align="right">0.87</td>
<td align="right">0.13</td>
</tr>
<tr class="odd">
<td align="left">6</td>
<td align="left">1</td>
<td align="left">📕</td>
<td align="right">0.26</td>
<td align="right">0.74</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">2</td>
<td align="left">📘 📘 📕 📗 📘 📘 📕 📘</td>
<td align="right">0.26</td>
<td align="right">0.74</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">1</td>
<td align="left">📘 📕</td>
<td align="right">0.13</td>
<td align="right">0.87</td>
</tr>
<tr class="even">
<td align="left">7</td>
<td align="left">2</td>
<td align="left">📘 📘 📗 📕 📘 📘 📗</td>
<td align="right">0.13</td>
<td align="right">0.87</td>
</tr>
<tr class="odd">
<td align="left">8</td>
<td align="left">1</td>
<td align="left">📗 📘 📕 📕 📕 📗 📕</td>
<td align="right">0.60</td>
<td align="right">0.40</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">2</td>
<td align="left">📕 📘 📘 📘 📕</td>
<td align="right">0.60</td>
<td align="right">0.40</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">1</td>
<td align="left">📕 📘 📘 📗 📗 📕 📘 📘 📘 📘 📕 📗</td>
<td align="right">0.77</td>
<td align="right">0.23</td>
</tr>
<tr class="even">
<td align="left">9</td>
<td align="left">2</td>
<td align="left">📘 📗</td>
<td align="right">0.77</td>
<td align="right">0.23</td>
</tr>
</tbody>
</table>
<p>(NOTE: Spread the table so topics are side by side for each doc)</p>
<p>The LDA generative process for each document is shown below<span class="citation">(Darling <a href="references.html#ref-darling2011theoretical">2011</a>)</span>:</p>
<p><span class="math display" id="eq:generativeLDA">\[
\begin{equation}
p(w,z,\theta,\phi|\alpha, B) = p(\phi|B)p(\theta|\alpha)p(z|\theta)p(w|\phi_{z})
\tag{5.1}
\end{equation}
\]</span></p>
<p>You may be like me and have a hard time seeing how we get to the equation above and what it even means. If we look back at the pseudo code for the LDA model it is a bit easier to see how we got here. We start by giving a probability of a topic for each word in the vocabulary, <span class="math inline">\(\phi\)</span>. This value is drawn randomly from a dirichlet distribution with the parameter <span class="math inline">\(\beta\)</span> giving us our first term <span class="math inline">\(p(\phi|\beta)\)</span>. The next step is generating documents which starts by calculating the topic mixture of the document, <span class="math inline">\(\theta_{d}\)</span> generated from a dirichlet distribution with the parameter <span class="math inline">\(\alpha\)</span>. This is our second term <span class="math inline">\(p(\theta|\alpha)\)</span>. You can see the following two terms also follow this trend. The topic, <em>z</em>, of the next word is drawn from a multinomial distribuiton with the parameter <span class="math inline">\(\theta\)</span>. Once we know <em>z</em>, we use the distribution of words in topic <em>z</em>, <span class="math inline">\(\phi_{z}\)</span>, to determine the word that is generated.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="word-representations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lda-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
